<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-25T12:31:13+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /你/米勒</name><uri>https://www.reddit.com/user/mierle </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;QLoRA: Effi量化 LLM 的 cient Finetuning&quot; title=&quot;QLoRA：量化 LLM 的高效微调&quot; />; &lt; /a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mierle&quot;>; /u/mierle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14314&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13r1hkg </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/"/><updated> 2023-05-24T23:29:47+00:00</updated><published> 2023-05-24T23:29:47+00:00</published><title> QLoRA：量化 LLM 的高效微调</title></entry><entry><author><name>/你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&quot; >;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qrtek </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/"/><updated> 2023-05-24T17:25:33+00:00</updated><published> 2023-05-24T17:25:33+00:00</published><title> [N] Andrej karpathy 在 MSBuild 2023 中的 GPT 状态</title></entry><entry><author><name>/u/我是布兰妮</name><uri>https://www.reddit.com/user/ISpearedBritney </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你的很多工作都涉及 AI 或 ML（无论标题如何），你能分享一下你的典型工作日是怎样的吗？您将时间花在什么上，最终经常使用哪些工具或资源？其中有多少是数据争论，你使用了多少数学？谢谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ISpearedBritney&quot;>; /u/ISpearedBritney &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rct07 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/"/><updated> 2023-05-25T09:21:06+00:00</updated><published> 2023-05-25T09:21:06+00:00</published><title> [D] 对于那些从事 ML/AI 工作的人，您的工作和工作日是什么样的？</title></entry><entry><author><name> /你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-模型架构&quot;>;https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-model-architecture&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qroe9 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/"/><updated> 2023-05-24T17:20:14+00:00</updated><published> 2023-05-24T17:20:14+00:00</published><title> [N] Meta AI 释放 Megabyte，一种革命性的可扩展模型架构</title></entry><entry><author><name>/u/Tomatomakko</name><uri> https://www.reddit.com/user/Tomatomakko </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;向量神经元 [&lt;a href=&quot;https://arxiv.org/pdf/2104.12229.pdf&quot;>;https://arxiv.org/ pdf/2104.12229.pdf&lt;/a>;] 是一种在 3D 点云处理网络中实现旋转等方差的方法。是否可以将相同的想法转移到 2D CNN？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Tomatomakko&quot;>; /u/Tomatomakko &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rfd15 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/"/><updated> 2023-05-25T11:36:16+00:00</updated><published> 2023-05-25T11:36:16+00:00</published><title> [D] 可以使用向量神经元在 2D CNN 中实现旋转等方差吗？</title></entry><entry><author><name> /u/太极官方</name><uri>https://www.reddit.com/user/TaichiOfficial </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/yR2V61hG-MR6-c-B7hm1M_8QJ7LEXpdvxPUZ7Gq9xFI.jpg&quot; alt=&quot;[P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP&quot; title=&quot;[P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP&quot; / >; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Taichi NeRF 使用神经辐射场实现高效的 3D 场景重建和新视点合成，同时提供基于 Python 的工作流程，用于即时 NGP 开发和移动设备上的轻松部署。&lt;/p>; &lt;p>;查看博客：&lt;a href=&quot;https://docs.taichi-lang.org/blog/taichi-instant- ngp&quot;>;https://docs.taichi-lang.org/blog/taichi-instant-ngp&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https ://i.redd.it/gymp61re7z1b1.gif&quot;>;https://i.redd.it/gymp61re7z1b1.gif&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/TaichiOfficial&quot;>; /u/TaichiOfficial &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13rbij5 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/yR2V61hG-MR6-c-B7hm1M_8QJ7LEXpdvxPUZ7Gq9xFI.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/"/><updated> 2023-05-25T08:03:13+00:00</updated><published> 2023-05-25T08:03:13+00:00</published><title> [P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP</title></entry><entry><author><name> /u/深渊</name><uri>https://www.reddit.com/user/abystoma </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如标题所示，任何人都可以向我推荐论文或任何使用启发式方法预测隐藏神经元和输出层输出的资源，因为我们有一个数据集的输入和输出。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/abystoma&quot;>; /u/abystoma &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r8gzk </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/"/><updated> 2023-05-25T05:03:18+00:00</updated><published> 2023-05-25T05:03:18+00:00</published><title> [D] 是否有任何工作通过使用启发式来预测隐藏神经元和输出层的输出？</title></entry><entry><author><name> /u/J00Nnn</name><uri> https://www.reddit.com/user/J00Nnn </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，有在 k8s 上进行 ML 训练的经验的人可以分享您使用的工具或框架吗？它不一定是端到端的管道解决方案（例如 Kubeflow）。&lt;/p>; &lt;p>;例如，我有 TensorFlow 模型，我想利用分布式训练，但是在 Kubernetes 资源上。有什么建议吗？&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;顺便说一句，我在这方面的经验很少，所以欢迎任何新的方向或更正，谢谢！！&lt;/p>; &lt;/ div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/J00Nnn&quot;>; /u/J00Nnn &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r670t </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/"/><updated> 2023-05-25T03:05:52+00:00</updated><published> 2023-05-25T03:05:52+00:00</published><title> [讨论] 关于在 Kubernetes 上训练 ML 模型的指南</title></entry><entry><author><name>/你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://techcrunch.com/2023/05/23/microsoft-debuts-azure-ai-studio-to- let-developers-build-their-own-ai-copilots/&quot;>;https://techcrunch.com/2023/05/23/microsoft-debuts-azure-ai-studio-to-let-developers-build-their- own-ai-copilots/&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qs34u </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/"/><updated> 2023-05-24T17:35:51+00:00</updated><published> 2023-05-24T17:35:51+00:00</published><title> [N] Microsoft 的 Azure AI Studio 让开发人员可以构建自己的 AI“副驾驶”</title></entry><entry><author><name> /你/nicku_a</name><uri> https://www.reddit.com/user/nicku_a </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们刚刚更新了 AgileRL，我们的强化学习训练框架比 SOTA 快 10 倍，支持离线强化学习！ &lt;/p>; &lt;p>;许多有 RL 可解决问题的人无法访问模拟器，但有大量数据。&lt;/p>; &lt;p>;您现在可以轻松地在静态数据上训练代理，而无需模拟，并使用进化超参数优化来更快更好地学习！&lt;/p>; &lt;p>;此版本包括：&lt;/p>; &lt;ul>; &lt;li>;新的通用离线 RL 训练功能，可从静态数据中学习&lt;/li >; &lt;li>;Conservative Q-Learning (CQL)&lt;/li>; &lt;li>;与 Minari 完全兼容&lt;/li>; &lt;/ul>; &lt;p>;查看：&lt;a href=&quot;https://github.com/ AgileRL/AgileRL&quot;>;https://github.com/AgileRL/AgileRL&lt;/a>; &lt;/p>; &lt;p>;如果你想参与这个项目，或者只是想进行讨论，请加入我们的discord （链接在我们的 GitHub 存储库顶部）！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/nicku_a&quot;>; /u/nicku_a &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qgzt5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/"/><updated> 2023-05-24T09:48:18+00:00</updated><published> 2023-05-24T09:48:18+00:00</published><title> [P] 离线强化学习 - 比具有进化 HPO 的 SOTA 快 10 倍</title></entry><entry><author><name>/u/trolls_toll</name><uri> https://www.reddit.com/user/trolls_toll </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我最近一直在用很多 1000x1000 矩阵做一些数值模拟，主要是为了分散过去几个月的疯狂。我想我还不如把每件事都做对，然后从头开始整个考验——为我的 M1 机器选择最好的 BLAS 库（实际上我只是超级生疏，谷歌搜索感觉比手工推导更容易） .&lt;/p>; &lt;p>;目前conda-forge已经预编译了基于三个BLAS实现的包：openblas、netlib和accelerate。前两个是非原生的，后者是 Apple 为其处理器优化的。可能还有其他版本可以通过 Anaconda 获得，但我并没有真正检查，因为那里的大多数数字库都链接到英特尔的 MKL，这在 mac 上不起作用。 &lt;/p>; &lt;p>;安装不同版本的 BLAS 很简单，实际上只需在 YAML conda 配方中设置一个标志。因此，我最终使用 numpy 和 scipy 的原生 &lt;code>;.test()&lt;/code>; 方法以及我在网上找到的两个脚本对所有三个 BLAS 包进行了基准测试：&lt;a href=&quot;https: //gist.github.com/MarkDana/a9481b8134cf38a556cf23e1e815dafb#2-benchmarks&quot;>;Mark Dana 的大量 SVD&lt;/a>; 和 &lt;a href=&quot;https://gist.github.com/markus-beuckelmann/8bc25531b11158431a5b09a45abd627 6&quot; >;由 Markus Beuckelmann 编写的具有一些矩阵和不同矩阵分解的要点&lt;/a>;。 &lt;/p>; &lt;p>;这是我的结果，都是在新的 conda 环境中完成的：&lt;/p>; &lt;p>;&lt;strong>;apple 的加速 &lt;code>;blas=*=accelerate&lt;/code>;&lt;/strong>;&lt;/p >; &lt;ul>; &lt;li>;svd 1.03 秒&lt;/li>; &lt;li>;matmuls 20 秒&lt;/li>; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 3 次失败，25083 次通过，393 次跳过，1309 次取消选择, 44 xfailed, 5 xpassed, 25 warnings in 76.34s (0:01:16)&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 在 linalg/tests/test_cython_blas.py 测试失败，在20% &lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;conda-forge vanilla&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 13.53 秒&lt;/li>; &lt;li>;matmuls 44 秒&lt;/li >; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 没有失败，25075 次通过，404 次跳过，1309 次取消选择，44 次失败，5 次通过，69.25 秒 (0:01:09) 中有 32 次警告&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 7 次失败，37984 次通过，2301 次跳过，12295 次取消选择，139 次失败，9 次通过，355.99 秒 (0:05:55) 中有 72 次警告&lt;/li>; &lt; /ul>; &lt;p>;&lt;strong>;netlib &lt;code>;=*=netlib&lt;/code>;&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 4.44 秒&lt;/li>; &lt;li>;matmuls 330 秒&lt;/ li>; &lt;li>;numpy 12 失败，25063 次通过，404 次跳过，1309 次取消选择，44 次失败，5 次通过，73.60 秒 (0:01:13) 中有 24 条警告&lt;/li>; &lt;li>;scipy 153 失败，37839 次通过， 2301 跳过，12295 取消选择，139 xfailed，8 xpassed，347.62s (0:05:47) 内有 86 个警告&lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;openblas &lt;code>;=*=openblas&lt;/code>; &lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 12.44 秒&lt;/li>; &lt;li>;matmuls 45 秒&lt;/li>; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 没有失败 25075 通过, 404 skiped, 1309 undeselected, 44 xfailed, 5 xpassed, 69.98s (0:01:09) 内有 32 个警告&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 7 failed, 37984 passed, 2301 skiped, 12295 undeselected, 139 xfailed, 9 xpassed, 72 warnings in 356.14s (0:05:56)&lt;/li>; &lt;/ul>; &lt;p>;这里有几个教训：a) vanilla conda-forge numpy 和 scipy版本带有 openblas，它工作得很好，b) 不要使用 netlib，除非你的矩阵很小并且你需要做很多 SVD，或者知道为什么 c) Apple 的 &lt;code>;veclib/accelerate&lt;/ code>; 非常快，但它在数值上也不稳定。以至于 scipy 的开发者 &lt;a href=&quot;https://github.com/scipy/scipy/wiki/Dropping-support-for-Accelerate&quot;>;早在 2018 年就放弃了对它的任何支持&lt;/a >;。像该死的。也就是说，他们显然将它带回来了，因为 macOS Ventura 的 13.3 版本在 &lt;code>;accelerate&lt;/code>; 性能方面有了一些重大改进。&lt;/p>; &lt;p>;FIN &lt;/p>; &lt;p>;ps我打算在 Mathematica 中做我的事情，因为动态 3D 图 &amp;gt;&amp;gt;&amp;gt;在这里和那里节省了几分钟。&lt;/p>; &lt;p>;pps 呃，忘了补充，它全部在 Apple M1 Pro 上测试，10 个内核运行 Ventura 13.3.1，python 3.10.11，conda 23.3.1，numpy 1.24 .3，scipy 1.10.1，libblas 3.9.0，openblas 0.3.21。 Netlib blas 的版本是 2.104，accelerate、openblas 和 vanilla 的版本是 2.116&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/trolls_toll&quot;>; /u/trolls_toll &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qp0s6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/"/><updated> 2023-05-24T15:37:56+00:00</updated><published> 2023-05-24T15:37:56+00:00</published><title> [D] Apple Silicon 选择哪个 BLAS 库？</title></entry><entry><author><name> /u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;在 ChatGPT 之前一个月左右，我是一个团队的一员，该团队提交了一篇论文，我们将 LLM 应用于临床文本笔记的特征提取用于分流目的。这篇论文本月发表在一份医学杂志上，所以它更适合临床人群，但我还是想在这里分享它：&lt;a href=&quot;https://www.annfammed.org /content/21/3/240&quot;>;https://www.annfammed.org/content/21/3/240&lt;/a>; &lt;/p>; &lt;blockquote>; &lt;p>;&lt;strong>;目的&lt;/strong>; 呼吸症状是初级保健中最常见的主诉。这些症状通常会自行解决，但它们可能表明患有严重的疾病。随着医生工作量和医疗保健成本的增加，在面对面咨询之前对患者进行分类会有所帮助，可能会为低风险患者提供其他沟通方式。本研究的目的是训练一个机器学习模型，在前往初级保健诊所之前对有呼吸道症状的患者进行分类，并在分类的背景下检查患者的结果。&lt;/p>; &lt;p>;&lt;strong>;方法&lt;/strong>;我们训练了一个机器学习模型，使用仅在就诊前可用的临床特征。从接受 7 个国际疾病分类第 10 次修订代码（J00、J10、JII、J15、J20、J44、J45）之一的患者的 1,500 条记录中提取临床文本注释。冰岛雷克雅未克地区的所有初级保健诊所都包括在内。该模型在 2 个外部数据集中对患者进行评分，并将他们分为 10 个风险组（值越高风险越大）。我们分析了每组的选定结果。&lt;/p>; &lt;p>;&lt;strong>;结果&lt;/strong>; 风险组 1 到 5 包括 C 反应蛋白值较低的年轻患者、初级和急诊的重新评估率、与第 6 至 10 组相比，抗生素处方率、胸部 X 光 (CXR) 转诊和具有肺炎体征的 CXR。第 1 至 5 组没有具有肺炎体征或医生诊断为肺炎的 CXR。&lt;/p>; &lt;p>;&lt;strong>;结论&lt;/strong>; 该模型根据预期结果对患者进行了分类。该模型可以通过消除风险组 1 到 5 中的 CXR 转诊数量来减少 CXR 转诊的数量，从而在没有临床医生输入的情况下减少临床上无关紧要的偶发瘤发现。&lt;/p>; &lt;/blockquote>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qwvl1 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/"/><updated> 2023-05-24T20:33:52+00:00</updated><published> 2023-05-24T20:33:52+00:00</published><title> [R] 在初级保健中针对呼吸道症状对人工智能患者进行分类以改善患者预后：一项回顾性诊断准确性研究</title></entry><entry><author><name>/u/waa007</name><uri> https://www.reddit.com/user/waa007 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Alpaca or LLaMA ?(严格来说，它们是open available，不是open source，open source的定义来自&lt;a href=&quot;https: //opensource.org/osd&quot;>;OSI&lt;/a>;)&lt;/p>; &lt;p>;是否有其他一些开放的&lt;del>;source&lt;/del>;可用的LLM？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/waa007&quot;>; /u/waa007 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qnc80 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/"/><updated> 2023-05-24T14:30:17+00:00</updated><published> 2023-05-24T14:30:17+00:00</published><title> [D] 目前最好的开源 LLM 是什么？</title></entry><entry><author><name> /u/Usual-Shopping-9638</name><uri> https://www.reddit.com/user/Usual-Shopping-9638 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;strong>;论文&lt;/strong>;&lt;br/>; &lt;a href=&quot;https://arxiv.org/abs/2210.05409&quot;>;https ://arxiv.org/abs/2210.05409&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;strong>;代码&lt;/strong>;&lt;/p>; &lt;p>;&lt;a href =&quot;https://github.com/kakaobrain/leco&quot;>;https://github.com/kakaobrain/leco&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;strong >;Abstract&lt;/strong>;&lt;/p>; &lt;p>;Episodic count 已被广泛用于设计一种简单而有效的内在动机，用于具有稀疏奖励的强化学习。然而，在高维状态空间以及长时间内使用情节计数需要彻底的状态压缩和快速散列，这阻碍了在如此困难和复杂的探索环境中对其进行严格的利用。此外，情节计数中与任务无关的观察的干扰可能会导致其内在动机忽略与任务相关的重要状态变化，而情节方式的新颖性会导致反复重访熟悉的状态。为了解决这些问题，在本文中，我们提出了一种可学习的基于哈希的情节计数，我们将其命名为 LECO，它可以在困难的探索问题中作为特定于任务的内在奖励有效地执行。特别是，所提出的内在奖励包括情节新颖性和特定于任务的调制，其中前者使用矢量量化变分自编码器自动获取离散状态代码以进行快速计数，而后者通过学习调制器来优化情节新颖性任务特定的外在奖励。拟议的 LECO 特别允许在强化学习期间从探索到利用的自动过渡。我们通过实验表明，与以前的探索方法相比，LECO 成功解决了困难的探索问题，并且还通过 MiniGrid 和 DMLab 环境中最困难的任务扩展到大状态空间。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -- >; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Usual-Shopping-9638&quot;>; /u/Usual-Shopping-9638 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13rbch6/r_leco_learnable_episodic_count_for_taskspecific/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rbch6/r_leco_learnable_episodic_count_for_taskspecific/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rbch6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rbch6/r_leco_learnable_episodic_count_for_taskspecific/"/><updated> 2023-05-25T07:53:14+00:00</updated><published> 2023-05-25T07:53:14+00:00</published><title> [R] LECO：针对特定任务的内在奖励的可学习情节计数</title></entry><entry><author><name>/你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/ai-generated-podcast-ads-on-spotify-could -soon-become-a-reality-1f6bb1a056b0&quot;>;https://medium.com/@tiago-mesquita/ai-generated-podcast-ads-on-spotify-could-soon-become-a-reality-1f6bb1a056b0&lt;/ a>;&lt;/p>; &lt;p>;在 &lt;strong>;The Bill Simmons Podcast&lt;/strong>; 的最近一集中，主持人兼 The Ringer 的创始人 Bill Simmons 表达了他相信利用自己的声音进行&lt;/p>; &lt;p>;&lt;strong>;他说：&lt;/strong>;&lt;/p>; &lt;blockquote>; &lt;p>;&lt;em>;“将有一种方法可以将我的声音用于广告。显然，你必须对声音表示认可，但从广告的角度来看，它为你打开了所有这些不同的巨大可能性。”&lt;/em>;&lt;/p>; &lt;/blockquote>; &lt;p>;Simmons 是The Ringer，一个播客网络和网站，在 2020 年被 Spotify 以近 2 亿美元的价格收购&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qq5ky </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/"/><updated> 2023-05-24T16:21:42+00:00</updated><published> 2023-05-24T16:21:42+00:00</published><title> [N] Spotify 可能正在研究提供 AI 生成的播客广告的可能性</title></entry><entry><author><name>/你/硬丸</name><uri>https://www.reddit.com/user/hardmaru </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;em>;Schmidhuber 采访表达了他对 AI 和 AGI 未来的看法。&lt;/em>;&lt;/p>; &lt;p>;&lt;em>;原创&lt;a href=&quot;https://www.forbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead -to-dystopia/&quot;>;来源&lt;/a>;。我认为 &lt;a href=&quot;/r/MachineLearning&quot;>;r/MachineLearning&lt;/a>; 对这次采访很感兴趣，并且与 AI 领域其他有影响力的领导者相比提出了另一种观点。&lt;/em>;&lt;/p>; &lt; p&lt;strong>;Juergen Schmidhuber，著名的“现代人工智能之父”，说他一生的工作不会导致反乌托邦&lt;/strong>;&lt;/p>; &lt;p>;&lt;em>;2023 年 5 月 23 日。供稿者 &lt;a href=&quot;https://twitter.com/hessiejones&quot;>;Hessie Jones&lt;/a>;.&lt;/em>;&lt;/p>; &lt;p>;随着人们越来越关注更先进的人工智能 (AI) 技术对社会的影响，技术界中有许多人担心这些进步的影响在生成人工智能中，如果他们不加检查。著名科学家、人工智能研究员、被广泛认为是该领域的先驱之一的于尔根·施密德胡伯博士则更为乐观。他宣称，许多突然警告 AI 危险的人只是为了宣传，利用媒体对杀手机器人的痴迷，这种机器人比医疗保健等领域的“好 AI”更受关注。&lt;/p>; &lt;p>;彻底改变各个行业并改善我们的生活是显而易见的，如果不良行为者利用该技术谋取私利，同样危险。我们是在走向一个反乌托邦的未来，还是有理由保持乐观？我有机会与 Juergen Schmidhuber 博士坐下来了解他对这辆看似飞速发展的 AI 火车的看法，它将带领我们迈向未来。&lt;/p>; &lt;p>;作为 1970 年代的少年，Juergen Schmidhuber 成为着迷于创造智能机器的想法，这些机器可以自己学习和改进，在他的有生之年变得比他自己更聪明。这最终导致了他在深度学习领域的开创性工作。&lt;/p>; &lt;p>;1980 年代，他在慕尼黑工业大学 (TUM) 学习计算机科学，并于 1987 年获得文凭。他的论文是在最终的自我改进机器上，它们不仅通过一些预先连接的人工设计的学习算法进行学习，而且还学习和改进学习算法本身。几十年后，这成为一个热门话题。他还获得了博士学位。 Schmidhuber 于 1991 年在慕尼黑工业大学工作，为现代人工智能奠定了一些基础。&lt;/p>; &lt;p>;Schmidhuber 最出名的是他对循环神经网络 (RNN) 发展的贡献，这是最强大的人工神经网络类型可以处理语音和自然语言等序列数据。他与他的学生 Sepp Hochreiter、Felix Gers、Alex Graves、Daan Wierstra 等人一起发表了长短期记忆 (LSTM) 的架构和训练算法，LSTM 是一种广泛用于自然语言处理、语音识别的 RNN 、视频游戏、机器人和其他应用。 LSTM 已成为 20 世纪被引用最多的神经网络，《商业周刊》将其称为“&lt;a href=&quot;https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and -facebook-owe-j-rgen-schmidhuber-a-fortune?leadSource=uverify%20wall&quot;>;可以说是最商业化的 AI 成就&lt;/a>;。&quot;&lt;/p>; &lt;p>;在他的整个职业生涯中，Schmidhuber 获得了各种因其开创性工作而获得的奖项和荣誉。 2013 年，他被授予亥姆霍兹奖，以表彰他在机器学习领域的重大贡献。 2016 年，他因“对深度学习和神经网络的开创性贡献”而获得 IEEE 神经网络先驱奖。媒体经常称他为“现代人工智能之父”，&lt;/em>;，因为&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/most-cited-neural-nets.html&quot;>;最引用的神经网络&lt;/a>;都建立在他实验室的工作之上。不过，他很快指出，人工智能的历史&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/deep-learning-history.html&quot;>;可以追溯到几个世纪前。&lt;/a>;&lt;/p >; &lt;p>;尽管他取得了许多成就，但在 60 岁时，他感到在有生之年构建通用人工智能的时间压力越来越大，并继续致力于推动 AI 研发的边界。现任KAUST AI Initiative主任，瑞士人工智能实验室IDSIA科学总监，人工智能公司NNAISENSE的联合创始人兼首席科学家，该公司的座右铭是“AI∀”这是一种受数学启发的表达“AI For All”的方式。他继续致力于尖端人工智能技术和应用，以改善人类健康、延长人类寿命并让每个人的生活更轻松。&lt;/p>; &lt;p>;&lt;em>;为清楚起见，对以下采访进行了编辑。&lt;/em>; &lt;/p>; &lt;p>;&lt;strong>;Jones：感谢 Juergen 加入我的行列。你已经签署了关于 AI 武器的警告信。但是你没有在最近的出版物“暂停巨大的人工智能实验：一封公开信”上签名？有原因吗？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 谢谢 Hessie。很高兴和你说话。我意识到，许多在公开场合警告 AI 危险的人只是为了宣传。我认为最新的这封信不会产生任何重大影响，因为许多 AI 研究人员、公司和政府会完全忽视它。&lt;/p>; &lt;p>;该提案经常使用“我们”这个词。指的是“我们”，人类。但正如我过去多次指出的那样，没有“我们”之分。每个人都可以认同。问 10 个不同的人，您会听到 10 种关于什么是“好”的不同意见。其中一些意见将彼此完全不相容。不要忘记许多人之间的巨大冲突。&lt;/p>; &lt;p>;这封信还说，“&lt;em>;如果这样的暂停不能很快到位，政府应该进行干预并实施暂停。&lt;/em>;“问题是不同的政府对于什么对他们和其他人有好处也有不同的看法。大国 A 会说，如果我们不这样做，大国 B 将会（也许是秘密地）获得对我们的优势。大国 C 和 D 也是如此。&lt;/p>; &lt;p>;&lt;strong>;琼斯：每个人都承认对当前的生成人工智能技术的恐惧。此外，&lt;/strong>; &lt;a href=&quot;https://www.bbc.com/news/world-us-canada-65616866&quot;>;&lt;strong>;Sam Altman&lt;/strong>; 已公开承认这项技术存在的威胁strong>;&lt;/a>;&lt;strong>;，OpenAI 的 CEO 本人，呼吁对 AI 进行监管。从您的角度来看，是否存在生存威胁？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;AI 确实可以被武器化，我毫不怀疑会有各种各样的威胁AI 军备竞赛，但 AI 不会引入新的生存威胁。 The threat coming from AI weapons seems to pale in comparison to the much older threat from nuclear hydrogen bombs that don&#39;t need AI at all. We should be much more afraid of half-century-old tech in the form of H-bomb rockets. The Tsar Bomba of 1961 had almost 15 times more destructive power than all weapons of WW-II combined. Despite the dramatic nuclear disarmament since the 1980s, there are still more than enough nuclear warheads to wipe out human civilization within two hours, without any AI I&#39;m much more worried about that old existential threat than the rather harmless AI weapons.&lt;/p>; &lt;p>;&lt;strong>;Jones: I realize that while you compare AI to the threat of nuclear bombs, there is a current danger that a current technology can be put in the hands of humans and enable them to “eventually” exact further harms to individuals of group in a very precise way, like targeted drone attacks. You are giving people a toolset that they&amp;#39;ve never had before, enabling bad actors, as some have pointed out, to be able to do a lot more than previously because they didn&amp;#39;t have this technology.&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Now, all that sounds horrible in principle, but our existing laws are sufficient to deal with these new types of weapons enabled by AI. If you kill someone with a gun, you will go to jail. Same if you kill someone with one of these drones. Law enforcement will get better at understanding new threats and new weapons and will respond with better technology to combat these threats. Enabling drones to target persons from a distance in a way that requires some tracking and some intelligence to perform, which has traditionally been performed by skilled humans, to me, it seems is just an improved version of a traditional weapon, like a gun, which is, you know, a little bit smarter than the old guns.&lt;/p>; &lt;p>;But, in principle, all of that is not a new development. For many centuries, we have had the evolution of better weaponry and deadlier poisons and so on, and law enforcement has evolved their policies to react to these threats over time. So, it&amp;#39;s not that we suddenly have a new quality of existential threat and it&amp;#39;s much more worrisome than what we have had for about six decades. A large nuclear warhead doesn&#39;t need fancy face recognition to kill an individual. No, it simply wipes out an entire city with ten million inhabitants.&lt;/p>; &lt;p>;&lt;strong>;Jones: The existential threat that&#39;s implied is the extent to which humans have control over this technology. We see some early cases of opportunism which, as you say, tends to get more media attention than positive breakthroughs. But you&#39;re implying that this will all balance out?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Historically, we have a long tradition of technological breakthroughs that led to advancements in weapons for the purpose of defense but also for protection. From sticks, to rocks, to axes to gunpowder to cannons to rockets… and now to drones… this has had a drastic influence on human history but what has been consistent throughout history is that those who are using technology to achieve their own ends are themselves, facing the same technology because the opposing side is learning to use it against them. And that&amp;#39;s what has been repeated in thousands of years of human history and it will continue. I don&amp;#39;t see the new AI arms race as something that is remotely as existential a threat as the good old nuclear warheads.&lt;/p>; &lt;p>;You said something important, in that some people prefer to talk about the downsides rather than the benefits of this technology, but that&amp;#39;s misleading, because 95% of all AI research and AI development is about making people happier and advancing human life and health.&lt;/p>; &lt;p>;&lt;strong>;Jones: Let&#39;s touch on some of those beneficial advances in AI research that have been able to radically change present day methods and achieve breakthroughs.&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; All right! For example, eleven years ago, our team with my postdoc Dan Ciresan was the first to win a &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/first-time-deep-learning-won-medical-imaging-contest-september-2012.html&quot;>;medical imaging competition through deep learning&lt;/a>;. We analyzed female breast cells with the objective to determine harmless cells vs. those in the pre-cancer stage. Typically, a trained oncologist needs a long time to make these determinations. Our team, who knew nothing about cancer, were able to train an artificial neural network, which was totally dumb in the beginning, on lots of this kind of data. It was able to outperform all the other methods. Today, this is being used not only for breast cancer, but also for radiology and detecting plaque in arteries, and many other things. Some of the neural networks that we have developed in the last 3 decades are now prevalent across thousands of healthcare applications, detecting Diabetes and Covid-19 and what not. This will eventually permeate across all healthcare. The good consequences of this type of AI are much more important than the click-bait new ways of conducting crimes with AI.&lt;/p>; &lt;p>;&lt;strong>;Jones: Adoption is a product of reinforced outcomes. The massive scale of adoption either leads us to believe that people have been led astray, or conversely, technology is having a positive effect on people&#39;s lives.&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; The latter is the likely case. There&amp;#39;s intense commercial pressure towards good AI rather than bad AI because companies want to sell you something, and you are going to buy only stuff you think is going to be good for you. So already just through this simple, commercial pressure, you have a tremendous bias towards good AI rather than bad AI. However, doomsday scenarios like in Schwarzenegger movies grab more attention than documentaries on AI that improve people&#39;s lives.&lt;/p>; &lt;p>;&lt;strong>;Jones: I would argue that people are drawn to good stories – narratives that contain an adversary and struggle, but in the end, have happy endings. And this is consistent with your comment on human nature and how history, despite its tendency for violence and destruction of humanity, somehow tends to correct itself.&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Let&#39;s take the example of a technology, which you are aware – GANs – General Adversarial Networks, which today has been used in applications for fake news and disinformation. In actuality, the purpose in the invention of GANs was far from what it is used for today.&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Yes, the name GANs was created in 2014 but we had the basic principle already in the early 1990s. More than 30 years ago, I called it &lt;em>;artificial curiosity&lt;/em>;. It&amp;#39;sa very simple way of injecting creativity into a little two network system. This creative AI is not just trying to slavishly imitate humans. Rather, it&#39;s inventing its own goals. Let me explain:&lt;/p>; &lt;p>;You have two networks. One network is producing outputs that could be anything, any action. Then the second network is looking at these actions and it&#39;s trying to predict the consequences of these actions. An action could move a robot, then something happens, and the other network is just trying to predict what will happen.&lt;/p>; &lt;p>;Now we can implement artificial curiosity by reducing the prediction error of the second network, which, at the same time, is the reward of the first network. The first network wants to maximize its reward and so it will invent actions that will lead to situations that will surprise the second network, which it has not yet learned to predict well.&lt;/p>; &lt;p>;In the case where the outputs are fake images, the first network will try to generate images that are good enough to fool the second network, which will attempt to predict the reaction of the environment: fake or real image, and it will try to become better at it. The first network will continue to also improve at generating images whose type the second network will not be able to predict. So, they fight each other. The 2nd network will continue to reduce its prediction error, while the 1st network will attempt to maximize it.&lt;/p>; &lt;p>;Through this zero-sum game the first network gets better and better at producing these convincing fake outputs which look almost realistic. So, once you have an interesting set of images by Vincent Van Gogh, you can generate new images that leverage his style, without the original artist having ever produced the artwork himself.&lt;/p>; &lt;p>;&lt;strong>;Jones: I see how the Van Gogh example can be applied in an education setting and there are countless examples of artists mimicking styles from famous painters but image generation from this instance that can happen within seconds is quite another feat. And you know this is how GANs has been used. What&#39;s more prevalent today is a socialized enablement of generating images or information to intentionally fool people. It also surfaces new harms that deal with the threat to intellectual property and copyright, where laws have yet to account for. And from your perspective this was not the intention when the model was conceived. What was your motivation in your early conception of what is now GANs?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; My old motivation for GANs was actually very important and it was not to create deepfakes or fake news but to enable AIs to be curious and invent their own goals, to make them explore their environment and make them creative.&lt;/p>; &lt;p>;Suppose you have a robot that executes one action, then something happens, then it executes another action, and so on, because it wants to achieve certain goals in the environment. For example, when the battery is low, this will trigger “pain” through hunger sensors, so it wants to go to the charging station, without running into obstacles, which will trigger other pain sensors. It will seek to minimize pain (encoded through numbers). Now the robot has a friend, the second network, which is a world model ––it&#39;s a prediction machine that learns to predict the consequences of the robot&#39;s actions.&lt;/p>; &lt;p>;Once the robot has a good model of the world, it can use it for planning. It can be used as a simulation of the real world. And then it can determine what is a good action sequence. If the robot imagines this sequence of actions, the model will predict a lot of pain, which it wants to avoid. If it plays this alternative action sequence in its mental model of the world, then it will predict a rewarding situation where it&#39;s going to sit on the charging station and its battery is going to load again. So, it&amp;#39;ll prefer to execute the latter action sequence.&lt;/p>; &lt;p>;In the beginning, however, the model of the world knows nothing, so how can we motivate the first network to generate experiments that lead to data that helps the world model learn something it didn&#39;t already know? That&#39;s what artificial curiosity is about. The dueling two network systems effectively explore uncharted environments by creating experiments so that over time the curious AI gets a better sense of how the environment works. This can be applied to all kinds of environments, and has medical applications.&lt;/p>; &lt;p>;&lt;strong>;Jones: Let&#39;s talk about the future. You have said, “&lt;/strong>;&lt;strong>;&lt;em>;Traditional humans won&#39;t play a significant role in spreading intelligence across the universe.&lt;/em>;&lt;/strong>;&lt;strong>;”&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Let&#39;s first conceptually separate two types of AIs. The first type of AI are tools directed by humans. They are trained to do specific things like accurately detect diabetes or heart disease and prevent attacks before they happen. In these cases, the goal is coming from the human. More interesting AIs are setting their own goals. They are inventing their own experiments and learning from them. Their horizons expand and eventually they become more and more general problem solvers in the real world. They are not controlled by their parents, but much of what they learn is through self-invented experiments.&lt;/p>; &lt;p>;A robot, for example, is rotating a toy, and as it is doing this, the video coming in through the camera eyes, changes over time and it begins to learn how this video changes and learns how the 3D nature of the toy generates certain videos if you rotate it a certain way, and eventually, how gravity works, and how the physics of the world works. Like a little scientist!&lt;/p>; &lt;p>;And I have predicted for decades that future scaled-up versions of such AI scientists will want to further expand their horizons, and eventually go where most of the physical resources are, to build more and bigger AIs. And of course, almost all of these resources are far away from earth out there in space, which is hostile to humans but friendly to appropriately designed AI-controlled robots and self-replicating robot factories. So here we are not talking any longer about our tiny biosphere; no, we are talking about the much bigger rest of the universe. Within a few tens of billions of years, curious self-improving &lt;a href=&quot;https://blogs.scientificamerican.com/observations/falling-walls-the-past-present-and-future-of-artificial-intelligence/&quot;>;AIs will colonize the visible cosmos&lt;/a>; in a way that&#39;s infeasible for humans. Those who don&#39;t won&#39;t have an impact. Sounds like science fiction, but since the 1970s I have been unable to see a plausible alternative to this scenario, except for a global catastrophe such as an all-out nuclear war that stops this development before it takes off.&lt;/p>; &lt;p>;&lt;strong>;Jones: How long have these AIs, which can set their own goals — how long have they existed? To what extent can they be independent of human interaction?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Neural networks like that have existed for over 30 years. My first simple adversarial neural network system of this kind is the one from 1990 described above. You don&#39;t need a teacher there; it&amp;#39;s just a little agent running around in the world and trying to invent new experiments that surprise its own prediction machine.&lt;/p>; &lt;p>;Once it has figured out certain parts of the world, the agent will become bored and will move on to more exciting experiments. The simple 1990 systems I mentioned have certain limitations, but in the past three decades, we have also built more &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/artificial-curiosity-since-1990.html&quot;>;sophisticated systems that are setting their own goals&lt;/a>; and such systems I think will be essential for achieving true intelligence. If you are only imitating humans, you will never go beyond them. So, you really must give AIs the freedom to explore previously unexplored regions of the world in a way that no human is really predefining.&lt;/p>; &lt;p>;&lt;strong>;Jones: Where is this being done today?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Variants of neural network-based artificial curiosity are used today for agents that learn to play video games in a human-competitive way. We have also started to use them for automatic design of experiments in fields such as materials science. I bet many other fields will be affected by it: chemistry, biology, drug design, you name it. However, at least for now, these artificial scientists, as I like to call them, cannot yet compete with human scientists.&lt;/p>; &lt;p>;I don&#39;t think it&#39;s going to stay this way but, at the moment, it&#39;s still the case. Sure, AI has made a lot of progress. Since 1997, there have been superhuman chess players, and since 2011, through the DanNet of my team, there have been &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/DanNet-triggers-deep-CNN-revolution-2011.html&quot;>;superhuman visual pattern recognizers&lt;/a>;. But there are other things where humans, at the moment at least, are much better, in particular, science itself. In the lab we have many first examples of self-directed artificial scientists, but they are not yet convincing enough to appear on the radar screen of the public space, which is currently much more fascinated with simpler systems that just imitate humans and write texts based on previously seen human-written documents.&lt;/p>; &lt;p>;&lt;strong>;Jones: You speak of these numerous instances dating back 30 years of these lab experiments where these self-driven agents are deciding and learning and moving on once they&#39;ve learned. And I assume that that rate of learning becomes even faster over time. What kind of timeframe are we talking about when this eventually is taken outside of the lab and embedded into society?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; This could still take months or even years :-) Anyway, in the not-too-distant future, we will probably see artificial scientists who are good at devising experiments that allow them to discover new, previously unknown physical laws.&lt;/p>; &lt;p>;As always, we are going to profit from the old trend that has held at least since 1941: every decade compute is getting 100 times cheaper.&lt;/p>; &lt;p>;&lt;strong>;Jones: How does this trend affect modern AI such as ChatGPT?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Perhaps you know that all the recent famous AI applications such as ChatGPT and similar models are largely based on principles of artificial neural networks invented in the previous millennium. The main reason why they works so well now is the incredible acceleration of compute per dollar.&lt;/p>; &lt;p>;ChatGPT is driven by a neural network called “Transformer” described in 2017 by Google. I am happy about that because a quarter century earlier in 1991 I had a particular Transformer variant which is now called the “&lt;a href=&quot;https://twitter.com/SchmidhuberAI/status/1576966129993797632?cxt=HHwWgMDSkeKVweIrAAAA&quot;>;Transformer with linearized self-attention&lt;/a>;”. Back then, not much could be done with it, because the compute cost was a million times higher than today. But today, one can train such models on half the internet and achieve much more interesting results.&lt;/p>; &lt;p>;&lt;strong>;Jones: And for how long will this acceleration continue?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; There&amp;#39;s no reason to believe that in the next 30 years, we won&amp;#39;t have another factor of 1 million and that&amp;#39;s going to be really significant. In the near future, for the first time we will have many not-so expensive devices that can compute as much as a human brain. The physical limits of computation, however, are much further out so even if the trend of a factor of 100 every decade continues, the physical limits (of 1051 elementary instructions per second and kilogram of matter) won&#39;t be hit until, say, the mid-next century. Even in our current century, however, we&#39;ll probably have many machines that compute more than all 10 billion human brains collectively and you can imagine, everything will change then!&lt;/p>; &lt;p>;&lt;strong>;Jones: That is the big question. Is everything going to change? If so, what do you say to the next generation of leaders, currently coming out of college and university. So much of this change is already impacting how they study, how they will work, or how the future of work and livelihood is defined. What is their purpose and how do we change our systems so they will adapt to this new version of intelligence?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; For decades, people have asked me questions like that, because you know what I&amp;#39;m saying now, I have basically said since the 1970s, it&#39;s just that today, people are paying more attention because, back then, they thought this was science fiction.&lt;/p>; &lt;p>;They didn&amp;#39;t think that I would ever come close to achieving my crazy life goal of building a machine that learns to become smarter than myself such that I can retire. But now many have changed their minds and think it&amp;#39;s conceivable. And now I have two daughters, 23 and 25. People ask me: what do I tell them? They know that Daddy always said, “&lt;em>;It seems likely that within your lifetimes, you will have new types of intelligence that are probably going to be superior in many ways, and probably all kinds of interesting ways.&lt;/em>;” How should they prepare for that? And I kept telling them the obvious: &lt;strong>;Learn how to learn new things&lt;/strong>;! It&amp;#39;s not like in the previous millennium where within 20 years someone learned to be a useful member of society, and then took a job for 40 years and performed in this job until she received her pension. Now things are changing much faster and we must learn continuously just to keep up. I also told my girls that no matter how smart AIs are going to get, learn at least the basics of math and physics, because that&#39;s the essence of our universe, and anybody who understands this will have an advantage, and learn all kinds of new things more easily. I also told them that social skills will remain important, because most future jobs for humans will continue to involve interactions with other humans, but I couldn&#39;t teach them anything about that; they know much more about social skills than I do.&lt;/p>; &lt;p>;You touched on the big philosophical question about people&#39;s purpose. Can this be answered without answering the even grander question: What&#39;s the purpose of the entire universe?&lt;/p>; &lt;p>;We don&#39;t know. But what&#39;s happening right now might be connected to the unknown answer. Don&#39;t think of humans as the crown of creation. Instead view human civilization as part of a much grander scheme, an important step (but not the last one) on the path of the universe from very simple initial conditions towards more and more unfathomable complexity. Now it seems ready to take its &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/deep-learning-history.html#future&quot;>;next step, a step comparable to the invention of life itself over 3.5 billion years ago&lt;/a>;. Alas, don&#39;t worry, in the end, all will be good!&lt;/p>; &lt;p>;&lt;strong>;Jones: Let&#39;s get back to this transformation happening right now with OpenAI. There are many questioning the efficacy and accuracy of ChatGPT, and are concerned its release has been premature. In light of the rampant adoption, educators have banned its use over concerns of plagiarism and how it stifles individual development. Should large language models like ChatGPT be used in school?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; When the calculator was first introduced, instructors forbade students from using it in school. Today, the consensus is that kids should learn the basic methods of arithmetic, but they should also learn to use the “artificial multipliers” aka calculators, even in exams, because laziness and efficiency is a hallmark of intelligence. Any intelligent being wants to minimize its efforts to achieve things.&lt;/p>; &lt;p>;And that&amp;#39;s the reason why we have tools, and why our kids are learning to use these tools. The first stone tools were invented maybe 3.5 million years ago; tools just have become more sophisticated over time. In fact, humans have changed in response to the properties of their tools. Our anatomical evolution was shaped by tools such as spears and fire. So, it&amp;#39;s going to continue this way. And there is no permanent way of preventing large language models from being used in school.&lt;/p>; &lt;p>;&lt;strong>;Jones: And when our children, your children graduate, what does their future work look like?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; A single human trying to predict details of how 10 billion people and their machines will evolve in the future is like a single neuron in my brain trying to predict what the entire brain and its tens of billions of neurons will do next year. 40 years ago, before the WWW was created at CERN in Switzerland, who would have predicted all those young people making money as YouTube video bloggers?&lt;/p>; &lt;p>;Nevertheless, let&#39;s make a few limited job-related observations. For a long time, people have thought that desktop jobs may require more intelligence than skills trade or handicraft professions. But now, it turns out that it&amp;#39;s much easier to replace certain aspects of desktop jobs than replacing a carpenter, for example. Because everything that works well in AI is happening behind the screen currently, but not so much in the physical world.&lt;/p>; &lt;p>;There are now artificial systems that can read lots of documents and then make really nice summaries of these documents. That is a desktop job. Or you give them a description of an illustration that you want to have for your article and pretty good illustrations are being generated that may need some minimal fine-tuning. But you know, all these desktop jobs are much easier to facilitate than the real tough jobs in the physical world. And it&amp;#39;s interesting that the things people thought required intelligence, like playing chess, or writing or summarizing documents, are much easier for machines than they thought. But for things like playing football or soccer, there is no physical robot that can remotely compete with the abilities of a little boy with these skills. So, AI in the physical world, interestingly, is much harder than AI behind the screen in virtual worlds. And it&amp;#39;s really exciting, in my opinion, to see that jobs such as plumbers are much more challenging than playing chess or writing another tabloid story.&lt;/p>; &lt;p>;&lt;strong>;Jones: The way data has been collected in these large language models does not guarantee personal information has not been excluded. Current consent laws already are outdated when it comes to these large language models (LLM). The concern, rightly so, is increasing surveillance and loss of privacy. What is your view on this?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; As I have indicated earlier: are surveillance and loss of privacy inevitable consequences of increasingly complex societies? Super-organisms such as cities and states and companies consist of numerous people, just like people consist of numerous cells. These cells enjoy little privacy. They are constantly monitored by specialized &amp;quot;police cells&amp;quot; and &amp;quot;border guard cells&amp;quot;: Are you a cancer cell? Are you an external intruder, a pathogen? Individual cells sacrifice their freedom for the benefits of being part of a multicellular organism.&lt;/p>; &lt;p>;Similarly, for super-organisms such as nations. Over 5000 years ago, writing enabled recorded history and thus became its inaugural and most important invention. Its initial purpose, however, was to facilitate surveillance, to track citizens and their tax payments. The more complex a super-organism, the more comprehensive its collection of information about its constituents.&lt;/p>; &lt;p>;200 years ago, at least, the parish priest in each village knew everything about all the village people, even about those who did not confess, because they appeared in the confessions of others. Also, everyone soon knew about the stranger who had entered the village, because some occasionally peered out of the window, and what they saw got around. Such control mechanisms were temporarily lost through anonymization in rapidly growing cities but are now returning with the help of new surveillance devices such as smartphones as part of digital nervous systems that tell companies and governments a lot about billions of users. Cameras and drones etc. are becoming increasingly tinier and more ubiquitous. More effective recognition of faces and other detection technology are becoming cheaper and cheaper, and many will use it to identify others anywhere on earth; the big wide world will not offer any more privacy than the local village. Is this good or bad? Some nations may find it easier than others to justify more complex kinds of super-organisms at the expense of the privacy rights of their constituents.&lt;/p>; &lt;p>;&lt;strong>;Jones: So, there is no way to stop or change this process of collection, or how it continuously informs decisions over time? How do you see governance and rules responding to this, especially amid&lt;/strong>; &lt;a href=&quot;https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html&quot;>;&lt;strong>;Italy&#39;s ban on ChatGPT following&lt;/strong>;&lt;/a>; &lt;strong>;suspected user data breach and the more recent news about the&lt;/strong>; &lt;a href=&quot;https://www.reuters.com/technology/facebook-given-record-13-bln-fine-given-5-months-stop-eu-us-data-flows-2023-05-22/&quot;>;&lt;strong>;Meta&#39;s record $1.3billion fine&lt;/strong>;&lt;/a>; &lt;strong>;in the company&#39;s handling of user information?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Data collection has benefits and drawbacks, such as the loss of privacy. How to balance those? I have argued for addressing this through data ownership in data markets. If it is true that data is the new oil, then it should have a price, just like oil. At the moment, the major surveillance platforms such as Meta do not offer users any money for their data and the transitive loss of privacy. In the future, however, we will likely see attempts at creating efficient data markets to figure out the data&amp;#39;s true financial value through the interplay between supply and demand.&lt;/p>; &lt;p>;Even some of the sensitive medical data should not be priced by governmental regulators but by patients (and healthy persons) who own it and who may sell or license parts thereof as micro-entrepreneurs in a healthcare data market.&lt;/p>; &lt;p>;Following a previous &lt;a href=&quot;https://www.swissre.com/institute/conferences/The-intelligence-behind-artificial-intelligence.html&quot;>;interview&lt;/a>;, I gave for one of the largest re-insurance companies , let&amp;#39;s look at the different participants in such a data market: patients, hospitals, data companies. (1) &lt;strong>;Patients&lt;/strong>; with a rare form of cancer can offer more valuable data than patients with a very common form of cancer. (2) &lt;strong>;Hospitals&lt;/strong>; and their machines are needed to extract the data, eg, through magnet spin tomography, radiology, evaluations through human doctors, and so on. (3) &lt;strong>;Companies&lt;/strong>; such as Siemens, Google or IBM would like to buy annotated data to make better artificial neural networks that learn to predict pathologies and diseases and the consequences of therapies. Now the market&#39;s invisible hand will decide about the data&#39;s price through the interplay between demand and supply. On the demand side, you will have several companies offering something for the data, maybe through an app on the smartphone (a bit like a stock market app). On the supply side, each patient in this market should be able to profit from high prices for rare valuable types of data. Likewise, competing data extractors such as hospitals will profit from gaining recognition and trust for extracting data well at a reasonable price. The market will make the whole system efficient through incentives for all who are doing a good job. Soon there will be a flourishing ecosystem of commercial data market advisors and what not, just like the ecosystem surrounding the traditional stock market. The value of the data won&#39;t be determined by governments or ethics committees, but by those who own the data and decide by themselves which parts thereof they want to license to others under certain conditions.&lt;/p>; &lt;p>;At first glance, a market-based system seems to be detrimental to the interest of certain monopolistic companies, as they would have to pay for the data - some would prefer free data and keep their monopoly. However, since every healthy and sick person in the market would suddenly have an incentive to collect and share their data under self-chosen anonymity conditions, there will soon be many more useful data to evaluate all kinds of treatments. On average, people will live longer and healthier, and many companies and the entire healthcare system will benefit.&lt;/p>; &lt;p>;&lt;strong>;Jones: Finally, what is your view on open source versus the private companies like Google and OpenAI? Is there a danger to supporting these private companies&#39; large language models versus trying to keep these models open source and transparent, very much like what LAION is doing?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; I signed this &lt;a href=&quot;https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/?sh=6973c08b62e3&quot;>;open letter by LAION&lt;/a>; because I strongly favor the open-source movement. And I think it&amp;#39;s also something that is going to challenge whatever big tech dominance there might be at the moment. Sure, the best models today are run by big companies with huge budgets for computers, but the exciting fact is that open-source models are not so far behind, some people say maybe six to eight months only. Of course, the private company models are all based on stuff that was created in academia, often in little labs without so much funding, which publish without patenting their results and open source their code and others take it and improved it.&lt;/p>; &lt;p>;Big tech has profited tremendously from academia; their main achievement being that they have scaled up everything greatly, sometimes even failing to credit the original inventors.&lt;/p>; &lt;p>;So, it&amp;#39;s very interesting to see that as soon as some big company comes up with a new scaled-up model, lots of students out there are competing, or collaborating, with each other, trying to come up with equal or better performance on smaller networks and smaller machines. And since they are open sourcing, the next guy can have another great idea to improve it, so now there&#39;s tremendous competition also for the big companies.&lt;/p>; &lt;p>;Because of that, and since AI is still getting exponentially cheaper all the time, I don&amp;#39;t believe that big tech companies will dominate in the long run. They find it very hard to compete with the enormous open-source movement. As long as you can encourage the open-source community, I think you shouldn&amp;#39;t worry too much. Now, of course, you might say if everything is open source, then the bad actors also will more easily have access to these AI tools. And there&amp;#39;s truth to that. But as always since the invention of controlled fire, it was good that knowledge about how technology works quickly became public such that everybody could use it. And then, against any bad actor, there&amp;#39;s almost immediately a counter actor trying to nullify his efforts. You see, I still believe in our old motto &amp;quot;AI∀&amp;quot; or &amp;quot;AI For All.&amp;quot;&lt;/p>; &lt;p>;&lt;strong>;Jones: Thank you, Juergen for sharing your perspective on this amazing time in history. It&#39;s clear that with new technology, the enormous potential can be matched by disparate and troubling risks which we&#39;ve yet to solve, and even those we have yet to identify. If we are to dispel the fear of a sentient system for which we have no control, humans, alone need to take steps for more responsible development and collaboration to ensure AI technology is used to ultimately benefit society. Humanity will be judged by what we do next.&lt;/strong>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/hardmaru&quot;>; /u/hardmaru &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13q6k4a </id><link href="https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/"/><updated> 2023-05-24T01:00:28+00:00</updated><published> 2023-05-24T01:00:28+00:00</published><title> Interview with Juergen Schmidhuber, renowned &#39;Father Of Modern AI&#39;, says his life&#39;s work won&#39;t lead to dystopia.</title></entry><entry><author><name> /u/jesst177</name><uri> https://www.reddit.com/user/jesst177 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi!&lt;/p>; &lt;p>;We recently decide to buy a workstation with a budget of $15K. We look at our option in local vendor and also check their compute power, and came up with a couple of option.&lt;/p>; &lt;p>;- 4X A4500&lt;/p>; &lt;p>;- 1XA6000&lt;/p>; &lt;p>;We can also look for any other alternatives with mid level options such as 2X A5000/A5500. However from our standing point A4500s are having more compute power, and will have around 80 GB memory. Although I am not sure whether we can use it all of them together as in multi-gpu setting (Can we?) which mean it is better option. Should we go with 4X A4500 or any of the mid options?&lt;/p>; &lt;p>;The machine we are interested in will be used in Deep Learning, with Transformers and ConvNets.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/jesst177&quot;>; /u/jesst177 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qxf3g </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/"/><updated> 2023-05-24T20:55:18+00:00</updated><published> 2023-05-24T20:55:18+00:00</published><title> [D] Should we go with a single A6000 or 4XA4500 or any other alternative such as 2XA5000</title></entry><entry><author><name> /u/wazazzz</name><uri> https://www.reddit.com/user/wazazzz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hey all, just wanting to share this open source library I&#39;ve been working on that aims to makes LLMs experimentation (prompt chain engineering, fine tuning, variable integrated code generation, token probability/perplexity analysis) more accessible and easier to setup.&lt;/p>; &lt;p>;Open for feedback and collaboration!&lt;/p>; &lt;p>;&lt;a href=&quot;https://github.com/Pan-ML/panml&quot;>;https://github.com/Pan-ML/panml&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/wazazzz&quot;>; /u/wazazzz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qj9ye </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/"/><updated> 2023-05-24T11:49:13+00:00</updated><published> 2023-05-24T11:49:13+00:00</published><title> [Project] PanML, a high level Python library for fast LLM experimentation</title></entry><entry><author><name> /u/Jean-Porte</name><uri> https://www.reddit.com/user/Jean-Porte </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi everyone,&lt;/p>; &lt;p>;I just finished the first version of tasksource-instruct.&lt;br/>; &lt;a href=&quot;https://huggingface.co/datasets/tasksource/tasksource-instruct-v0&quot;>;https://huggingface.co/datasets/tasksource/tasksource-instruct-v0&lt;/a>;&lt;br/>; It is based on hundreds of classification datasets on huggingface. Tasks not in flan include dynasent (adversarial sentiment analysis), Dynahate (adversarial hate speech detection, discriminative babi, epistemic logic, ruletaker, MANY natural language inference datasets.&lt;/p>; &lt;p>;It is also focused on explicitly classification, which isolates reasoning and specific linguistic problems, and complements flan.&lt;/p>; &lt;p>;I believe that it can be a valuable contributions to current open source LLM.&lt;/p>; &lt;p>;I would be glad to know what you think, thank you.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jean-Porte&quot;>; /u/Jean-Porte &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qowlg </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/"/><updated> 2023-05-24T15:33:03+00:00</updated><published> 2023-05-24T15:33:03+00:00</published><title> [R] tasksource-instruct: an open source instruction-tuning dataset focused on classification, with many tasks not in flan.</title></entry><entry><author><name> /u/rockorocko47</name><uri> https://www.reddit.com/user/rockorocko47 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;I was never planning on working for a healthcare company but these guys just made me kinda fall in love with them and I really wanna give it a hard try.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/rockorocko47&quot;>; /u/rockorocko47 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rf0d2/d_please_suggest_me_some_portfolio_project_ideas/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rf0d2/d_please_suggest_me_some_portfolio_project_ideas/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13rf0d2 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rf0d2/d_please_suggest_me_some_portfolio_project_ideas/"/><updated> 2023-05-25T11:19:10+00:00</updated><published> 2023-05-25T11:19:10+00:00</published><title> [D] Please suggest me some portfolio project ideas that will land me a Data scientist interview for sure at a healthcare provider company.</title></entry><entry><author><name> /u/herbiebradley</name><uri> https://www.reddit.com/user/herbiebradley </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi all,&lt;/p>; &lt;p>;We at CarperAI have developed a new technique called &lt;a href=&quot;https://carper.ai/quality-diversity-through-ai-feedback/&quot;>;Quality-Diversity with AI Feedback (QDAIF)&lt;/a>;, combining large language models and evolutionary algorithms to generate diverse and high-quality natural language text.&lt;/p>; &lt;p>;QDAIF is all using LMs to provide quality and diversity evaluations, which we use as feedback to optimize a search process which explores the space of text generations from LMs.&lt;/p>; &lt;p>;We use the evolutionary algorithm &lt;a href=&quot;https://arxiv.org/abs/1504.04909&quot;>;MAP-Elites&lt;/a>;, in which a grid defined by our diversity dimensions is populated with increasingly high quality texts generated by our LM evolution operator.&lt;/p>; &lt;p>;QDAIF can improve on some of the limitations of current QD algorithms which often require hand-coded measures of diversity &amp;amp; quality, and can help generate fine-tuning data to help a model improve. We think this highlights the potential to build powerful search algorithms through LM feedback that can explore and refine diverse solutions to nuanced qualitative problems.&lt;/p>; &lt;p>;Blog post: &lt;a href=&quot;https://carper.ai/quality-diversity-through-ai-feedback/&quot;>;https://carper.ai/quality-diversity-through-ai-feedback/&lt;/a>;&lt;/p>; &lt;p>;This was a collaboration with &lt;a href=&quot;https://www.aleph-alpha.com/&quot;>;Aleph Alpha&lt;/a>;, &lt;a href=&quot;https://twitter.com/jennyzhangzt&quot;>;Jenny Zhang&lt;/a>;, &lt;a href=&quot;https://twitter.com/jeffclune&quot;>;Jeff Clune&lt;/a>;, and &lt;a href=&quot;https://twitter.com/kenneth0stanley&quot;>;Ken Stanley&lt;/a>;!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/herbiebradley&quot;>; /u/herbiebradley &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r1j7a </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/"/><updated> 2023-05-24T23:31:32+00:00</updated><published> 2023-05-24T23:31:32+00:00</published><title> [P] Quality-Diversity with AI Feedback</title></entry><entry><author><name> /u/adeel06</name><uri> https://www.reddit.com/user/adeel06 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Have a way to have the compute resources to rival the largest players in the game already. &lt;/p>; &lt;p>;Do you believe you have what it takes to assist a division in a team developing an LLM?&lt;/p>; &lt;p>;Not sure if Reddit would be the best place to find talent, but I&#39;ve been surprised before.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/adeel06&quot;>; /u/adeel06 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rch84/p_interested_in_working_on_creating_an_llm/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rch84/p_interested_in_working_on_creating_an_llm/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13rch84 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rch84/p_interested_in_working_on_creating_an_llm/"/><updated> 2023-05-25T09:01:39+00:00</updated><published> 2023-05-25T09:01:39+00:00</published><title> [P] Interested in working on creating an LLM?</title></entry><entry><author><name> /u/qooooob</name><uri> https://www.reddit.com/user/qooooob </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;I&amp;#39;m collecting a dataset from documents which are essentially scanned papers with text and tables within them. Sometimes the question is best answered by detecting, parsing and cleaning the table data (eg with AWS Textract + post-processing), but other times it would be beneficial to use the raw text from OCR. For LLMs I&amp;#39;ve been using just the OCR output as context to answer the question, but information in tables is lost.&lt;/p>; &lt;p>;I can see LLMs struggle answering questions especially when part of the context of the answer originates from tabular data, since OCR just parses that as a string of words separated by &lt;code>;\n&lt;/code>; and the table structure is lost in the process. &lt;/p>; &lt;p>;A document could look like this:&lt;/p>; &lt;blockquote>; &lt;p>;Here is a table consisting of answers. &lt;/p>; &lt;p>;As we can see a large part of increase in cost of &lt;/p>; &lt;p>;living can be attributed to increased rent. [...]&lt;/p>; &lt;/blockquote>; &lt;table>;&lt;thead>; &lt;tr>; &lt;th align=&quot;left&quot;>;&lt;/th>; &lt;th align=&quot;left&quot;>;30.6.2021&lt;/th>; &lt;th align=&quot;left&quot;>;30.6.2020&lt;/th>; &lt;/tr>; &lt;/thead>;&lt;tbody>; &lt;tr>; &lt;td align=&quot;left&quot;>;Cost of living&lt;/td>; &lt;td align=&quot;left&quot;>;5 021,55&lt;/td>; &lt;td align=&quot;left&quot;>;4 921,31&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Apartment&lt;/td>; &lt;td align=&quot;left&quot;>;2 421,56&lt;/td>; &lt;td align=&quot;left&quot;>;2 200,60&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Cost of food&lt;/td>; &lt;td align=&quot;left&quot;>;&lt;/td>; &lt;td align=&quot;left&quot;>;400,00&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Electricity&lt;/td>; &lt;td align=&quot;left&quot;>;B00,00&lt;/td>; &lt;td align=&quot;left&quot;>;799,00&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;in OCR this could look like &lt;/p>; &lt;pre>;&lt;code>;Here is a table consisting of answers. As we can see a large part of increase in cost of living can be attributed to increased rent. [...] 30.6.2021 30.6.2020 Cost of living 5 021,55 4 921,31 Apartment 2 421,56 2 200,60 Cost of food 400,00 Electricity B00,00 799,00 &lt;/code>;&lt;/pre>; &lt;p>;So basically the context from the table is lost and eg for &lt;code>;Cost of food&lt;/code>; it&amp;#39;s impossible to know whether the figure is from 2020 or 2021. Intuitively I think it would be beneficial for the LLM to see the data in the order it appears so that data in tables is somehow structured in the text. So that the output would look like this instead&lt;/p>; &lt;pre>;&lt;code>;Here is a table consisting of answers. As we can see a large part of increase in cost of living can be attributed to increased rent. [...] Cost of living (30.6.2021): 5 021,55 Cost of living (30.6.2020): 4 921,31 Apartment (30.6.2021): 2 421,56 Apartment (30.6.2020): 2 200,60 Cost of food (30.6.2021): No data Cost of food (30.6.2020): 400,00 &amp;lt;continued...&amp;gt; &lt;/code>;&lt;/pre>; &lt;p>;First of all I don&amp;#39;t know if this is necessary, or if there is a better approach to sending documents that contain both text/tabular data to LLMs. I have looked into libraries such as &lt;code>;unstructured&lt;/code>; that can return the layout of the document and the table data within it as HTML using &lt;code>;detectron2&lt;/code>;, which could be then parsed into something that looks like the above example, but I&amp;#39;m not very pleased with the quality of the table detection and it is quite slow. Also I imagine this library tries to fit many more use cases than what I need - essentially text and tabular text in different forms (lists, tables, borderless tables). At the moment I&amp;#39;m using AWS textract for table detection which works great but I&amp;#39;d like to move away from it an create my own model that is optimized for my use case and that is free.&lt;/p>; &lt;p>;Currently I&amp;#39;m thinking about creating a pipeline where I train a custom table detection model on my own dataset&lt;/p>; &lt;p>;-&amp;gt; Turn PDF page to an image&lt;br/>; -&amp;gt; Detect location of tables with a model like Table Transformer (TATR)&lt;br/>; -&amp;gt; Collect and remove table from image&lt;br/>; -&amp;gt; Run regular OCR on image with only text and no tables and&lt;br/>; -&amp;gt; Run a model for table recognition/extraction like AWS textract or TATR to extract tabular data&lt;br/>; -&amp;gt; Turn table data into structured text data&lt;br/>; -&amp;gt; Join the text and table datas in order of appearance to create one long text document with all the info of the PDF.&lt;/p>; &lt;p>;Any feedback or suggestions on this? Also is Microsoft&amp;#39;s Table Transformer a smart model to fine-tune with my own data, or are there others that perform better?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/qooooob&quot;>; /u/qooooob &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qhdmz </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/"/><updated> 2023-05-24T10:10:37+00:00</updated><published> 2023-05-24T10:10:37+00:00</published><title> [D] Exctracting from documents that consist of text and tabular data for use with LLMs</title></entry><entry><author><name> /u/baqirjafari</name><uri> https://www.reddit.com/user/baqirjafari </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi everyone,&lt;/p>; &lt;p>;I am working on a case study that requires a multilingual embedding model. I did some research and found out that &lt;a href=&quot;https://www.sbert.net/docs/pretrained_models.html#model-overview&quot;>;paraphrase-multilingual-mpnet-base-v2&lt;/a>; is a good option. However, I am wondering if there is a better model that can handle languages like English, Urdu, Persian, Arabic, etc. Does anyone have any suggestions or experiences with other multilingual embedding models? I would appreciate any help or advice. Thank you very much!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/baqirjafari&quot;>; /u/baqirjafari &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r5x6s </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/"/><updated> 2023-05-25T02:53:08+00:00</updated><published> 2023-05-25T02:53:08+00:00</published><title> [D] Looking for a better multilingual embedding model</title></entry><entry><author><name> /u/Cold_Cantaloupe9212</name><uri> https://www.reddit.com/user/Cold_Cantaloupe9212 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;I am interested in developing a conditional diffusion model that guarantees consistent outputs for a given input. I would like to reduce or remove the stochasticity in the model to achieve this goal. Is there a way to accomplish this while maintaining some level of variability?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Cold_Cantaloupe9212&quot;>; /u/Cold_Cantaloupe9212 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r3qao </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/"/><updated> 2023-05-25T01:10:06+00:00</updated><published> 2023-05-25T01:10:06+00:00</published><title> [D]eterministic diffusion models</title></entry></feed>