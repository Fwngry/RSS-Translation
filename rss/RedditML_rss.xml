<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-25T20:15:25+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /u/I_will_delete_myself</name><uri> https://www.reddit.com/user/I_will_delete_myself </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我犹豫了一会儿，但听到这个消息后虚伪让我发疯。&lt;/p>; &lt;p>;SMH 这家公司就像白衣骑士一样，他们认为他们凌驾于所有人之上。他们想要监管，但他们希望不受该监管的影响。只想伤害其他人，但不想伤害“全能的”Sam 和朋友。&lt;/p>; &lt;p>;向国会撒谎说建议在欧盟采取类似的做法，但现在开始抱怨他们。在任何政治领域都不应该认真对待这个家伙。&lt;/p>; &lt;p>;我的观点是，这家公司通过锁定与其品牌名称相悖的东西来反对 AI 进步。如果他们甚至不能忠于这样简单的事情，我们怎么能指望他们忠于更难的 AI 安全？&lt;/p>; &lt;p>;我很高兴他们现在改变了立场，但我很高兴他们如何他们认为他们有权为了自己的利益而腐败。 SMH!!!!!!!!&lt;/p>; &lt;p>;你有什么想法？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/I_will_delete_myself&quot;>; /u/I_will_delete_myself &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rie0e </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/"/><updated> 2023-05-25T13:51:58+00:00</updated><published> 2023-05-25T13:51:58+00:00</published><title> OpenAI 现在抱怨人工智能的监管 [D]</title></entry><entry><author><name> /u/Ok_Bank_2217</name><uri> https://www.reddit.com/user/Ok_Bank_2217 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们需要为我们的平台获取大量 YouTube 数据并训练自定义 ML 模型，但除了 YouTube 之外找不到任何有用的东西8M Dataset，相当过时，信息非常有限。官方的 YouTube 数据 API 也被限制在大约 10.000 个积分，这远远不能满足我们需要的数量。&lt;/p>; &lt;p>;这就是为什么我们说去他妈的，并决定自己构建一个巨大的 YouTube 数据集。在为超过 1 亿个视频编制索引并构建自定义 API 来访问它之后，我们决定公开 API 并允许人们购买访问权限！&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.blizzy -data.com/&quot;>;链接到网站&lt;/a>;&lt;/p>; &lt;p>;我们很乐意听到我们的 ML 工程师和数据科学家的反馈，并希望解决您和我们遇到的问题!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Ok_Bank_2217&quot;>; /u/Ok_Bank_2217 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rh9yj </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/"/><updated> 2023-05-25T13:04:43+00:00</updated><published> 2023-05-25T13:04:43+00:00</published><title> [P] 我们创建了一个大型 YouTube 视频数据集来替换 YouTube 数据 API</title></entry><entry><author><name> /u/我是布兰妮</name><uri>https://www.reddit.com/user/ISpearedBritney </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你的很多工作都涉及 AI 或 ML（无论标题如何），你能分享一下你的典型工作日是怎样的吗？您将时间花在什么上，最终经常使用哪些工具或资源？其中有多少是数据争论，你使用了多少数学？谢谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ISpearedBritney&quot;>; /u/ISpearedBritney &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rct07 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/"/><updated> 2023-05-25T09:21:06+00:00</updated><published> 2023-05-25T09:21:06+00:00</published><title> [D] 对于那些从事 ML/AI 工作的人，您的工作和工作日是什么样的？</title></entry><entry><author><name> /u/奇异语2501</name><uri> https://www.reddit.com/user/Singularian2501 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>; &lt;img src=&quot;https://a.thumbs.redditmedia .com/pwokgMFTSRP9bRbBlTbOA1gPSTlPoECDQdwoNNPMuG0.jpg&quot; alt=&quot;[R] Gorilla：连接大量 API 的大型语言模型 - Microsoft Research 2023 - 在编写 API 调用方面超越 GPT-4 的性能。 title=&quot;[R] Gorilla：连接大量 API 的大型语言模型 - Microsoft Research 2023 - 在编写 API 调用方面超越 GPT-4 的性能。&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.15334 &quot;>;https://arxiv.org/abs/2305.15334&lt;/a>; &lt;/p>; &lt;p>;Github：&lt;a href=&quot;https://github.com/ShishirPatil/gorilla&quot;>;https://github。 com/ShishirPatil/gorilla&lt;/a>; &lt;/p>; &lt;p>;博客：&lt;a href=&quot;https://gorilla.cs.berkeley.edu/&quot;>;https://gorilla.cs.berkeley.edu/&lt; /a>; &lt;/p>; &lt;p>;摘要：&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 最近出现了令人印象深刻的进步浪潮，模型现在在各种任务中表现出色，例如数学推理和程序综合。然而，它们通过 API 调用有效使用工具的潜力仍未实现。即使对于当今最先进的 LLM（例如 GPT-4）而言，这也是一项具有挑战性的任务，这主要是因为它们无法生成准确的输入参数，并且它们倾向于产生错误的 API 调用用法。我们发布了 Gorilla，这是一种经过微调的基于 LLaMA 的模型，在编写 API 调用方面超越了 GPT-4 的性能。当与文档检索器结合使用时，Gorilla 展示了适应测试时文档更改的强大能力，支持灵活的用户更新或版本更改。 &lt;strong>;它还大大减轻了直接提示 LLM 时经常遇到的幻觉问题。&lt;/strong>;为了评估模型的能力，我们引入了 APIBench，这是一个由 HuggingFace、TorchHub 和 TensorHub API 组成的综合数据集。 &lt;strong>;检索系统与 Gorilla 的成功集成表明 LLM 有潜力更准确地使用工具，跟上经常更新的文档，从而提高其输出的可靠性和适用性。&lt;/strong>;&lt;/p>; &lt; /blockquote>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/n5ezjchbg12b1.jpg?width=872&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eb5b7e11a22abe59d49504fad7278006a2b878a6&quot;>;https://preview.redd。它/n5ezjchbg12b1.jpg?width=872&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eb5b7e11a22abe59d49504fad7278006a2b878a6&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/e2xhpfhbg12b1.jpg ?width=1075&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b3c0f6ed7a6d72c93e681266977a0ec0f129ba6d&quot;>;https://preview.redd.it/e2xhpfhbg12b1.jpg?width=1075&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b 3c0f6ed7a6d72c93e681266977a0ec0f129ba6d&lt;/a >;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/i7i7bfhbg12b1.jpg?width=1213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5a287aba81199b66d1334457c6e8a12b3b5881c0&quot;>;https://预览。 redd.it/i7i7bfhbg12b1.jpg?width=1213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5a287aba81199b66d1334457c6e8a12b3b5881c0&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Singularian2501&quot;>; /u/Singularian2501 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13rl3v9 </id><media:thumbnail url="https://a.thumbs.redditmedia.com/pwokgMFTSRP9bRbBlTbOA1gPSTlPoECDQdwoNNPMuG0.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/"/><updated> 2023-05-25T15:42:26+00:00</updated><published> 2023-05-25T15:42:26+00:00</published><title> [R] Gorilla: Large Language Model Connected with Massive APIs - Microsoft Research 2023 - 在编写 API 调用方面超越了 GPT-4 的性能。</title></entry><entry><author><name> /u/恩里科希波尔</name><uri>https://www.reddit.com/user/EnricoShippole </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;很高兴发布 FLAN V2 数据集的开源复制品。&lt;/p>; &lt;p>;可以在此处找到完整的数据集：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/FLAN_2022&quot;>;https://huggingface.co/datasets/conceptofmind/FLAN_2022&lt;/a>;&lt;/p>; &lt;p>;我和主要作者 Shayne Longpre 一起工作FLAN 合集重现他的伟大作品并公开发布高质量的指令调优数据。我们修复了编码问题并将序列长度增加到 4096：&lt;a href=&quot;https://twitter.com/EnricoShippole/status/1661756166248996867?s=20&quot;>;https://twitter.com/EnricoShippole/status/1661756166248996867 ?s=20&lt;/a>;&lt;/p>; &lt;p>;每个单独的子混音也可以在 huggingface 上下载。子混音是 T0、FLAN2021、CoT、NIv2 和 Dialog。每个都包含相关的元数据，例如输入、目标、任务源、任务名称和模板类型。&lt;/p>; &lt;p>;T0 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/t0_submix_original&quot;>; https://huggingface.co/datasets/conceptofmind/t0_submix_original&lt;/a>;&lt;/p>; &lt;p>;Flan2021 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/flan2021_submix_original&quot;>;https:/ /huggingface.co/datasets/conceptofmind/flan2021_submix_original&lt;/a>;&lt;/p>; &lt;p>;CoT 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/cot_submix_original&quot;>;https://huggingface. co/datasets/conceptofmind/cot_submix_original&lt;/a>;&lt;/p>; &lt;p>;NIv2 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/niv2_submix_original&quot;>;https://huggingface.co/datasets /conceptofmind/niv2_submix_original&lt;/a>;&lt;/p>; &lt;p>;对话子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/dialog_submix_original&quot;>;https://huggingface.co/datasets/conceptofmind/ dialog_submix_original&lt;/a>;&lt;/p>; &lt;p>;您可以在这里找到原始的 FLAN 存储库和 Shayne Longpre 的所有令人难以置信的工作：&lt;a href=&quot;https://github.com/google-research/FLAN /tree/main/flan/v2#download&quot;>;https://github.com/google-research/FLAN/tree/main/flan/v2#download&lt;/a>;&lt;/p>; &lt;p>;一定要也通读 Shayne 关于 FLAN 集合的论文，以更好地了解数据是如何创建的：&lt;a href=&quot;https://arxiv.org/abs/2301.13688&quot;>;https://arxiv.org/abs /2301.13688&lt;/a>;&lt;/p>; &lt;p>;我们将很快发布一个包含数百 GB 高质量指令数据的大规模因果语言建模数据集。在不久的将来留意该版本。&lt;/p>; &lt;p>;我们在 FLAN V2 和相关项目的开放复制方面的工作都归功于 CarperAI 和 StabilityAI 的慷慨赞助。&lt;/p>; &lt;p>;你可以在此处了解有关 CarperAI 的更多信息：&lt;a href=&quot;https://carper.ai/&quot;>;https://carper.ai/&lt;/a>;&lt;/p>; &lt;p>;在此处了解 StabilityAI：&lt;a href=&quot; https://stability.ai/&quot;>;https://stability.ai/&lt;/a>;&lt;/p>; &lt;p>;非常感谢 Jason Phang 和 Fabrizio Milo 帮助构建数据集。&lt;/p >; &lt;p>;你可以在这里找到 Jason Phang 的推特：&lt;a href=&quot;https://twitter.com/zhansheng&quot;>;https://twitter.com/zhansheng&lt;/​​a>;&lt;/p>; &lt; p>;Fabrizio Milo 在这里：&lt;a href=&quot;https://twitter.com/fabmilo&quot;>;https://twitter.com/fabmilo&lt;/a>;&lt;/p>; &lt;p>;您可以查看在此处查看 Shayne 关于构建预训练数据集的新论文：&lt;a href=&quot;https://github.com/shayne-longpre/a-pretrainers-guide/blob/main/A%20Pretrainer&amp;#x27;s %20Guide%20To%20Training%20Data.pdf&quot;>;https://github.com/shayne-longpre/a-pretrainers-guide/blob/main/A%20Pretrainer&amp;#39;s%20Guide%20To%20Training%20Data。 pdf&lt;/a>;&lt;/p>; &lt;p>;这不是 Google 或 StabilityAI 的官方产品。&lt;/p>; &lt;p>;如果您对数据有任何疑问，请务必联系我们并询问！我会尽量及时回复：&lt;a href=&quot;https://twitter.com/EnricoShippole&quot;>;https://twitter.com/EnricoShippole&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON - ->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/EnricoShippole&quot;>; /u/EnricoShippole &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rmqx5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/"/><updated> 2023-05-25T16:46:42+00:00</updated><published> 2023-05-25T16:46:42+00:00</published><title> [P] FLAN V2 数据集的开源再现</title></entry><entry><author><name>/u/奇异语2501</name><uri> https://www.reddit.com/user/Singularian2501 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/huTwcaqVaNrBZzikekNSL9XJi3tfEGZsggLCg50LMYM.jpg&quot; alt=&quot;[R] 用语言模型推理就是用世界模型进行规划 - Shibo Hao 等加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，计划相对改进了 33%世代设定！” title=&quot;[R] 用语言模型推理就是用世界模型进行规划 - Shibo Hao 等加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，在计划生成设置中相对改进了 33%！” />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.14992 &quot;>;https://arxiv.org/abs/2305.14992&lt;/a>; &lt;/p>; &lt;p>;摘要：&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 已显示出非凡的推理能力，尤其是当提示生成中间推理步骤（例如，Chain-of-Thought，CoT）。然而，LLM 仍然难以解决对人类来说很容易的问题，例如为在给定环境中执行任务生成行动计划，或者执行复杂的数学、逻辑和常识推理。缺陷源于一个关键事实，即 LLM 缺乏内部&lt;em>;世界模型&lt;/em>;来预测世界&lt;em>;状态&lt;/em>;（例如，环境状态、中间变量值）并模拟长期结果动作。这可以防止 LLM 执行类似于人脑的深思熟虑的计划，这涉及探索替代推理路径、预测未来状态和奖励，以及迭代改进现有推理步骤。为了克服这些限制，我们提出了一个新的 LLM 推理框架，&lt;strong>;&lt;em>;R&lt;/em>;&lt;/strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;easoning via&lt;/em>; /strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;P&lt;/em>;&lt;/strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;规划&lt;/em>;&lt;/strong >; &lt;strong>;(RAP).&lt;/strong>; RAP 将 &lt;strong>;LLM 重新定位为世界模型和推理代理，并结合原则性规划算法（基于蒙托卡罗树搜索）在广阔的推理中进行战略探索&lt;/strong>; 在推理过程中，LLM（作为代理）在LLM（作为世界模型）和任务特定奖励的指导下，增量构建推理树，并在适当的平衡下高效地获得高奖励推理路径在探索 &lt;em>; 与 &lt;/em>; 开发之间。我们将 RAP 应用于各种具有挑战性的推理问题，包括计划生成、数学推理和逻辑推理。这些任务的实证结果证明了 RAP 优于各种强大的基线，包括 CoT 和自洽性从最少到最多的提示。 &lt;strong>;LLAMA-33B 上的 RAP 超过了 GPT-4 上的 CoT，计划生成设置相对改进了 33%。&lt;/strong>;&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;a href=&quot;https://preview .redd.it/jaoiil2mc12b1.jpg?width=747&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53&quot;>;https://preview.redd.it/jaoiil2mc12b1.jpg?width=747&amp;amp;format= pjpg&amp;auto= webp&amp;amp;s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7389d75d0ff7 d1d8787c7c5f9add4787b02b47be &quot;>;https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7389d75d0ff7d1d8787c7c5f9add4787b02b47be&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https:/ /preview.redd.it/ykpqvp2mc12b1.jpg?width=980&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=834c39fb3e549418b8396725e86ddff3c6584077&quot;>;https://preview.redd.it/ykpqvp2mc12b1.jpg?width=9 80&amp;amp;格式=pjpg&amp;amp; auto=webp&amp;amp;s=834c39fb3e549418b8396725e86ddff3c6584077&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s =9c5192d6c012bfe4390fa67b010580b8e4508daa&quot;>;https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9c5192d6c012bfe4390fa67b010580 b8e4508daa&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https //preview.redd.it/qd8pjo2mc12b1.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea&quot;>;https://preview.redd.it/qd8pjo2mc12b1.jpg?width=140 0&amp;格式= pjpg&amp;amp;auto=webp&amp;amp;s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Singularian2501&quot;>; /u/Singularian2501 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13rkhzx </id><media:thumbnail url="https://b.thumbs.redditmedia.com/huTwcaqVaNrBZzikekNSL9XJi3tfEGZsggLCg50LMYM.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/"/><updated> 2023-05-25T15:17:59+00:00</updated><published> 2023-05-25T15:17:59+00:00</published><title> [R] 使用语言模型进行推理就是使用世界模型进行规划 - Shibo Hao 等人，加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，计划生成设置相对改进了 33%！</title></entry><entry><author><name> /u/铁叶神经元</name><uri>https://www.reddit.com/user/tiedyeneuron </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;诚然，我在标题问题的措辞上略显幼稚和片面，以引发讨论。我认为从事深度学习研究的学术实验室有一定的优势。然而，许多重大突破现在似乎确实发生在工业实验室，而不是小型大学实验室。这可能是由于 DL 从新兴研究领域成熟为工业技术。&lt;/p>; &lt;p>;鉴于 DL 的最新发展，人们对在工业中进行深度学习研究的相对优点有何看法与学术界？例如，如果有人可以选择在顶级学术实验室（如麻省理工学院、斯坦福大学、加州大学伯克利分校等）担任研究员或加入 OpenAI/Anthropic/DeepMind 等，他们为什么要选择学术路径？&lt;/p >; &lt;p>;我理解有些人可能出于成为教授的愿望而选择学术界，但似乎越来越多的顶尖大学乐于让行业研究人员担任客座教授或担任兼职教授。许多行业科学家也接受实习生，因此他们仍然可以充当导师，就像他们是学术实验室的 PI 一样。仍然，显然留在 AI 学术界仍然有一些独特的价值，因为我能想到许多选择这样做的顶级研究人员。我很想知道人们认为与行业实验室相比有什么好处。&lt;/p>; &lt;p>;（我知道这是一个与职业相关的帖子，但它看起来不像 &lt;a href=&quot;https:// www.reddit.com/r/cscareerquestions/&quot;>;r/cscareerquestions&lt;/a>; 拥有合适的受众或专业知识来推动这一讨论。此外，我认为目前这一讨论非常针对跨行业/学术界的 ML 社区及时。）&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/tiedyeneuron&quot;>; /u/tiedyeneuron &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rl1q6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/"/><updated> 2023-05-25T15:40:05+00:00</updated><published> 2023-05-25T15:40:05+00:00</published><title> [D] 鉴于深度学习方法的扩展，作为 AI 研究人员留在学术界的剩余优点是什么？</title></entry><entry><author><name> /u/钢铁侠马克20</name><uri> https://www.reddit.com/user/IronManMark20 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/IronManMark20&quot;>; /u/IronManMark20 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://gorilla.cs.berkeley. edu/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rpvgn/gorilla_large_language_model_connected_with/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rpvgn </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rpvgn/gorilla_large_language_model_connected_with/"/><updated> 2023-05-25T18:49:35+00:00</updated><published> 2023-05-25T18:49:35+00:00</published><title> Gorilla：连接大量 API 的大型语言模型</title></entry><entry><author><name>/u/真实RJT</name><uri> https://www.reddit.com/user/RealRJT </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我的两个朋友正在从事一个文本到 3D 的项目，重点是 3D 打印。他们以 DreamFusion 等近期工作为基础，但优化了 3D 打印的输出，并取得了一些很酷的进步。他们正在寻找 Beta 测试人员来提供反馈并改进基础模型 - 您只需要人工反馈 :)。&lt;/p>; &lt;p>;您可以在此处查看他们的工作：&lt;a href=&quot;https://www.kirin3d.com&quot;>;www.kirin3d.com&lt;/a>;。非常欢迎提供反馈 - 特别是如果您是具有 3D 打印经验的爱好者！&lt;/p>; &lt;p>;谢谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;# 32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/RealRJT&quot;>; /u/RealRJT &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rngbp/project_kirin_text_to_reality/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rngbp/project_kirin_text_to_reality/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rngbp </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rngbp/project_kirin_text_to_reality/"/><updated> 2023-05-25T17:14:13+00:00</updated><published> 2023-05-25T17:14:13+00:00</published><title> [项目] 麒麟：文字转现实</title></entry><entry><author><name>/你/米勒</name><uri>https://www.reddit.com/user/mierle </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;QLoRA: Effi量化 LLM 的 cient Finetuning&quot; title=&quot;QLoRA：量化 LLM 的高效微调&quot; />; &lt; /a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mierle&quot;>; /u/mierle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14314&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13r1hkg </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/"/><updated> 2023-05-24T23:29:47+00:00</updated><published> 2023-05-24T23:29:47+00:00</published><title> QLoRA：量化 LLM 的高效微调</title></entry><entry><author><name>/u/天眼2006</name><uri> https://www.reddit.com/user/dayeye2006 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我以前是一名机器学习工程师。但我已经多年没有接触 Pytorch（我在自己的初创公司工作，作为一名全栈工程师）。&lt;/p>; &lt;p>;有哪些好的资源可以刷新我的 PyTorch 技能？&lt;/p>; &lt;p>;我喜欢以“愚蠢的方式”学习东西。我计划从头开始实现一些最经典的模型（ResNet、TextCNN、transformers，...）。&lt;/p>; &lt;p>;当我学习编程语言时，我最喜欢参考的资源是 &lt;a href=&quot;https://github.com/topics/koans&quot;>;公案&lt;/a>;。这有助于我快速熟悉新语言。深度学习界有对应的吗？&lt;/p>; &lt;p>;谢谢&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/dayeye2006&quot;>; /u/dayeye2006 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13royi6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/"/><updated> 2023-05-25T18:13:41+00:00</updated><published> 2023-05-25T18:13:41+00:00</published><title> [D] 有哪些资源可以提高我的 PyTorch 技能？</title></entry><entry><author><name> /u/_negativeonetwelfth</name><uri> https://www.reddit.com/user/_negativeonetwelfth </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;在从许多不同的来源阅读了这些算法的实现之后，我仍然看到了关于此的相互矛盾的信息。一些消息来源说（或暗示）你可以获得更高的帧率，因为你可以更少地运行深度学习对象检测器，并连续几帧使用卡尔曼滤波器预测框。另一方面，一些消息来源表明情况并非如此，因为过滤器仅用于根据先前位置预测当前（而非未来）位置，并且需要在每次迭代中使用深度学习检测进行更新。&lt; /p>; &lt;p>;我想知道是否有人对这些算法有经验并且能够提供一个真实而明确的答案。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/_negativeonetwelfth&quot;>; /u/_negativeonetwelfth &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rgy1g </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/"/><updated> 2023-05-25T12:50:26+00:00</updated><published> 2023-05-25T12:50:26+00:00</published><title> [D] 使用卡尔曼滤波器（如 SORT 和 DeepSORT）的跟踪算法是否会增加系统的帧率？</title></entry><entry><author><name> /u/弗兰克米勒MC</name><uri> https://www.reddit.com/user/FrankMillerMC </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://huggingface.co/tiiuae&quot;>;https://huggingface.co/tiiuae&lt;/a>;&lt;/ p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/FrankMillerMC&quot;>; /u/FrankMillerMC &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rizo0/new_large_language_model_for_use_commercial/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rizo0/new_large_language_model_for_use_commercial/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rizo0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rizo0/new_large_language_model_for_use_commercial/"/><updated> 2023-05-25T14:16:29+00:00</updated><published> 2023-05-25T14:16:29+00:00</published><title>用于商业的新大型语言模型（开源）[N]</title></entry><entry><author><name> /u/内部-Industry758</name><uri> https://www.reddit.com/user/Internal-Industry758 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你在没有在顶级会议上发表任何论文的情况下完成了博士学位，你现在在做什么？你仍然觉得博士学位值得吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Internal-Industry758&quot;>; /u/Internal-Industry758 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rm0uf </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/"/><updated> 2023-05-25T16:18:09+00:00</updated><published> 2023-05-25T16:18:09+00:00</published><title> [D] 没有顶级出版物的博士：你现在在做什么？</title></entry><entry><author><name> /u/panthsdger</name><uri> https://www.reddit.com/user/panthsdger </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;全文：&lt;a href=&quot;https://arxiv.org/abs/2305.11252v1&quot;>;https://arxiv.org/abs /2305.11252v1&lt;/a>;&lt;/p>; &lt;p>;人工神经网络 (ANN) 已成为机器学习的重要工具，在图像和语音生成、游戏和机器人技术等多个领域取得了显著成功。然而，人工神经网络之间存在根本差异。操作机制和生物大脑的机制，特别是关于学习过程。本文全面回顾了人工神经网络中当前受大脑启发的学习表征。我们研究整合更多生物学上合理的机制，例如突触可塑性，以增强这些网络的功能。能力。此外，我们深入研究了伴随这种方法的潜在优势和挑战。最终，我们为这个快速发展的领域的未来研究指出了有前途的途径，这可以使我们更接近理解智能的本质。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/panthsdger&quot;>; /u/panthsdger &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rinw8 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/"/><updated> 2023-05-25T14:02:59+00:00</updated><published> 2023-05-25T14:02:59+00:00</published><title> [r] 人工神经网络中的类脑学习：综述</title></entry><entry><author><name>/u/双倍体</name><uri>https://www.reddit.com/user/Gaploid </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/SESB3xwR0W4B6ja9sC1v5aZbQ7rHU_rvGpMiqXUef5g.jpg&quot; alt=&quot;[P] 使用 GPT-4 自动从数据仪表板中提取见解&quot; title=&quot;[P] 使用 GPT-4 自动从数据仪表板中提取见解&quot; />; &lt;/a>; &lt; /td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;我们刚刚推出了一个新的 GPT-4 驱动的我们的数据分析平台的功能，并希望征求社区的意见。 &lt;/p>; &lt;p>;&lt;a href=&quot;https://i.redd.it/cb151k919z1b1.gif&quot;>;https://i.redd.it/cb151k919z1b1.gif&lt;/a>;&lt;/p>; &lt;p >;借助新功能，现在用户只需单击一下即可获得图表或仪表板上显示的数据的简单而全面的解释。 ChatGPT 无需任何特殊提示即可根据特定领域的知识生成适用的见解、解释甚至建议。这是可能的，因为我们开发了一种从图表中提取数据并将其以柱状格式传递到引擎盖下的提示的机制。这允许系统理解图表的上下文并使用深入分析所需的原始数据。&lt;/p>; &lt;p>;同时与您分享我们在开发新功能时发现的一些发现可能是对其他人有用：&lt;/p>; &lt;ol>; &lt;li>;提示的措辞至关重要；问题越具体，答案往往越准确。 &lt;strong>;所需语言、答案的最大长度和数据解释&lt;/strong>;等清晰的规范有助于改善结果。角色扮演或模拟专家也可以指导模型在特定知识领域内提供更详细的响应。&lt;/li>; &lt;li>;ChatGPT 擅长解析和&lt;strong>;处理表格数据&lt;/strong>;，包括 CSV。由于其紧凑性、准确性和可读性，我们选择这种格式将原始数据传输到模型。该模型甚至可以概念化来自此类表格的数据可以使用不同图表类型表示的方式，并可以使用这些可视化来解释数据。&lt;/li>; &lt;li>;值得注意的是，ChatGPT 似乎在挣扎具有&lt;strong>;大值和小数&lt;/strong>;的小数点。为了克服这个问题，我们将数字四舍五入到最多 2-3 位小数。这种做法不仅提高了准确性，而且减少了使用的令牌数量。&lt;/li>; &lt;/ol>; &lt;p>;我们想邀请大家试用我们的新功能并与我们分享您的想法。请点击&lt;a href=&quot;https://double.cloud/services/doublecloud-visualization/&quot;>;此链接访问&lt;/a>;我们的免费试用版——无需信用卡或 ChatGPT API 密钥。&lt;/p>; &lt;p >;我们很想听听您对我们开发的产品的看法。我们非常欢迎任何改进建议或潜在用途示例。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Gaploid&quot;>; /u/Gaploid &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13rhgt5 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/SESB3xwR0W4B6ja9sC1v5aZbQ7rHU_rvGpMiqXUef5g.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/"/><updated> 2023-05-25T13:12:40+00:00</updated><published> 2023-05-25T13:12:40+00:00</published><title> [P] 使用 GPT-4 自动从数据仪表板中提取见解</title></entry><entry><author><name>/u/Tomatomakko</name><uri> https://www.reddit.com/user/Tomatomakko </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;向量神经元 [&lt;a href=&quot;https://arxiv.org/pdf/2104.12229.pdf&quot;>;https://arxiv.org/ pdf/2104.12229.pdf&lt;/a>;] 是一种在 3D 点云处理网络中实现旋转等方差的方法。是否可以将相同的想法转移到 2D CNN？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Tomatomakko&quot;>; /u/Tomatomakko &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rfd15 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/"/><updated> 2023-05-25T11:36:16+00:00</updated><published> 2023-05-25T11:36:16+00:00</published><title> [D] 可以使用向量神经元在 2D CNN 中实现旋转等方差吗？</title></entry><entry><author><name> /u/威廉弗林奇博</name><uri>https://www.reddit.com/user/WilliamFlinchbaugh </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我正在尝试使用常规 bart-large 预训练模型进行文本摘要。我的代码非常适合 Pegasus，但当我切换到 BART 时，它会生成来自其他语言的随机符号和字符。这真的很奇怪，我还没有找到任何修复它的方法。输入数据不会导致这种情况。我真的无法在网上找到任何信息。&lt;/p>; &lt;p>;另外，我对数据进行了一些预处理以确保文本块的长度在 1024 个令牌以下，所以这不应该导致&lt;/p>; &lt;p>;生成摘要的代码：&lt;/p>; &lt;pre>;&lt;code>;model_name = &quot;facebook/bart-large&quot;; tokenizer = BartTokenizer.from_pretrained(model_name) model = BartForConditionalGeneration.from_pretrained(model_name) chunk = &quot;*在此处输入文本*&quot;; tokenized = tokenizer(chunk, truncation=True, padding=“longest”, return_tensors=“pt”, max_length=tokenizer.max_len_single_sentence)[&#39;input_ids&#39;] generated = model.generate(tokenized, max_length= 256) decoded = tokenizer.decode(generated.squeeze(), skip_special_tokens=True) &lt;/code>;&lt;/pre>; &lt;p>;我的一个输出看起来像这样：&lt;/p>; &lt;pre>;&lt;code>;nihc # 981 -40-48。 ------------------------------------------ ---------- dob �︎︎━━━┻━━━━━━━━╣━━━ﻺ━━⻺╣╣┻────────━━━ ╢ ━━═━━ - ─━━═━━━━△ớ┻╣໛╣⻄⻄╣资源┺╢╣═━╕╣┻━────────╣────━──────━━╗╣─━╔╣㻚──╣մ╣══╣░╛━╚ ╢┻ ┻╕_╟╣↓╛╔┻К &lt;/code>;&lt;/pre>; &lt;p>;如果有人能提供帮助，我将不胜感激！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/WilliamFlinchbaugh&quot;>; /u/WilliamFlinchbaugh &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rqxur/p_bart_giving_random_characters_as_output/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rqxur/p_bart_giving_random_characters_as_output/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rqxur </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rqxur/p_bart_giving_random_characters_as_output/"/><updated> 2023-05-25T19:31:46+00:00</updated><published> 2023-05-25T19:31:46+00:00</published><title> [P] Bart 给出随机字符作为输出</title></entry><entry><author><name>/你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/transforming-youtube-shorts-google-deepminds-flamingo-reinvents -metadata-for-maximum-impact-f817e1141dde&quot;>;https://medium.com/@tiago-mesquita/transforming-youtube-shorts-google-deepminds-flamingo-reinvents-metadata-for-maximum-impact-f817e1141dde&lt;/ a>;&lt;br/>; ‍&lt;br/>; Google 的人工智能研究部门 DeepMind 最近与 Google Brain 合并，组成了一个专注于推进人工智能技术的强大团队。&lt;/p>; &lt;p>;他们的最新项目 Flamingo 是一种视觉语言模型 (VLM)，它被用于通过生成自动和准确的视频描述来提高 YouTube Shorts 的可发现性。&lt;/p>; &lt;p>;YouTube 短片创作者通常优先考虑快速制作而不是创建有用的标题，而 Flamingo 旨在解决&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl49e/n_google_deepminds_flamingo_is_focusing_on/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl49e/n_google_deepminds_flamingo_is_focusing_on/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rl49e </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rl49e/n_google_deepminds_flamingo_is_focusing_on/"/><updated> 2023-05-25T15:42:54+00:00</updated><published> 2023-05-25T15:42:54+00:00</published><title> [N] Google DeepMind 的 Flamingo 专注于改进 YouTube 短片的描述以提高可发现性</title></entry><entry><author><name>/u/行星</name><uri>https://www.reddit.com/user/planetoryd </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;独裁政权（例如中国）一直在使用盲水印，以简单和隐写的方式，通过嵌入隐藏信息来迫害举报人/发起人在应用程序界面中。我不是专家，但我认为待办事项是：&lt;/p>; &lt;ul>; &lt;li>;用于局部盲水印去除的高效 ML 模型（或者，是否适合 ML） 去除（半）可见/盲水印，同时保留视觉/语义内容。&lt;/li>; &lt;li>;它的加速推理引擎，例如在 Rust 中。&lt;/li>; &lt;li>;开源移动和桌面应用程序界面。 （可能集成到现有的 EXIF 去除器工作流程中）&lt;/li>; &lt;/ul>; &lt;p>;现有方法包括拍照而不是屏幕截图。 （屏幕摄像头攻击）它可能不那么安全。 &lt;a href=&quot;https://ieeexplore.ieee.org/document/9136707&quot;>;paper1&lt;/a>; &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S1047320323000871&quot; >;paper2&lt;/a>;&lt;/p>; &lt;p>;它经常在中国异议 Reddit 社区中被提及。 （搜索 &lt;code>;reddit 盲水印&lt;/code>;）该技术也可能被出口。中国已经在与伊朗合作开发防火墙。我们需要做好准备。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/planetoryd&quot;>; /u/planetoryd &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rj2wp/d_a_call_to_implement_a_blind_watermark_removal/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rj2wp/d_a_call_to_implement_a_blind_watermark_removal/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rj2wp </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rj2wp/d_a_call_to_implement_a_blind_watermark_removal/"/><updated> 2023-05-25T14:20:08+00:00</updated><published> 2023-05-25T14:20:08+00:00</published><title> [D] 呼吁实施盲水印去除应用程序以捍卫公民自由。</title></entry><entry><author><name> /你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&quot; >;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qrtek </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/"/><updated> 2023-05-24T17:25:33+00:00</updated><published> 2023-05-24T17:25:33+00:00</published><title> [N] Andrej karpathy 在 MSBuild 2023 中的 GPT 状态</title></entry><entry><author><name>/u/环境-Bet-37</name><uri> https://www.reddit.com/user/Environmental-Bet-37 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨，我有一个包含 80k+ 行和 300+ 列的数据集。它是一个表格数据集，是一个回归问题。它需要历史数据和性能来预测结果。虽然原始特征只有大约 50-70 个，但我对这些特征有很好的理解，并且知道可以更好地分解它们以帮助算法（最有可能是 Xgboost）通过创建新特征来提供更好的结果。所以我做了很多特征创建、比率、乘法、^ 等等。我觉得可能有不重要的特征，但有一些组合可以真正帮助我的模型。&lt;/p>; &lt;p>;可能混合了线性和非线性特征，这些特征也具有相同的潜在概念。有很多方法，比如使用基于树的模型、特征重要性、神经网络、互信息、相关性、RFE、selectkbest、信息增益、前向和后向选择等等。 &lt;/p>; &lt;p>;但是，我很困惑，因为例如，树和神经网络有时会忽略很多更细微的特征，并寻找更明显的关系，甚至是非线性的，我们不必删除共线特征我们可能仍然会遇到相似特征会窃取彼此重要性的情况。 &lt;/p>; &lt;p>;所以，它有很多，我知道的很少。&lt;/p>; &lt;p>;但是，对于这样的数据集，我真的很想知道什么是最好的方法，你会怎样如何选择最佳特征？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Environmental-Bet-37&quot;>; /u/Environmental-Bet-37 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13rp9b2/feature_selection_from_300_features_discussion/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rp9b2/feature_selection_from_300_features_discussion/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rp9b2 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rp9b2/feature_selection_from_300_features_discussion/"/><updated> 2023-05-25T18:25:28+00:00</updated><published> 2023-05-25T18:25:28+00:00</published><title>从 300 多个特征中选择特征 [讨论]</title></entry><entry><author><name> /你/巴希博</name><uri>https://www.reddit.com/user/bahibo </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我一直在研究使用 LORA 微调 llms。许多像 huggingface 这样的网站声称它克服了灾难性遗忘，因为它冻结了原始模型的权重。但是在看LORA论文的时候，这个事实我不是很清楚，论文也没有提到这个。有人对此有更多见解吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/bahibo&quot;>; /u/bahibo &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rp5sa/d_does_lora_actually_mitigate_catastrophic/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rp5sa/d_does_lora_actually_mitigate_catastrophic/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rp5sa </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rp5sa/d_does_lora_actually_mitigate_catastrophic/"/><updated> 2023-05-25T18:21:34+00:00</updated><published> 2023-05-25T18:21:34+00:00</published><title> [D] 微调 llms 时，LORA 是否真的减轻了灾难性遗忘？</title></entry><entry><author><name> /u/太极官方</name><uri>https://www.reddit.com/user/TaichiOfficial </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/yR2V61hG-MR6-c-B7hm1M_8QJ7LEXpdvxPUZ7Gq9xFI.jpg&quot; alt=&quot;[P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP&quot; title=&quot;[P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP&quot; / >; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Taichi NeRF 使用神经辐射场实现高效的 3D 场景重建和新视点合成，同时提供基于 Python 的工作流程，用于即时 NGP 开发和移动设备上的轻松部署。&lt;/p>; &lt;p>;查看博客：&lt;a href=&quot;https://docs.taichi-lang.org/blog/taichi-instant- ngp&quot;>;https://docs.taichi-lang.org/blog/taichi-instant-ngp&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https ://i.redd.it/gymp61re7z1b1.gif&quot;>;https://i.redd.it/gymp61re7z1b1.gif&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/TaichiOfficial&quot;>; /u/TaichiOfficial &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13rbij5 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/yR2V61hG-MR6-c-B7hm1M_8QJ7LEXpdvxPUZ7Gq9xFI.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/"/><updated> 2023-05-25T08:03:13+00:00</updated><published> 2023-05-25T08:03:13+00:00</published><title> [P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP</title></entry><entry><author><name> /u/周杰伦</name><uri>https://www.reddit.com/user/JayCTee </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嘿伙计们，我正在做一个（假设的）项目，我想在大量非结构化客户数据上训练预训练的 LLM，以获得大公司。这个想法是它可以作为来自所有数据源的客户的知识库，可用于超个性化（例如产品匹配、内容生成等）。&lt;/p>; &lt;p>;我对训练部分——如果我想让它“学习”客户数据，我是使用微调、提示调优还是 RAG（我将微调理解为调整现有参数的权重，提示调优为添加更多参数，并且很难理解 RAG 是什么）。我看到一些消息来源说微调不能用于学习新数据？&lt;/p>; &lt;p>;任何人都可以就此过程或需要考虑的事项（例如数据敏感性）提供任何指示。我的在线搜索并不是那么富有成果&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/JayCTee&quot;>; /u/JayCTee &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rgmyt/p_uptraining_a_pretrained_model_using_company_data/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rgmyt/p_uptraining_a_pretrained_model_using_company_data/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rgmyt </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rgmyt/p_uptraining_a_pretrained_model_using_company_data/"/><updated> 2023-05-25T12:36:29+00:00</updated><published> 2023-05-25T12:36:29+00:00</published><title> [P] 使用公司数据训练预训练模型？</title></entry></feed>