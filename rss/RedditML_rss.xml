<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-29T16:19:08+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /你/极客酋长</name><uri>https://www.reddit.com/user/geekinchief </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ujsy7/n_nvidia_ace_brings_ai_to_game_characters_allows/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/sn7y0_ZzyL6A1Z0c8kbCVNhHxJMA4TVhtzWh4AEpcTU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b38cf19809b41a1f7a20ac618d26df9e9ed93614&quot; alt=&quot;[N] Nvidia ACE 带来s AI 到游戏角色，允许逼真的对话&quot; title=&quot;[N] Nvidia ACE Brings AI 到游戏角色，让对话栩栩如生&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/geekinchief&quot;>; /u/geekinchief &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.tomshardware.com/新闻/nvidia-ace-brings-npcs-to-life&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ujsy7/n_nvidia_ace_brings_ai_to_game_characters_allows/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13ujsy7 </id><media:thumbnail url="https://external-preview.redd.it/sn7y0_ZzyL6A1Z0c8kbCVNhHxJMA4TVhtzWh4AEpcTU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b38cf19809b41a1f7a20ac618d26df9e9ed93614"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13ujsy7/n_nvidia_ace_brings_ai_to_game_characters_allows/"/><updated> 2023-05-29T03:39:37+00:00</updated><published> 2023-05-29T03:39:37+00:00</published><title> [N] Nvidia ACE 为游戏角色带来 AI，让对话栩栩如生</title></entry><entry><author><name>/你/玛雅桑</name><uri>https://www.reddit.com/user/mayasang </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨，我最近在一家科技公司的机器学习面试中遇到了这个问题。假设我们构建了一个分类器来预测用户的行为。某些动作（例如，点击广告）。&lt;/p>; &lt;p>;(1) 我们如何评估这个模型（假设它是一个严重不平衡的数据集）&lt;/p>; &lt;p>;- 我提到我们可以使用 AUC 和归一化交叉熵。 （定义：每次展​​示的平均日志损失除以模型预测每次展示的背景点击率 (CTR) 时每次展示的平均日志损失将是多少 [1]）。&lt;/p>; &lt;p>;如下-up 问题，面试官问，&lt;/p>; &lt;p>;(2) 如果我们有两个模型：&lt;/p>; &lt;ul>; &lt;li>;模型 1 在未经采样的原始数据上训练：AUC1，logloss1 在 eval 数据上（非-sampled)&lt;/li>; &lt;li>;Model2 在 10% neg-downampled 数据上训练：AUC2，logloss2 在 eval 数据上（非采样）&lt;/li>; &lt;/ul>; &lt;p>;如果他们的 AUC1 == AUC2，并且logloss1 == logloss2，哪个指标表明模型更好？我们应该看哪个指标？哪个型号更好？ &lt;/p>; &lt;p>;我提到过，如果测试数据集没有被下采样，并且如果它们的 AUC 和交叉熵相同，那么这两个模型的结果是一样的。质量似乎是一样的。我不确定这是否是正确的答案，但我不确定我是否遗漏了任何内容，面试官也没有对我的答案提供任何反馈。你怎么认为？感谢您提前提供见解！&lt;/p>; &lt;p>;[1] Facebook 广告点击预测实践经验，ADKDD 14&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mayasang&quot;>; /u/mayasang &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13uqbrm/d_interview_question_comparing_two_models_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uqbrm/d_interview_question_comparing_two_models_with/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13uqbrm </id><link href="https://www.reddit.com/r/MachineLearning/comments/13uqbrm/d_interview_question_comparing_two_models_with/"/><updated> 2023-05-29T09:52:50+00:00</updated><published> 2023-05-29T09:52:50+00:00</published><title> [D]（面试题）在测试数据集上比较有负采样和无负采样但 AUC 和 logloss 相同的两个模型：哪个模型更好？</title></entry><entry><author><name> /u/ching7788</name><uri> https://www.reddit.com/user/ching7788 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uuli6/unicontrol_a_unified_diffusion_model_for/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;UniControl: A Un野外可控视觉生成的扩散模型 [P]&quot; title=&quot;UniControl:野外可控视觉生成的统一扩散模型 [P]&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;UniControl: A野外可控视觉生成的统一扩散模型&lt;/p>; &lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.11147&quot;>;https://arxiv.org/abs/2305.11147&lt;/ a>;&lt;/p>; &lt;p>;代码：&lt;a href=&quot;https://github.com/salesforce/UniControl&quot;>;https://github.com/salesforce/UniControl&lt;/a>;&lt;/p>; &lt;p >;秦灿†⋆, 张舒†, 宁宇†, Yihao Feng†, Xinyi Yang†, Yingbo Zhou†, Huan Wang†, Juan Carlos Niebles†, Caiming Xiong†, Silvio Savarese†, Stefano Ermon‡, Yun Fu⋆ , and Ran Xu†&lt;/p>; &lt;p>;†Salesforce AI Research, ⋆Northeastern University, ‡Stanford University&lt;/p>; &lt;p>;&lt;strong>;Overview:&lt;/strong>;&lt;/p>; &lt;p>;&lt;a href =&quot;https://preview.redd.it/7i81ugz7sr2b1.png?width=5030&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c6bb321f62eae67272e4d99bb082349dcb07e17&quot;>;UniControl 使用统一模型进行多项任务训练，进一步展示了有前途的能力在零样本任务泛化中，上面显示了视觉示例结果。&lt;/a>;&lt;/p>; &lt;p>;UniControl 的贡献：&lt;/p>; &lt;ul>; &lt;li>;UniControl 是一个统一模型（1.4B #params，5.78 GB 检查点）能够处理可控视觉生成的各种视觉条件。&lt;/li>; &lt;li>;为多条件视觉生成收集了一个新的数据集，其中包含超过 2000 万个图像-文本-条件（图像大小>;=512）三元组超过五个类别的九个不同任务。我们将很快开源训练数据。&lt;/li>; &lt;li>;大量实验表明，由于学习了不同视觉条件之间的内在关系，UniControl 优于每个单任务控制图像生成。&lt;/li>; &lt;li>; UniControl 展示了以零样本方式适应未知任务的能力，突出了它的多功能性和在野外广泛采用的潜力。&lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;方法：&lt;/strong>;&lt;/ p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/obznwqi9sr2b1.png?width=2018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d64f6da1e99102c4cf439c8951e5c79ec597df5b&quot;>;为了适应不同的任务，我们设计了一个混合专家（MOE）适配器，包含每个任务大约 ∼70K #params，以及一个任务感知 HyperNet（∼12M #params）来调制 N（即 7）个零卷积层。这种结构允许在单一模型中实现多任务功能，与等效的单任务模型堆栈相比显着减小了模型大小，每个模型具有大约 1.4B #params。&lt;/a>;&lt;/p>; &lt;p>;&lt;strong >;预训练任务结果：&lt;/strong>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/hpqn2vdbsr2b1.png?width=8061&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s =ef2796fee6d116586f702331b963cea3846b7df2&quot;>;官方或重新实现的任务特定 ControlNet 与我们提出的模型之间的视觉比较。示例数据是从我们从 COCO 和 Laion 采样的测试集中收集的。&lt;/a>;&lt;/p>; &lt;p>;&lt;strong>;零样本任务结果：&lt;/strong>;&lt;/p>; &lt;p>;&lt;a href= “https://preview.redd.it/hk3ewfndsr2b1.png?width=3930&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5fc63350ea9ea61020f915740a29f0fcba2811e”>;(a)-(b)：UniControl 对混合的示例结果（看不见的组合）带有关键字“背景”的条件和“前景”附在提示中。 (c)-(e)：UniControl 在三个未见过的任务（去模糊、着色、修复）上的示例结果。&lt;/a>;&lt;/p>; &lt;p>;&lt;strong>;Gradio 演示结果：&lt;/strong>;&lt;/p>; &lt;p>;所有任务的渐变 UI：&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/tfjmlsdfsr2b1.png?width=3134&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdca67175d8d6eb066b8e294437ff3174288a007 &quot;>;https://preview.redd.it/tfjmlsdfsr2b1.png?width=3134&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cdca67175d8d6eb066b8e294437ff3174288a007&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https:/ /preview.redd.it/6klax4zgsr2b1.png?width=3084&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=039ffb21fe6e9ff772880ce36b61ee72bcec75bd&quot;>;https://preview.redd.it/6klax4zgsr2b1.png?width=3084&amp; amp;格式=png&amp;amp; auto=webp&amp;amp;s=039ffb21fe6e9ff772880ce36b61ee72bcec75bd&lt;/a>;&lt;/p>; &lt;p>;单个任务的渐变 UI：&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/80mxa1cisr2b1.png? width=3090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a7274a1d311f674ea8f39bad9cebb1f5c853c32&quot;>;https://preview.redd.it/80mxa1cisr2b1.png?width=3090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a7 274a1d311f674ea8f39bad9cebb1f5c853c32&lt;/a>; &lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/4f4c4qfjs​​r2b1.png?width=3106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48fa021e67243490b549a69fd3a9707709c4bd06&quot;>;https://preview.redd .it/4f4c4qfjs​​r2b1.png?width=3106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48fa021e67243490b549a69fd3a9707709c4bd06&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/55vrpxzksr2b1 . png?width=3136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da2c6efce561f26a7418291ae4941452b26ac913&quot;>;https://preview.redd.it/55vrpxzksr2b1.png?width=3136&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da2c 6efce561f26a7418291ae4941452b26ac913&lt;/ a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/fq9yolmlsr2b1.png?width=3056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3bcd24401c89f0d8993deb25c670ba407488e26&quot;>;https://preview .redd.it/fq9yolmlsr2b1.png?width=3056&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3bcd24401c89f0d8993deb25c670ba407488e26&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/ 3sfm1q8msr2b1.png?width=3112&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1166905a7721e40918f8fb33aa1b723e392b68b&quot;>;https://preview.redd.it/3sfm1q8msr2b1.png?width=3112&amp; amp;格式=png&amp;amp;auto=webp&amp;amp;s=f1166905a7721e40918f8fb33aa1b723e392b68b &lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/fp68tk2nsr2b1.png?width=3102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2ef1a96e3a2006c0a7f84623233827bae430336&quot;>;https:/ /preview.redd.it/fp68tk2nsr2b1.png?width=3102&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2ef1a96e3a2006c0a7f84623233827bae430336&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.它/744w1hwnsr2b1.png?width=3124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a330e62a6e5541d2443744e6387085a26e6012&quot;>;https://preview.redd.it/744w1hwnsr2b1.png?width=3124&amp;amp ;格式=png&amp;amp;auto=webp&amp;amp;s =06a330e62a6e5541d2443744e6387085a26e6012&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/cam8g1apsr2b1.png?width=3114&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d935c63132c8a33 a5b0465bb7b94fdfec84bbde1&quot;>;https ://preview.redd.it/cam8g1apsr2b1.png?width=3114&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d935c63132c8a33a5b0465bb7b94fdfec84bbde1&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview. redd.it/77xg2w9qsr2b1.png?width=3092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc2a136bee5dfe6c7b3a0735d5db8c534ceaa8fc&quot;>;https://preview.redd.it/77xg2w9qsr2b1.png?width=3092&amp; amp;格式=png&amp;amp;auto=webp&amp;amp ;s=fc2a136bee5dfe6c7b3a0735d5db8c534ceaa8fc&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ching7788&quot;>; /u/ching7788 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13uuli6/unicontrol_a_unified_diffusion_model_for/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uuli6/unicontrol_a_unified_diffusion_model_for/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13uuli6 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13uuli6/unicontrol_a_unified_diffusion_model_for/"/><updated> 2023-05-29T13:13:38+00:00</updated><published> 2023-05-29T13:13:38+00:00</published><title> UniControl：野外可控视觉生成的统一扩散模型 [P]</title></entry><entry><author><name> /u/datachomper</name><uri> https://www.reddit.com/user/datachomper </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;作为我在内部向一些工作场所同事教授的迷你课程的一部分，我很乐意向他们展示一个不错的 &lt; em>;对抗性计算机视觉&lt;/em>; Colab 笔记本/可运行教程。我的专业领域是 ML、DL 中与 CV 相去甚远的部分，所以我觉得我不是制作&lt;em>;对抗性 CV&lt;/em>; Notebook / 的最佳人选可从头开始运行的教程。&lt;/p>; &lt;p>;我发现了这些：&lt;/p>; &lt;ul>; &lt;li>;从 TensorFlow 文档中，&lt;em>;快速梯度签名方法 (FGSM)&lt;/em>;来自旧 Goodfellow 论文的攻击：&lt;a href=&quot;https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb&quot;>;https:/ /colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb&lt;/a>;&lt;/li>; &lt;li>;阿姆斯特丹大学的深度学习课程：&lt; a href=&quot;https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb&quot;>;Github&lt;/a>; 和 &lt;a href=&quot;https://colab.research. google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb&quot;>;Colab&lt;/a>; 包括另一个 FGSM 示例&lt;/li>; &lt;/ul>; &lt;p>;我是想知道：是否有人有任何方便的 Colab Notebooks 参考资料，其中显示了除 FGSM 之外的一些其他方法，他们认为这些方法可以作为很好的教材/指南？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/datachomper&quot;>; /u/datachomper &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13uwmkn/d_favorite_colab_notebooks_runnable_tutorials_on/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uwmkn/d_favorite_colab_notebooks_runnable_tutorials_on/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13uwmkn </id><link href="https://www.reddit.com/r/MachineLearning/comments/13uwmkn/d_favorite_colab_notebooks_runnable_tutorials_on/"/><updated> 2023-05-29T14:35:53+00:00</updated><published> 2023-05-29T14:35:53+00:00</published><title> [D] 最喜欢的 Colab 笔记本/关于对抗性 CV 的可运行教程</title></entry><entry><author><name>/u/kazhdan_d</name><uri> https://www.reddit.com/user/kazhdan_d </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uurn2/p_fomo_as_a_service_compare_your_models_against/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/wUaSYINqTxMnWIhlJyItg-DE0FBAqsMKmpHSH7cbzHA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b032be1844bc44c6a94b1164ce57856f1816f5d&quot; alt=&quot;[P] &quot;FoMo 即服务“：将您的模型与 (Fo)undational (Mo) 进行比较用于对象检测的 dels&quot; title=&quot;[P] “FoMo 即服务”：将您的模型与用于对象检测的 (Fo)undational (Mo)dels 进行比较” />; &lt;/a>; &lt;/td>;&lt;td>; &lt;! -- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;br/>;我们将在 &lt; a href=&quot;https://www.tenyks.ai/&quot;>;Tenyks&lt;/a>;，其中：&lt;/p>; &lt;ul>; &lt;li>;您将最喜欢的对象检测数据集（以及可选的模型预测）上传到 Tenyks平台&lt;/li>; &lt;li>;Tenyks 为您设置最先进的零样本对象检测基线（例如基于 SAM 的基线）&lt;/li>; &lt;li>;您将模型/注释与基础模型/注释进行比较使用平台对您的数据建模&lt;/li>; &lt;li>;=>;gt;您可以就基础模型是否对您的用例有益做出明智的决定&lt;/li>; &lt;/ul>; &lt;p>;如果这听起来令人兴奋 - 请在此处联系：[&lt;a href=&quot;mailto:social@tenyks. ai&quot;>;social@tenyks.ai&lt;/a>;](mailto:&lt;a href=&quot;mailto:social@tenyks.ai&quot;>;social@tenyks.ai&lt;/a>;)（主题为“&lt;strong>; FoMo Offer&lt;/strong>;”)&lt;/p>; &lt;p>;PS 下面是一个零样本拥抱面模型的例子，它把汽车仪表盘当作“汽车”来处理。 :)&lt;/p>; &lt;p>;&lt;a href=&quot;https://reddit.com/link/13uurn2/video/5yluexcosr2b1/player&quot;>;有趣的 Huggingface 模型边缘案例&lt;/a>;&lt;/p>; &lt;/div>; &lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kazhdan_d&quot;>; /u/kazhdan_d &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13uurn2/p_fomo_as_a_service_compare_your_models_against/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uurn2/p_fomo_as_a_service_compare_your_models_against/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13uurn2 </id><media:thumbnail url="https://external-preview.redd.it/wUaSYINqTxMnWIhlJyItg-DE0FBAqsMKmpHSH7cbzHA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b032be1844bc44c6a94b1164ce57856f1816f5d"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13uurn2/p_fomo_as_a_service_compare_your_models_against/"/><updated> 2023-05-29T13:20:39+00:00</updated><published> 2023-05-29T13:20:39+00:00</published><title> [P]“FoMo 即服务”：将您的模型与用于对象检测的 (Fo)undational (Mo)dels 进行比较</title></entry><entry><author><name>/u/kkimdev</name><uri> https://www.reddit.com/user/kkimdev </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我想知道我们是否可以使用 LoRA 进行预训练，每 N 步将 LoRA 权重与冻结权重合并。或者有没有类似的预训练研究？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kkimdev&quot;>; /u/kkimdev &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13upogz/d_lora_weight_merge_every_n_step_for_pretraining/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13upogz/d_lora_weight_merge_every_n_step_for_pretraining/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13upogz </id><link href="https://www.reddit.com/r/MachineLearning/comments/13upogz/d_lora_weight_merge_every_n_step_for_pretraining/"/><updated> 2023-05-29T09:13:59+00:00</updated><published> 2023-05-29T09:13:59+00:00</published><title> [D] [LoRA + weight merge every N step]预训练？</title></entry><entry><author><name> /u/姆班多</name><uri>https://www.reddit.com/user/Mbando </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;ol>; &lt;li>;协同驾驶文档/填写表格的模型在概念上是否与问答模型不同？&lt;/li>; &lt;li>;如果是这样，关于训练那种模型的任何资源（博客文章、教程）？&lt;/li>; &lt;/ol>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Mbando&quot;>; /u/Mbando &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13uymug/d_resources_for_documentwriting_models/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uymug/d_resources_for_documentwriting_models/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13uymug </id><link href="https://www.reddit.com/r/MachineLearning/comments/13uymug/d_resources_for_documentwriting_models/"/><updated> 2023-05-29T15:53:59+00:00</updated><published> 2023-05-29T15:53:59+00:00</published><title> [D] 文档编写模型的资源？</title></entry><entry><author><name> /你/硬丸</name><uri>https://www.reddit.com/user/hardmaru </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tqvdn/uncensored_models_finetuned_without_artificial/&quot;>; &lt;img src=&quot;https://preview.redd.it /jb5pl4n1xh2b1.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1b0d8f5b3a6a9e5349d7a51af1b00ab5c575307&quot; alt=&quot;Uncensored models, fine-tuned without artificial moralizing, such as “Wizard-Vicuna-13B-Uncensored-HF”执行擅长LLM 评估基准，即使与更大的 65B、40B、30B 模型相比也是如此。有没有关于审查制度如何阻碍模型能力的研究？” title=&quot;Uncensored models, fine-tuned without artificial moralizing, such as “Wizard-Vicuna-13B-Uncensored-HF” 在 LLM 评估基准测试中表现良好，即使与更大的 65B、40B、30B 模型相比也是如此。是否有任何研究关于审查制度如何阻碍模型的能力？” />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/hardmaru&quot;>; /u/hardmaru &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://i.redd.it/ jb5pl4n1xh2b1.jpg&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tqvdn/uncensored_models_finetuned_without_artificial/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13tqvdn </id><media:thumbnail url="https://preview.redd.it/jb5pl4n1xh2b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c1b0d8f5b3a6a9e5349d7a51af1b00ab5c575307"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13tqvdn/uncensored_models_finetuned_without_artificial/"/><updated> 2023-05-28T04:03:10+00:00</updated><published> 2023-05-28T04:03:10+00:00</published><title>未经审查的模型，在没有人工道德化的情况下进行微调，例如“Wizard-Vicuna-13B-Uncensored-HF”在 LLM 评估基准测试中表现良好，即使与更大的 65B、40B、30B 模型相比也是如此。是否有关于审查制度如何阻碍模型能力的研究？</title></entry><entry><author><name> /u/睡虎4</name><uri> https://www.reddit.com/user/Sleepin-tiger4 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;斯坦福发布了一个非凡的新二阶优化器，称为 Sophia，它使用估算器并利用裁剪机制。 &lt;/p>; &lt;blockquote>; &lt;p>;根据这篇论文，它的效率提高了 100K 步，计算所需的挂钟时间明显减少。&lt;/p>; &lt;/blockquote>; &lt;p>;这篇论文令人惊叹，是一个里程碑至少在我看来。他们没有提供任何代码，但提供了伪代码和算法来对优化器进行编程。我发现它有助于编程或理解代码，而不是仅仅阅读文献本身，甚至是它的伪代码。这就是为什么我花时间编写一个利用优化器的函数。 &lt;/p>; &lt;p>;如果您对他们使用的超参数感兴趣，在他们的论文中非常清楚，他们还提到使用网格搜索并基于 AdamW 获取 sophia 的超参数和 Lion 的参数选择。 &lt;/p>; &lt;p>;这是一个非常快的项目，所以我只能以非常基本的方式编写代码，没有任何 pytorch 或 jax。我很乐观地添加了一个训练脚本和一些漂亮的功能。直到几周后。 &lt;/p>; &lt;p>;我个人认为阅读代码和学习 Sophia 将非常有帮助，并且对许多人来说它可以提供一个新的研究方向（也许对你的论文也是如此）。我已将 github 链接添加到我的代码中。 &lt;/p>; &lt;p>;&lt;strong>;贡献：&lt;/strong>;&lt;/p>; &lt;p>;罗马不是自己建造的。如果您认为可以提供一些东西，请随时为存储库做出贡献。它会帮助其他人学习。你也是。如果您发现我的工作有趣或有帮助，请考虑给星标，这有助于许多人看到存储库，并有点激励我考虑为项目提供更新和很酷的东西。 &lt;/p>; &lt;p>;否则，这是 GitHub 代码和论文链接&lt;/p>; &lt;p>;&lt;strong>;GitHub 代码：&lt;/strong>; &lt;a href=&quot;https://github.com/ sleepingcat4/Sophia&quot;>;https://github.com/sleepingcat4/Sophia&lt;/a>;&lt;/p>; &lt;p>;&lt;strong>;论文链接：&lt;/strong>; &lt;a href=&quot;https://arxiv.org/ abs/2305.14342&quot;>;https://arxiv.org/abs/2305.14342&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Sleepin-tiger4&quot;>; /u/Sleepin-tiger4 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/MachineLearning/comments/13u36x6/p_sophia_programmedout/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13u36x6/p_sophia_programmedout/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13u36x6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13u36x6/p_sophia_programmedout/"/><updated> 2023-05-28T15:31:25+00:00</updated><published> 2023-05-28T15:31:25+00:00</published><title> [P] Sophia（程序外）</title></entry><entry><author><name> /你/flerakml</name><uri> https://www.reddit.com/user/flerakml </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我正在尝试解析 Calvin Luo 的非常全面的论文 &lt;a href=&quot;https://arxiv.org/pdf/2208.11970.pdf&quot; >;https://arxiv.org/pdf/2208.11970.pdf.&lt;/a>;&lt;br/>;任何人都可以从数学上展示如何从等式 (43) -&amp;gt; (45) 使用期望方程和 PGM？我需要帮助来理解变量在预期中消失的位置。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/flerakml&quot;>; /u/flerakml &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13ufl4h/d_understanding_understanding_diffusion_models_a/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ufl4h/d_understanding_understanding_diffusion_models_a/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13ufl4h </id><link href="https://www.reddit.com/r/MachineLearning/comments/13ufl4h/d_understanding_understanding_diffusion_models_a/"/><updated> 2023-05-29T00:16:52+00:00</updated><published> 2023-05-29T00:16:52+00:00</published><title> [D] 理解 - 理解扩散模型：一个统一的视角</title></entry><entry><author><name>/u/asotos11</name><uri> https://www.reddit.com/user/asotos11 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;strong>;摘要：&lt;/strong>;&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 中采用的自回归转换器很难扩展到长序列。尽管有几项工作试图降低其计算成本，但大多数 LLM 仍然在序列中的所有标记对之间采用注意力层，从而产生二次成本。在这项研究中，我们提出了一种新方法，可以动态修剪上下文信息，同时保留模型的表达能力，从而减少推理过程中的内存和计算需求。我们的方法采用了一种可学习的机制，该机制确定可以在生成过程中的任何时候从上下文中删除哪些无信息的标记。通过这样做，我们的方法不仅解决了性能问题，而且增强了可解释性，为模型的决策过程提供了有价值的见解。我们的技术可以通过简单的微调过程应用于现有的预训练模型，并且修剪强度可以由稀疏参数指定。值得注意的是，我们的实证结果表明，我们可以有效地修剪高达 80% 的上下文，而不会显着降低下游任务的性能，从而为降低推理成本提供了一个有价值的工具。我们的参考实现将推理吞吐量提高了 2 倍，甚至节省了更多内存。&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;strong>;工作原理：&lt;/strong>;&lt;/p>; &lt;blockquote>; &lt;p>;我们发现，&lt;strong>;高达 80% 的上下文可以被成功修剪&lt;/strong>;，&lt;strong>;在困惑度和零样本性能方面的恶化最小&lt;/strong>;，同时在推理过程中需要的资源明显减少。我们展示了这些改进如何带来可衡量的实际收益，方法是&lt;strong>;提供一种有效的实现方式来减少令牌生成期间用于缓存的内存使用量&lt;/strong>;。更具体地说，对于更大的上下文大小，&lt;strong>;我们在每一代&lt;/strong>;步骤中减少了高达 50% 的墙时间延迟，同时仍然&lt;strong>;以高达 2 倍大的批处理大小进行解码&lt;/strong>;，领先从而获得显着的性能优势。这些发现强调了上下文修剪作为一种强大技术的潜力，可以提高 NLP 中 Transformer 的效率和可解释性。&lt;/p>; &lt;/blockquote>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/asotos11&quot;>; /u/asotos11 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13utt6e/r_dynamic_context_pruning_for_efficient_and/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13utt6e/r_dynamic_context_pruning_for_efficient_and/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13utt6e </id><link href="https://www.reddit.com/r/MachineLearning/comments/13utt6e/r_dynamic_context_pruning_for_efficient_and/"/><updated> 2023-05-29T12:41:00+00:00</updated><published> 2023-05-29T12:41:00+00:00</published><title> [R] 高效且可解释的自回归变压器的动态上下文修剪</title></entry><entry><author><name>/u/智能使用5990</name><uri> https://www.reddit.com/user/IntelligentUse5990 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;需要紧急，但旧链接说“发生内部错误”&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/IntelligentUse5990&quot;>; /u/IntelligentUse5990 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13uk1rm/p_does_anyone_have_the_dataset_called_recipe_1m/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uk1rm/p_does_anyone_have_the_dataset_called_recipe_1m/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13uk1rm </id><link href="https://www.reddit.com/r/MachineLearning/comments/13uk1rm/p_does_anyone_have_the_dataset_called_recipe_1m/"/><updated> 2023-05-29T03:51:55+00:00</updated><published> 2023-05-29T03:51:55+00:00</published><title> [P] 有没有人有名为 Recipe 1M+ 或逆向烹饪的数据集？</title></entry><entry><author><name> /你/窗口</name><uri>https://www.reddit.com/user/windoze </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;背景：&lt;/p>; &lt;p>;我一直在研究如何创建循环 seq-to-seq 模型，这&amp;# 39;不是变形金刚。我实施的想法行不通。似乎偏离了常走的路，到处都是陷阱——我应该如何调整参数，添加偏差，归一化，这个数据集是不可能的，梯度爆炸和消失等等。&lt;/p>; &lt;p>;来自“research =梯度下降”从一个角度来看，我被困在一个没有梯度的点上——我不知道我做错了什么，或者怎么做才能得到更好的结果。我是否缺少工作流程。直觉、工具或其他东西？&lt;/p>; &lt;p>;你使用什么元方法来获得结果？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/windoze&quot;>; /u/windoze &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13ur6sh/d_research_method_and_advice/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ur6sh/d_research_method_and_advice/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13ur6sh </id><link href="https://www.reddit.com/r/MachineLearning/comments/13ur6sh/d_research_method_and_advice/"/><updated> 2023-05-29T10:38:19+00:00</updated><published> 2023-05-29T10:38:19+00:00</published><title> [D] 研究方法与建议．</title></entry><entry><author><name> /u/蔬菜技能-9700</name><uri> https://www.reddit.com/user/Vegetable-Skill-9700 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;虽然 LLM 在大量数据上进行了训练并且可以很好地泛化到许多任务，但它们仍然容易出错。这里的社区成员采用哪些最佳实践来识别和解决此类案例？&lt;/p>; &lt;p>;我正在构建一个开源存储库，可以帮助您识别此类边缘案例并评估您的 GPT 驱动的应用程序在它们上面，以便可以安全地部署它们（比如在调整提示、链等之后）。想了解问题陈述有多大。非常感谢任何反馈。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Vegetable-Skill-9700&quot;>; /u/Vegetable-Skill-9700 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13uuzyd/d_do_you_care_about_edge_cases_while_building_llm/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uuzyd/d_do_you_care_about_edge_cases_while_building_llm/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13uuzyd </id><link href="https://www.reddit.com/r/MachineLearning/comments/13uuzyd/d_do_you_care_about_edge_cases_while_building_llm/"/><updated> 2023-05-29T13:30:05+00:00</updated><published> 2023-05-29T13:30:05+00:00</published><title> [D] 你在构建 LLM 应用程序时关心边缘案例吗？</title></entry><entry><author><name> /u/SwaroopMeher</name><uri> https://www.reddit.com/user/SwaroopMeher </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;你好，请问有AI主流任务中最新的SOTA模型或架构的综合列表吗？&lt;/p>; &lt;p>;如果没有，我请求你在评论中分享一些你知道的。&lt;/p>; &lt;p>;有这么多模型，很难找到最适合手头给定任务的模型。&lt;/p>; &lt;p>;如果您能分享此信息，我将不胜感激。我的研究需要它。&lt;/p>; &lt;p>;PS 我知道提到“AI”这个问题太模糊了。我只想收集尽可能多的任务和它们各自的 SOTA 模型。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/SwaroopMeher&quot;>; /u/SwaroopMeher &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13um1r4/r_list_of_sota_modelsarchitectures_in_machine/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13um1r4/r_list_of_sota_modelsarchitectures_in_machine/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13um1r4 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13um1r4/r_list_of_sota_modelsarchitectures_in_machine/"/><updated> 2023-05-29T05:38:00+00:00</updated><published> 2023-05-29T05:38:00+00:00</published><title> [R] 机器学习中的 SOTA 模型/架构列表</title></entry><entry><author><name>/u/crp1994</name><uri> https://www.reddit.com/user/crp1994 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13u49sz/r_umat_uncertaintyaware_single_image_high/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/aVU_c5UeVjr0WEbNcThOJybDF2JBq6PMv7gLgCiyvdk.jpg&quot; alt=&quot;[R] UMat：不确定性感知单图像高分辨率材料捕获&quot; title=&quot;[R] UMat：不确定性感知单图像高分辨率材料捕获&quot; />; &lt;/a>; &lt; /td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://i.redd.it /rhzc83xfkl2b1.gif&quot;>;https://i.redd.it/rhzc83xfkl2b1.gif&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/crp1994&quot;>; /u/crp1994 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13u49sz/r_umat_uncertaintyaware_single_image_high/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13u49sz/r_umat_uncertaintyaware_single_image_high/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13u49sz </id><media:thumbnail url="https://b.thumbs.redditmedia.com/aVU_c5UeVjr0WEbNcThOJybDF2JBq6PMv7gLgCiyvdk.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13u49sz/r_umat_uncertaintyaware_single_image_high/"/><updated> 2023-05-28T16:15:23+00:00</updated><published> 2023-05-28T16:15:23+00:00</published><title> [R] UMat：不确定性感知单图像高分辨率材料捕获</title></entry><entry><author><name>/u/松散研究-3105</name><uri> https://www.reddit.com/user/Loose-Research-3105 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;ARR 结果中的分数与 softconf START 中的分数相比如何？我们可以认为我们在这个 ARR 中获得的分数与我们从直接提交到 START（*例如 EMNLP）中获得的分数具有可比性吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp; #32；由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Loose-Research-3105&quot;>; /u/Loose-Research-3105 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13uohe7/d_arr_scores_vs_start_softconf_scores/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uohe7/d_arr_scores_vs_start_softconf_scores/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13uohe7 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13uohe7/d_arr_scores_vs_start_softconf_scores/"/><updated> 2023-05-29T07:59:50+00:00</updated><published> 2023-05-29T07:59:50+00:00</published><title> [D] ARR 分数 vs START softconf 分数</title></entry><entry><author><name>/你/塞拉施卡</name><uri>https://www.reddit.com/user/seraschka </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13u20mn/p_historical_tidbits_about_transformers_about/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/kUxj6CnxzIRDysMx6ikazH21j-o26FrLLdAlrUW-bCk.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=37090819bf367e44d271ed7dbdea2fe14d4fe5d9&quot; alt=&quot;[P] 关于变形金刚的历史花絮: 关于原始 Transformer 论文和 Schmidhuber 的 Fast 中的 LayerNorm 变体1990 年代的权重程序员&quot; title=&quot;[P] 关于 Transformers 的历史花絮：关于原始 Transformer 论文中的 LayerNorm 变体和 Schmidhuber 的 1990 年代的快速权重程序员&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;# 32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/seraschka&quot;>; /u/seraschka &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://magazine.sebastianraschka.com/ p/why-the-original-transformer-figure&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13u20mn/p_historical_tidbits_about_transformers_about/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13u20mn </id><media:thumbnail url="https://external-preview.redd.it/kUxj6CnxzIRDysMx6ikazH21j-o26FrLLdAlrUW-bCk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=37090819bf367e44d271ed7dbdea2fe14d4fe5d9"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13u20mn/p_historical_tidbits_about_transformers_about/"/><updated> 2023-05-28T14:39:25+00:00</updated><published> 2023-05-28T14:39:25+00:00</published><title> [P] 关于 Transformers 的历史花絮：关于原始 Transformer 论文中的 LayerNorm 变体和 Schmidhuber 的 1990 年代的快速权重程序员</title></entry><entry><author><name>/u/__data_cactus__</name><uri> https://www.reddit.com/user/__data_cactus__ </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;我教机器学习已经几年了，我写了一篇关于我使用的过程的文章用于我的 IT 专业人员培训课程。&lt;/p>; &lt;p>;这是关于我在短时间内（不需要 CS 数学课程）在神经网络上建立直觉的策略，同时它&amp;# 39;s 主要面向这个领域的教育工作者，我想你们中的许多人会喜欢阅读。&lt;/p>; &lt;p>;让我知道您的想法！ :D&lt;/p>; &lt;p>;&lt;a href=&quot;https://medium.com/@matei.simtinica/how-i-teach-the-intuition-behind-neural-networks-d7b7ca418873&quot;>;https:// medium.com/@matei.simtinica/how-i-teach-the-intuition-behind-neural-networks-d7b7ca418873&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/__data_cactus__&quot;>; /u/__data_cactus__ &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13ucjp6/d_teaching_the_intuition_behind_nns/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ucjp6/d_teaching_the_intuition_behind_nns/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13ucjp6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13ucjp6/d_teaching_the_intuition_behind_nns/"/><updated> 2023-05-28T22:05:11+00:00</updated><published> 2023-05-28T22:05:11+00:00</published><title> [D] 教授神经网络背后的直觉</title></entry><entry><category label="r/MachineLearning" term="MachineLearning"></category><content type="html"> &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嘿，我是机器学习的新手，我正在使用决策树预测全世界农村人口中的儿童营养不良。你能指导我如何从 Kaggle 上可用的这么多具有​​复杂功能的 Excel 文件中提取数据库到我的目的数据库，即用于农村人口。如果你能帮助我，那就太好了。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/ MachineLearning/comments/13ulg2z/projectp/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ulg2z/projectp/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13ulg2z</id><link href="https://www.reddit.com/r/MachineLearning/comments/13ulg2z/projectp/"/><updated> 2023-05-29T05:03:57+00:00</updated><published> 2023-05-29T05:03:57+00:00</published><title>项目[P]</title></entry><entry><author><name> / u / thelazyaz</name><uri> https://www.reddit.com/user/thelazyaz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13umdtr/realtime_trash_object_detection_web_app_p/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/_BTytXBtwwvQtEBYr9O6PA5LR9URNaSfKFnpOS8evOc.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7000b862d79708eb1c9b1eede6938e48f1fc6e4&quot; alt=&quot;实时垃圾对象检测网络应用程序[P]&quot; title=&quot;实时垃圾对象检测Web App [ P]&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/thelazyaz&quot;>; /u/thelazyaz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://youtu.be/Kn8Hsz1YT78&quot; >;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13umdtr/realtime_trash_object_detection_web_app_p/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13umdtr </id><media:thumbnail url="https://external-preview.redd.it/_BTytXBtwwvQtEBYr9O6PA5LR9URNaSfKFnpOS8evOc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d7000b862d79708eb1c9b1eede6938e48f1fc6e4"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13umdtr/realtime_trash_object_detection_web_app_p/"/><updated> 2023-05-29T05:57:17+00:00</updated><published> 2023-05-29T05:57:17+00:00</published><title>实时垃圾目标检测 Web App [P]</title></entry><entry><author><name> /u/CS-fan-101</name><uri> https://www.reddit.com/user/CS-fan-101 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uca9c/p_introducing_model_lab_a_new_tool_to_make_sense/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia ... >; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/CS-fan-101&quot;>; /u/CS-fan-101 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;/r /mlscaling/comments/13rvx0i/t_introducing_model_lab_a_new_tool_to_make_sense/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13uca9c/p_introducing_model_lab_a_new_tool_to_make_sense/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13uca9c </id><media:thumbnail url="https://b.thumbs.redditmedia.com/Mbp_hnrdO0qsteXoi5cnum-mSb8oV8S5FSUWahoaMmE.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13uca9c/p_introducing_model_lab_a_new_tool_to_make_sense/"/><updated> 2023-05-28T21:54:33+00:00</updated><published> 2023-05-28T21:54:33+00:00</published><title> [P] 介绍模型实验室 - 一种理解培训 LLM 的新工具</title></entry><entry><author><name>/u/BlackLands123</name><uri> https://www.reddit.com/user/BlackLands123 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;我写这篇文章是为了寻求你对一个问题的帮助（&lt;a href= &quot;https://en.wikipedia.org/wiki/Nurse_scheduling_problem&quot;>;护士调度问题&lt;/a>;) 我正在尝试使用遗传算法来解决。&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt; p>;我的问题如下：我需要为一个由一定数量的人组成的团队自动生成花名册。每个人都有不同的雇佣合同，包括不同的每周工作时间和不同的休息天数。&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;因为我还处于起步阶段点，我设定了我的初始目标，即分配给每个人的每周工作时间与他或她的合同中的工作时间相等。&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;每个人在population 由长度等于的二进制向量组成：7（一周中的天数）* 8（商店每天营业的小时数）* N（团队中的人数）。该向量将代表每个团队成员的每周工作班次。如果算法对索引1和2的数组位置赋值为11，则表示周一第一个人的工作时间为9点到10点（每一位为一小时的工作），依此类推。&lt;/p>; &lt;p >;&amp;#x200B;&lt;/p>; &lt;p>;算法还传递了一个数组，指示每个人每周必须工作的小时数，例如，[40,40,32,32] 表示第一个人必须工作 40小时，第二个 40，依此类推。&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;我的问题是算法始终固定在一个解决方案上，同时改变变异概率或种群大小。如果一个团队由 4 名成员组成，他们必须分别工作 40、40、32 和 32 小时，则算法为所有成员分配 36 小时的池。&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p >; &lt;p>;下面我展示了适应度函数的部分代码（我使用了 Python 和 DEAP 库），其中我分配了一个惩罚，该惩罚与解决方案与每个员工每周必须工作的正确小时数的距离成正比&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;code>;def countWeeklyHoursViolations(self, employeeShiftDict):&lt;/code>;&lt;br/>; &lt;code>;“””&lt;/ code>;&lt;br/>; &lt;code>;:param employeeShiftDict: 以员工姓名为键，以周班次为值的员工班次字典&lt;/code>;&lt;br/>; &lt;code>;:return: 周班次数hours violations&lt;/code>;&lt;br/>; &lt;code>;&amp;quot;&amp;quot;&amp;quot;&lt;/code>;&lt;br/>; &lt;code>;#模拟退火会尝试最小化这个函数&lt;/code>; &lt;/p>; &lt;p>; &lt;code>;weeklyHoursViolation = 0&lt;/code>;&lt;br/>; &lt;code>;对于 self.employees 中的员工：&lt;/code>;&lt;br/>; &lt;code>;weekly_hours_calculated = sum(employeeShiftDict[employee])&lt;/code>;&lt;br />; &lt;code>;weekly_hours_expected = self.weeklyHours[self.employees.index(employee)]&lt;/code>;&lt;br/>; &lt;code>;# 对预期和计算的每周小时数的平方差求和 - 差值越大惩罚越大&lt; /code>;&lt;br/>; &lt;code>;weeklyHoursViolation += self.hardConstraintPenalty * abs(weekly_hours_calculated - weekly_hours_expected)**2&lt;/code>;&lt;br/>; &lt;code>;返回 weeklyHoursViolation&lt;/code>;&lt;/p>; &lt;p>; &amp;#x200B;&lt;/p>; &lt;p>;你建议我做什么？你有什么想法？在此先感谢您！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/BlackLands123&quot;>; /u/BlackLands123 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13u8krw/p_genetic_algorithm_gots_stuck_variation_of/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13u8krw/p_genetic_algorithm_gots_stuck_variation_of/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13u8krw </id><link href="https://www.reddit.com/r/MachineLearning/comments/13u8krw/p_genetic_algorithm_gots_stuck_variation_of/"/><updated> 2023-05-28T19:17:32+00:00</updated><published> 2023-05-28T19:17:32+00:00</published><title> [P] 遗传算法卡住了 - Nurses 问题的变体</title></entry><entry><author><name>/u/BidImpossible555</name><uri> https://www.reddit.com/user/BidImpossible555 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 改善事实通过多主体辩论在语言模型中的质量和推理&quot; title=&quot;[R] 改善事实通过多智能体辩论在语言模型中进行和推理&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/BidImpossible555&quot;>; /u/BidImpossible555 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14325&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13t83xv </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/"/><updated> 2023-05-27T13:53:41+00:00</updated><published> 2023-05-27T13:53:41+00:00</published><title> [R] 通过多主体辩论改善语言模型中的事实和推理</title></entry><entry><author><name>/你/卡尔法斯扬</name><uri>https://www.reddit.com/user/kalfasyan </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tu09b/p_plakakia_tiles_in_greek_is_an_image_tiling/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia ... .这是我制作的第一个开源库，所以希望我能向更有经验的人学习。” title=&quot;[P] Plakakia（希腊语中的 tiles）是我制作的一个图像平铺库，用于从图像快速生成平铺。如果人们尝试它并在 github 上提供一些反馈/提出问题，那就太好了。它是第一个开源我做过的图书馆，所以希望我能向更有经验的人学习。” />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kalfasyan&quot;>; /u/kalfasyan &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://github.com/kalfasyan/ plakakia&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tu09b/p_plakakia_tiles_in_greek_is_an_image_tiling/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13tu09b </id><media:thumbnail url="https://b.thumbs.redditmedia.com/jje0KypwRW1CZuUVf5fmUp4TjijHWX006uJQRoOfzHY.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13tu09b/p_plakakia_tiles_in_greek_is_an_image_tiling/"/><updated> 2023-05-28T07:15:17+00:00</updated><published> 2023-05-28T07:15:17+00:00</published><title> [P] Plakakia（希腊语中的 tiles）是我制作的一个图像平铺库，用于从图像快速生成平铺。如果人们尝试它并在 github 上提供一些反馈/提出问题，那就太好了。这是我创建的第一个开源库，所以希望我能向更有经验的人学习。</title></entry></feed>