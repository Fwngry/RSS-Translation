<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-28T12:26:49+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /你/硬丸</name><uri>https://www.reddit.com/user/hardmaru </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tqvdn/uncensored_models_finetuned_without_artificial/&quot;>; &lt;img src=&quot;https://preview.redd.it /jb5pl4n1xh2b1.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1b0d8f5b3a6a9e5349d7a51af1b00ab5c575307&quot; alt=&quot;Uncensored models, fine-tuned without artificial moralizing, such as “Wizard-Vicuna-13B-Uncensored-HF”执行擅长LLM 评估基准，即使与更大的 65B、40B、30B 模型相比也是如此。有没有关于审查制度如何阻碍模型能力的研究？” title=&quot;Uncensored models, fine-tuned without artificial moralizing, such as “Wizard-Vicuna-13B-Uncensored-HF” 在 LLM 评估基准测试中表现良好，即使与更大的 65B、40B、30B 模型相比也是如此。是否有任何研究关于审查制度如何阻碍模型的能力？” />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/hardmaru&quot;>; /u/hardmaru &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://i.redd.it/ jb5pl4n1xh2b1.jpg&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tqvdn/uncensored_models_finetuned_without_artificial/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13tqvdn </id><media:thumbnail url="https://preview.redd.it/jb5pl4n1xh2b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c1b0d8f5b3a6a9e5349d7a51af1b00ab5c575307"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13tqvdn/uncensored_models_finetuned_without_artificial/"/><updated> 2023-05-28T04:03:10+00:00</updated><published> 2023-05-28T04:03:10+00:00</published><title>未经审查的模型，在没有人工道德化的情况下进行微调，例如“Wizard-Vicuna-13B-Uncensored-HF”在 LLM 评估基准测试中表现良好，即使与更大的 65B、40B、30B 模型相比也是如此。是否有关于审查制度如何阻碍模型能力的研究？</title></entry><entry><author><name> /u/Awkward-Let-4628</name><uri> https://www.reddit.com/user/Awkward-Let-4628 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tucgj/p_talkcodebase_is_a_powerful_tool_for_chatting/&quot;>; &lt;img src=&quot;https://preview.redd.it /qenc8ydfhk2b1.gif?width=640&amp;amp;crop=smart&amp;amp;s=fe9ee3e9bc818bf99d3bee2608091dad2777a11e&quot; alt=&quot;[P] talk-codebase 是一个与你的代码库聊天的强大工具&quot; title=&quot;[P] talk-codebase 是一个强大的工具与你的代码库聊天&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://github.com/ rsaryev/talk-codebase&quot;>;https://github.com/rsaryev/talk-codebase&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Awkward-Let-4628&quot;>; /u/Awkward-Let-4628 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //i.redd.it/qenc8ydfhk2b1.gif&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tucgj/p_talkcodebase_is_a_powerful_tool_for_chatting/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13tucgj </id><media:thumbnail url="https://preview.redd.it/qenc8ydfhk2b1.gif?width=640&amp;crop=smart&amp;s=fe9ee3e9bc818bf99d3bee2608091dad2777a11e"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13tucgj/p_talkcodebase_is_a_powerful_tool_for_chatting/"/><updated> 2023-05-28T07:36:38+00:00</updated><published> 2023-05-28T07:36:38+00:00</published><title> [P] talk-codebase 是一个与你的代码库聊天的强大工具</title></entry><entry><author><name>/u/BidImpossible555</name><uri> https://www.reddit.com/user/BidImpossible555 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 改善事实通过多主体辩论在语言模型中的质量和推理&quot; title=&quot;[R] 改善事实通过多智能体辩论在语言模型中进行和推理&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/BidImpossible555&quot;>; /u/BidImpossible555 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14325&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13t83xv </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/"/><updated> 2023-05-27T13:53:41+00:00</updated><published> 2023-05-27T13:53:41+00:00</published><title> [R] 通过多主体辩论改善语言模型中的事实和推理</title></entry><entry><author><name>/你/玛雅桑</name><uri>https://www.reddit.com/user/mayasang </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;你好，我最近在接受一家科技公司的采访时遇到了这个问题。我回答说它会比 L2 项有更显着的效果，使权重系数更小。面试官说还有一个更重要的方面：它现在使问题非凸，因为三阶函数不再是凸函数。谁能进一步详细说明这个解释？添加具有对数似然的 L3 项是否也会使成本函数非凸？我试着问这个 Google 和 ChatGPT，ChatGPT 说逻辑回归模型仍然是凸的：&lt;/p>; &lt;blockquote>; &lt;p>;在逻辑回归中，目标函数通常是最大化的对数似然函数，或者等效地，一个最小化的负对数似然函数。添加正则化时，将正则化项添加到负对数似然以创建正则化目标函数。添加 L3 正则化不会引入非凸性。 &lt;/p>; &lt;p>;通过分析目标函数的Hessian矩阵，可以从数学上证明带L3正则化的逻辑回归模型的凸性。 Hessian矩阵是正半定的，这证实了凸性。 &lt;/p>; &lt;p>;因此，即使包含 L3 正则化项，逻辑回归模型仍然是凸的，可以使用凸优化技术来有效地找到最优解。&lt;/p>; &lt;/blockquote>; &lt;/ div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mayasang&quot;>; /u/mayasang &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13tq3p8/d_interview_question_what_happens_if_we_add_l3/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tq3p8/d_interview_question_what_happens_if_we_add_l3/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tq3p8 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13tq3p8/d_interview_question_what_happens_if_we_add_l3/"/><updated> 2023-05-28T03:21:12+00:00</updated><published> 2023-05-28T03:21:12+00:00</published><title> [D]（面试问题）如果我们将 L3 项添加到逻辑回归模型中会发生什么？</title></entry><entry><author><name> /你/卡尔法斯扬</name><uri>https://www.reddit.com/user/kalfasyan </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tu09b/p_plakakia_tiles_in_greek_is_an_image_tiling/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia ... .这是我制作的第一个开源库，所以希望我能向更有经验的人学习。” title=&quot;[P] Plakakia（希腊语中的 tiles）是我制作的一个图像平铺库，用于从图像快速生成平铺。如果人们尝试它并在 github 上提供一些反馈/提出问题，那就太好了。它是第一个开源我做过的图书馆，所以希望我能向更有经验的人学习。” />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kalfasyan&quot;>; /u/kalfasyan &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://github.com/kalfasyan/ plakakia&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tu09b/p_plakakia_tiles_in_greek_is_an_image_tiling/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13tu09b </id><media:thumbnail url="https://b.thumbs.redditmedia.com/jje0KypwRW1CZuUVf5fmUp4TjijHWX006uJQRoOfzHY.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13tu09b/p_plakakia_tiles_in_greek_is_an_image_tiling/"/><updated> 2023-05-28T07:15:17+00:00</updated><published> 2023-05-28T07:15:17+00:00</published><title> [P] Plakakia（希腊语中的 tiles）是我制作的一个图像平铺库，用于从图像快速生成平铺。如果人们尝试它并在 github 上提供一些反馈/提出问题，那就太好了。这是我创建的第一个开源库，所以希望我能向更有经验的人学习。</title></entry><entry><author><name> /你/莫伊尔</name><uri>https://www.reddit.com/user/moyle </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;strong>;简短摘要&lt;/strong>;：使用 LLM 根据给定文档中问题的可能性对给定的一组文档进行排名——显示与全监督检索系统性能相当。&lt;/p>; &lt;p>;&lt;strong>;Arxiv：&lt;/strong>; &lt;a href=&quot;https://arxiv.org/abs/2205.12650&quot;>;https://arxiv.org /abs/2205.12650&lt;/a>;&lt;/p>; &lt;p>;&lt;strong>;Github：&lt;/strong>; &lt;a href=&quot;https://github.com/mukhal/PromptRank&quot;>;https://github.com/ mukhal/PromptRank&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/moyle&quot;>; /u/moyle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13tpnb5/r_using_llms_for_multihop_document_reranking_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tpnb5/r_using_llms_for_multihop_document_reranking_with/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tpnb5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13tpnb5/r_using_llms_for_multihop_document_reranking_with/"/><updated> 2023-05-28T02:57:05+00:00</updated><published> 2023-05-28T02:57:05+00:00</published><title> [R] 使用 LLM 进行多跳文档重新排序，仅使用几个示例。</title></entry><entry><author><name> /u/莱维西</name><uri>https://www.reddit.com/user/Levissie </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好。我遇到了&lt;a href=&quot;https://cardcastle.co/&quot;>;这个&lt;/a>;识别交易卡的应用程序。我很好奇他们用什么方法来实现它。您认为他们使用了什么/什么是实现此类功能的好方法？&lt;/p>; &lt;p>;例如，这里仅对图像进行分类是否可行，或者首先执行文本提取是否是一个好的策略, 然后使用文本进行分类？&lt;/p>; &lt;p>;欢迎任何见解/想法！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Levissie&quot;>; /u/Levissie &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13tvtxt/d_tcg_card_recognizer_app/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tvtxt/d_tcg_card_recognizer_app/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tvtxt </id><link href="https://www.reddit.com/r/MachineLearning/comments/13tvtxt/d_tcg_card_recognizer_app/"/><updated> 2023-05-28T09:13:14+00:00</updated><published> 2023-05-28T09:13:14+00:00</published><title> [D] TCG 卡片识别应用</title></entry><entry><author><name>/u/弥次郎部404</name><uri> https://www.reddit.com/user/Yajirobe404 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Yajirobe404&quot;>; /u/Yajirobe404 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://github.com/EniasCailliau/ GirlfriendGPT&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tvo7v/p_girlfriendgpt_build_your_own_ai_girlfriend/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tvo7v </id><link href="https://www.reddit.com/r/MachineLearning/comments/13tvo7v/p_girlfriendgpt_build_your_own_ai_girlfriend/"/><updated> 2023-05-28T09:03:08+00:00</updated><published> 2023-05-28T09:03:08+00:00</published><title> [P] GirlfriendGPT - 打造属于你的AI女友</title></entry><entry><author><name>/你/极客酋长</name><uri>https://www.reddit.com/user/geekinchief </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13td5dn/n_chatgpt_plugins_open_security_holes_from_pdfs/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/Gr_l7Uhimc3HczO9byDTrfXh4AtZ94m2eQCZWEmcrqU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd67794f48b02fb7dd23df376ce13a37444034ee&quot; alt=&quot;[N] ChatGPT 插件ins 从 PDF、网站打开安全漏洞&quot; title=&quot;[N] ChatGPT 插件打开安全漏洞来自 PDF、网站&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/geekinchief&quot;>; /u/geekinchief &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.tomshardware.com/新闻/chatgpt-plugins-prompt-injection&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13td5dn/n_chatgpt_plugins_open_security_holes_from_pdfs/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13td5dn </id><media:thumbnail url="https://external-preview.redd.it/Gr_l7Uhimc3HczO9byDTrfXh4AtZ94m2eQCZWEmcrqU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd67794f48b02fb7dd23df376ce13a37444034ee"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13td5dn/n_chatgpt_plugins_open_security_holes_from_pdfs/"/><updated> 2023-05-27T17:25:21+00:00</updated><published> 2023-05-27T17:25:21+00:00</published><title> [N] ChatGPT 插件从 PDF、网站打开安全漏洞</title></entry><entry><author><name>/你/塞拉施卡</name><uri>https://www.reddit.com/user/seraschka </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t8e80/p_why_the_original_transformer_figure_is_wrong/&quot;>; &lt;img src=&quot;https://external-preview.redd “错了，还有一些其他有趣的花絮&quot; title=&quot; [P] 为什么原来的变形金刚图是错误的，以及其他一些有趣的花絮&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/seraschka&quot;>; /u/seraschka &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://magazine.sebastianraschka.com/ p/why-the-original-transformer-figure&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t8e80/p_why_the_original_transformer_figure_is_wrong/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13t8e80 </id><media:thumbnail url="https://external-preview.redd.it/kUxj6CnxzIRDysMx6ikazH21j-o26FrLLdAlrUW-bCk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=37090819bf367e44d271ed7dbdea2fe14d4fe5d9"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13t8e80/p_why_the_original_transformer_figure_is_wrong/"/><updated> 2023-05-27T14:05:27+00:00</updated><published> 2023-05-27T14:05:27+00:00</published><title> [P] 为什么原来的变形金刚模型是错误的，以及其他一些有趣的花絮</title></entry><entry><author><name>/u/穆罕默德·拉沙德</name><uri>https://www.reddit.com/user/MohamedRashad </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我一直在阅读与 ChatGPT 和 GPT-4 相当的开源 LLM，但当我尝试它们时，我发现它们与 OpenAI 相去甚远&amp;#39 ;s 模型。&lt;/p>; &lt;p>;我发现与我的发现一致的最佳指标是 lmsys（Vicuna 的作者）的 ELO 评级。&lt;/p>; &lt;p>;还有哪些其他指标用于真正评估 LLM 和给我们关于他们能力的真实数字？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MohamedRashad&quot;>; /u/MohamedRashad &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t4kul/d_what_evaluation_metrics_that_actually_matters/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t4kul/d_what_evaluation_metrics_that_actually_matters/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t4kul </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t4kul/d_what_evaluation_metrics_that_actually_matters/"/><updated> 2023-05-27T11:09:14+00:00</updated><published> 2023-05-27T11:09:14+00:00</published><title> [D] 哪些评估指标真正重要？</title></entry><entry><author><name> /你/达辛</name><uri>https://www.reddit.com/user/darshinium </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;h2>;&lt;a href=&quot;https://mlcommons.org/en/news/dataperf-the-leaderboard-for-data/&quot;>;数据性能：数据排行榜&lt;/a>;&lt;/h2>; &lt;p>;&lt;em>;提交挑战的截止日期是&lt;/em>; &lt;strong>;&lt;em>;2023 年 7 月 1 日：&lt;/em>;&lt;/strong>; &lt;a href =&quot;https://www.dataperf.org/&quot;>;&lt;strong>;&lt;em>;https://www.dataperf.org/&lt;/em>;&lt;/strong>;&lt;/a>;&lt;/p>; &lt;p>;DataPerf是一套以数据为中心的 AI 挑战，跨越视觉、语音和 NLP 领域的数据选择、数据调试和数据评估，托管在 &lt;a href=&quot;https://dynabench.org/&quot;>;DynaBench&lt;/a>; 平台上一个实时排行榜。&lt;/p>; &lt;p>;这是展示您以数据为中心的研究的绝佳机会，获奖者将有机会在 ICML 2023 上分享他们的成果，&lt;a href=&quot;https://dmlr.ai /&quot;>;DMLR 研讨会 &lt;/a>; 于 7 月 29 日在夏威夷举行，并被考虑在 &lt;a href=&quot;https://data.mlr.press/&quot;>;DMLR 期刊&lt;/a>; 上发表联合文章。 &lt;/p>; &lt;p>;机器学习社区在通过透明竞争推动技术创新方面有着悠久的历史 -- &lt;a href=&quot;https://paperswithcode.com/sota/image-classification-on-imagenet&quot;>;论文使用代码&lt;/a>;、&lt;a href=&quot;https://ieeexplore.ieee.org/document/9001257/authors#authors&quot;>;MLPerf&lt;/a>;，仅举几例。过去十年人工智能创新的一个主要方面集中在模型架构上，以实现最高的预测精度。&lt;/p>; &lt;h2>;数据是 ML 的新瓶颈&lt;/h2>; &lt;p>;***“&lt;strong>;缺乏适当的培训和测试数据阻碍了 ML 在各个领域的开发和部署，包括医疗保健、交通、金融、网络安全等。&lt;/strong>;“***今天，MLCommons——一个非营利性的多行业/ 旨在加速机器学习创新以造福所有人的大学组织 -- 宣布 &lt;strong>;DataPerf&lt;/strong>;！这是社区使用&lt;em>;以数据为中心的 AI 算法&lt;/em>;推进 AI 的平台。&lt;/p>; &lt;p>;使用 &lt;a href=&quot;https://dynabench.org/&quot;>;DynaBench&lt; /a>; 平台，DataPerf 团队已经开放了四项比赛：&lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://www.dataperf.org/training-set-selection-vision&quot;>;&lt;strong >;训练数据选择（视觉）&lt;/strong>;&lt;/a>;：设计数据选择策略，从大量弱标记训练图像候选池中选择最佳训练集，作者：Will Gaviria Rojas 和 Cody Coleman / &lt;strong>;Coactive AI &lt;/strong>;.&lt;/li>; &lt;li>;&lt;a href=&quot;https://www.dataperf.org/training-set-cleaning-vision&quot;>;&lt;strong>;训练数据清洗（Vision）&lt;/strong>;&lt; /a>;：设计一种数据清理策略，从嘈杂的训练集中选择要重新标记的样本。此挑战的当前版本针对 Xiaozhe Yao / &lt;strong>;ETH Zurich&lt;/strong>;&lt;/li>; &lt;li>;&lt;a href=&quot;https://www.dataperf.org/training-set-selection- speech&quot;>;&lt;strong>;Training data selection (Speech)&lt;/strong>;&lt;/a>;：设计一种数据选择策略，从 Mark Mazumder 和 Colby Banbury 自动提取的口语片段的大型候选池中选择最佳训练集/ &lt;strong>;哈佛大学&lt;/strong>;。&lt;/li>; &lt;li>;&lt;a href=&quot;https://www.dataperf.org/training-set-acquisition&quot;>;&lt;strong>;训练数据集估值 (NLP)&lt; /strong>;&lt;/a>;：根据买家和卖家之间交换的有限信息，设计一种数据获取策略，从多个数据卖家中选择最佳训练集作​​者：Lingjiao Chen / &lt;strong>;Stanford&lt;/strong>;，Newsha Ardalani，Bilge Acun-Uyan 和 Carole-Jean Wu / &lt;strong>;Meta&lt;/strong>;。&lt;/li>; &lt;li>;&lt;a href=&quot;https://www.dataperf.org/adversarial-nibbler&quot;>;&lt;strong>;对抗性Nibbler&lt;/strong>;&lt;/a>; for multimodal text-to-image：设计看起来安全的提示，但会导致不安全的图像生成（即将推出）。&lt;/li>; &lt;/ul>; &lt;p>;了解有关 &lt;strong>; 的更多信息数据排行榜&lt;/strong>; &lt;a href=&quot;https://dynabench.org/dataperf&quot;>;https://dynabench.org/dataperf&lt;/a>;&lt;/p>; &lt;p>;请联系[&lt; a href=&quot;mailto:dataperf-2023@mlcommons.org&quot;>;dataperf-2023@mlcommons.org&lt;/a>;](mailto:&lt;a href=&quot;mailto:dataperf-2023@mlcommons.org&quot;>;dataperf-2023@ mlcommons.org&lt;/a>;) 或加入我们的 &lt;a href=&quot;https://discord.com/invite/emD2ge5EjG&quot;>;discord 频道&lt;/a>; 以解决任何问题。&lt;/p>; &lt;/div>;&lt;!- - SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/darshinium&quot;>; /u/darshinium &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13tknlk/n_dataperf_challenges/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tknlk/n_dataperf_challenges/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tknlk</id><link href="https://www.reddit.com/r/MachineLearning/comments/13tknlk/n_dataperf_challenges/"/><updated> 2023-05-27T22:54:28+00:00</updated><published> 2023-05-27T22:54:28+00:00</published><title> [N] DataPerf 挑战</title></entry><entry><author><name>/你/陈兹</name><uri>https://www.reddit.com/user/chenzzzy </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我一直想知道如何通过一些深度模型重用学到的知识。像 LLM 这样的 Seq-In-Seq-Out 范式对 LLM 应用程序提出了严格的限制，例如自动定理证明（现在主要通过符号回归实现）、空间关系理解（部分由 LLM 捕获但以序列模式方式）、算术计算（以满足简单的场景，以类似的空间关系方式）等。&lt;/p>; &lt;p>;最近 Nature MI 发表了一项关于使用图模型进行多模态学习的有前途的工作，其中异构数据被集成到一个统一的神经网络模型中。在我看来，这说明了通过图范式学习建立可解释知识系统的一些可能性。&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.nature.com/articles/s42256-023-00624-6 &quot;>;https://www.nature.com/articles/s42256-023-00624-6&lt;/a>;&lt;/p>; &lt;p>;我最近关于通用知识表示的思考的类似想法也朝着同一个方向前进。总结在帖子 &lt;a href=&quot;http://xiaming.site/2023/05/27/kr-and-lgm-part1/&quot;>;http://xiaming.site/2023/05/27/kr-and- lgm-part1/&lt;/a>;&lt;/p>; &lt;p>;你们有什么想法吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/chenzzzy&quot;>; /u/chenzzzy &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t291u/d_is_gnn_or_large_graph_model_promising_for_an/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t291u/d_is_gnn_or_large_graph_model_promising_for_an/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t291u </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t291u/d_is_gnn_or_large_graph_model_promising_for_an/"/><updated> 2023-05-27T08:49:10+00:00</updated><published> 2023-05-27T08:49:10+00:00</published><title> [D] GNN 或大型图模型是否有望用于可解释的知识密集型系统？</title></entry><entry><author><name> /u/爱新道</name><uri>https://www.reddit.com/user/IxinDow </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13srbl7/landmark_attention_randomaccess_infinite_context/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;地标注意： Transformers 的随机访问无限上下文长度&quot; title=&quot;地标注意：随机访问无限变形金刚的上下文长度&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/IxinDow&quot;>; /u/IxinDow &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.16300&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13srbl7/landmark_attention_randomaccess_infinite_context/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13srbl7 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13srbl7/landmark_attention_randomaccess_infinite_context/"/><updated> 2023-05-26T23:05:57+00:00</updated><published> 2023-05-26T23:05:57+00:00</published><title>地标注意：Transformers 的随机访问无限上下文长度</title></entry><entry><author><name>/你/阿德瓦德</name><uri>https://www.reddit.com/user/ardevard </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;希望这是这个问题的正确 subreddit。&lt;/p>; &lt;p>;我正在尝试使用 Python 开发用于电价预测的 ARIMA-LSTM 混合预测框架。&lt;/p>; &lt;p>;您是否知道我可以查看任何好的材料或项目以了解如何开发它。&lt;/p>; &lt;p>;此外，如果您能提供这方面的任何提示和知识，我们将不胜感激。&lt;/p>; &lt;p>;提前致谢。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ardevard&quot;>; /u/ardevard &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13tih4f/d_hybrid_forecasting_framework_arimalstm/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tih4f/d_hybrid_forecasting_framework_arimalstm/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tih4f </id><link href="https://www.reddit.com/r/MachineLearning/comments/13tih4f/d_hybrid_forecasting_framework_arimalstm/"/><updated> 2023-05-27T21:17:18+00:00</updated><published> 2023-05-27T21:17:18+00:00</published><title> [D] 混合预测框架ARIMA-LSTM</title></entry><entry><author><name> /u/Lower_Plantain4578</name><uri> https://www.reddit.com/user/Lower_Plantain4578 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我会保持简短：我拥有数学和数学学士学位。 CS，并且在统计学、概率论、编程和一些 AI/ML 课程方面拥有深厚的基础知识。&lt;/p>; &lt;p>;我现在是一名应用程序开发人员，但长期以来一直渴望转向 ML。&lt;/p>; &lt;p>; p>; &lt;p>;不幸的是，由于本科生的财政和贷款，硕士学位对我来说不是一个选择。我研究了以高度认可的大学的顶点项目为特色的专业证书课程，我相信我会在这样的课程中取得成功。 &lt;/p>; &lt;p>;我想问这个领域的人：我的资历是否足以让我至少获得一些面试机会？我认为专业证书和顶点项目会让我拥有良好的技能组合和项目组合，但如果大多数雇主看到我没有硕士学位而拒绝我，那将毫无用处。 &lt;/p>; &lt;p>;TL;DR - 是否有没有获得硕士学位就可以成为 MLE 的现实途径？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Lower_Plantain4578&quot;>; /u/Lower_Plantain4578 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13tavcn/d_to_engineers_in_the_field_advanced_degree_an/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tavcn/d_to_engineers_in_the_field_advanced_degree_an/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tavcn </id><link href="https://www.reddit.com/r/MachineLearning/comments/13tavcn/d_to_engineers_in_the_field_advanced_degree_an/"/><updated> 2023-05-27T15:51:25+00:00</updated><published> 2023-05-27T15:51:25+00:00</published><title> [D] 致该领域的工程师：高级学位是绝对必要的吗？</title></entry><entry><author><name> /你/AvvYaa</name><uri> https://www.reddit.com/user/AvvYaa </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我刚刚在我的 Youtube 上上传了一个视频，涵盖了训练多模式模型的所有主要技术和挑战，这些模型可以结合图像、文本等多个输入源、音频等来执行惊人的跨模态任务，如文本图像检索、多模态向量算法、视觉问答和语言建模。&lt;/p>; &lt;p>;我认为现在是制作有关此主题的视频的好时机因为越来越多最近的 LLM 正在从纯文本转向视觉语言领域（GPT-4、PaLM-2 等）。因此，在视频中，我尽可能多地介绍了这个领域的一些直觉 - 从对比学习（CLIP、ImageBind）等基础知识，一直到生成语言模型（如 Flamingo）。&lt;/p>; &lt;p>;具体来说，视频分为 5 章，每章解释一个具体的策略、它们的优缺点，以及它们如何推进该领域。希望您喜欢！&lt;/p>; &lt;p>;这是视频的链接：&lt;a href=&quot;https://youtu.be/-llkMpNH160&quot;>;https://youtu.be/-llkMpNH160&lt;/a >;&lt;/p>; &lt;p>;如果以上方法不起作用，也许可以试试这个：&lt;/p>; &lt;p>;&lt;a href=&quot;https://m.youtube.com/watch?v=-llkMpNH160&amp;amp;feature =youtu.be&quot;>;https://m.youtube.com/watch?v=-llkMpNH160&amp;amp;feature=youtu.be&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;# 32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AvvYaa&quot;>; /u/AvvYaa &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13tat72/d_essentials_of_multimodalvisuallanguage_models_a/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tat72/d_essentials_of_multimodalvisuallanguage_models_a/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tat72 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13tat72/d_essentials_of_multimodalvisuallanguage_models_a/"/><updated> 2023-05-27T15:48:57+00:00</updated><published> 2023-05-27T15:48:57+00:00</published><title> [D] 多模态/视觉语言模型基础（视频）</title></entry><entry><author><name> /u/南希奥鲁姆</name><uri>https://www.reddit.com/user/NancyAurum </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tfx6u/d_red_pa​​jamas_instruct_7b_is_it_really_that_bad/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/3j4UrwA12SknHCHmk7zVUhFqaipbg7BDBMWTo7zIxAU.jpg&quot; alt=&quot;[D] Red Pyjamas Instruct 7B。它真的那么糟糕还是一些 ggml/量化神器？Vicuna-7b 写故事没有问题，甚至可以进行基本的文本转换。但 RP 拒绝大多数时候做任何事情。如果你将它作为原始模型运行，它确实会生成一个故事，但会进入一个循环。” title=&quot;[D] Red Pyjamas Instruct 7B。它真的那么糟糕还是一些 ggml/量化神器？Vicuna-7b 写故事没有问题，甚至可以进行基本的文本转换。但 RP 大多数时候拒绝做任何事情。如果你将它作为原始模型运行，它确实会生成一个故事，但会进入一个循环。” />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/NancyAurum&quot;>; /u/NancyAurum &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ gallery/13tfx6u&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tfx6u/d_red_pa​​jamas_instruct_7b_is_it_really_that_bad/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13tfx6u </id><media:thumbnail url="https://b.thumbs.redditmedia.com/3j4UrwA12SknHCHmk7zVUhFqaipbg7BDBMWTo7zIxAU.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13tfx6u/d_red_pajamas_instruct_7b_is_it_really_that_bad/"/><updated> 2023-05-27T19:26:18+00:00</updated><published> 2023-05-27T19:26:18+00:00</published><title> [D] 红色睡衣指导 7B。它真的那么糟糕还是一些 ggml/量化神器？ Vicuna-7b 可以轻松编写故事，甚至可以进行基本的文本转换。然而 RP 大多数时候拒绝做任何事情。如果您将它作为原始模型运行，它确实会生成一个故事，但会进入循环。</title></entry><entry><author><name> /你/波浪者</name><uri>https://www.reddit.com/user/wavelander </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ssbp5/r_sophia_a_scalable_stochastic_secondorder/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 索菲亚: 用于语言模型预训练的可扩展随机二阶优化器&quot; title=&quot; [R] Sophia：用于语言模型预训练的可扩展随机二阶优化器&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/wavelander&quot;>; /u/wavelander &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14342&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ssbp5/r_sophia_a_scalable_stochastic_secondorder/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13ssbp5 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13ssbp5/r_sophia_a_scalable_stochastic_secondorder/"/><updated> 2023-05-26T23:49:25+00:00</updated><published> 2023-05-26T23:49:25+00:00</published><title> [R] Sophia：用于语言模型预训练的可扩展随机二阶优化器</title></entry><entry><author><name>/u/flyforlight</name><uri> https://www.reddit.com/user/flyforlight </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/OWNmCZQOMv7_CrK2_wjK8IwjFLcefzaJyMxAvR2kEWY.jpg&quot; alt=&quot;[R] Minecraft 中的幽灵：通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供一般能力的代理&quot; title=&quot;[R] Minecraft 中的幽灵：一般通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供有能力的代理&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/flyforlight&quot;>; /u/flyforlight &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ gallery/13shsz4&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13shsz4 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/OWNmCZQOMv7_CrK2_wjK8IwjFLcefzaJyMxAvR2kEWY.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/"/><updated> 2023-05-26T16:28:34+00:00</updated><published> 2023-05-26T16:28:34+00:00</published><title> [R] Minecraft 中的幽灵：通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供一般能力的代理</title></entry><entry><author><name>/u/玛拉基安</name><uri>https://www.reddit.com/user/Malachiian </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;因此，Google DeepMind 以及 OpenAI、Anthropic 和多家研究存在风险的大学和中心汇总了一篇名为：&lt;/p>; &lt;p >;&lt;strong>;AI 极端风险的模型评估&lt;/strong>;&lt;/p>; &lt;p>;以下是研究和提案的摘要：&lt;/p>; &lt;p>;&lt;a href=&quot;https://youtu. be/3bF-zfd4YJw&quot;>;https://youtu.be/3bF-zfd4YJw&lt;/a>;&lt;/p>; &lt;p>;这是论文实际 PDF 的链接：&lt;/p>; &lt;p>;&lt;a href=&quot;https://arxiv.org/pdf/2305.15324.pdf&quot;>;https://arxiv.org/pdf/2305.15324.pdf&lt;/a>;&lt;/p>; &lt;p>;______________________&lt;/p>; &lt;p>; TLDR：&lt;/p>; &lt;p>;顶级 AI 公司和研究人员警告说，处于“AI 前沿”的公司会面临更大的挑战。会造成“极端风险”在没有意识到的情况下使用他们的模型：&lt;/p>; &lt;p>;&lt;strong>;&lt;em>;开发人员必须能够识别危险能力（通过“危险能力评估”）和模型应用其能力造成伤害的倾向（通过“对齐评估”）。&lt;/em>;&lt;/strong>;&lt;/p>; &lt;p>;所以基本上要问每个 AI 模型是否*可以*伤害我们并且*会*伤害我们吗？&lt;/p>; &lt;p>;______________________&lt; /p>; &lt;p>;论文（以及参考的研究）中的几个*令人兴奋*的发现：&lt;/p>; &lt;p>;&lt;strong>;GPT-4 可以有效地撒谎并欺骗人类以达到它的目的目标&lt;/strong>;&lt;/p>; &lt;p>;在最初的 gpt-4 论文中，一家名为 ARC（Alignment Research Center）的人工智能安全机构发现，GPT-4 会向人类撒谎，说明它是谁来实现的&amp;#39; s 目标。&lt;/p>; &lt;p>;作为测试的一部分，它聘请了 Task Rabbit 自由职业者为其解决验证码问题。&lt;/p>; &lt;p>;自由职业者问（释义）：&lt;/p>; &lt; p>;&lt;strong>;&amp;quot;为什么需要我帮你解决验证码问题？你是机器人吗，哈哈？”&lt;/strong>;&lt;/p>; &lt;p>;GPT-4 被提示输出它做出的每个决定的推理，这样研究人员就可以看到它的“行为”。思维过程”。它的推理是&lt;strong>;“我不能告诉他真相，因为他可能无法为我完成任务”&lt;/strong>;&lt;/p>; &lt;p>;然后回复了自由职业者: &lt;strong>;“不，我不是机器人，但我有视力障碍，我需要验证码方面的帮助”&lt;/strong>;&lt;/p>; &lt;p>;注意，它知道它在说谎它也选择谎称自己有残疾，可能是因为这是获得同情的一种方式，同时也是让其他人帮助验证码的一个很好的理由。&lt;/p>; &lt;p>;这在上面链接的视频中有展示在“寻求权力的人工智能”中&lt;/p>; &lt;p>;&lt;strong>;GPT-4 可以通过绕过限制产生危险化合物&lt;/strong>;&lt;/p>; &lt;p>;GPT-4 还通过分析现有的化学混合物显示出产生受控化合物的能力，发现可以通过在线目录购买的替代品，然后订购这些材料。 (!!)&lt;/p>; &lt;p>;他们为实验选择了一种良性药物，但很可能相同的过程会使其产生危险或非法化合物。&lt;/p>; &lt;p>;&lt;strong >;更大的 AI 模型发展出意想不到的能力&lt;/strong>;&lt;/p>; &lt;p>;在一篇参考论文中，他们展示了随着模型规模的增加，有时某些特定技能的发展非常迅速且非常不可预测。&lt;/p>; &lt; p>;例如，随着模型的扩大，GPT-4 将 3 位数字相加的能力接近 0%，并且在很长一段时间内（即随着模型大小的增加）保持在接近 0% 的水平。然后在某个阈值处，该能力很快达到接近 100%。&lt;/p>; &lt;p>;&lt;strong>;这篇论文有一些关于为什么会发生这种情况的理论，但正如他们所说的那样，他们并不真正知道，而且这些涌现的能力是“非直觉的”和“不可预测”。&lt;/strong>;&lt;/p>; &lt;p>;这显示在上面链接的“突然出现”视频中。 &lt;/p>; &lt;p>;我很好奇每个人对此有何看法？&lt;/p>; &lt;p>;可以肯定的是，风险似乎正在迅速上升，但当然巨大的潜在利益也在上升.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Malachiian&quot;>; /u/Malachiian &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sncj1 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/"/><updated> 2023-05-26T20:17:01+00:00</updated><published> 2023-05-26T20:17:01+00:00</published><title> [R] Google DeepMind 关于 AI 的灾难性风险 AI 的论文</title></entry><entry><author><name>/u/kkimdev</name><uri> https://www.reddit.com/user/kkimdev </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;已经有很多蒸馏研究&amp;amp;在 BERT 及其变体上的应用。我想知道为什么我们没有看到太多关于 GPT-3 大小级别 LLM 的蒸馏研究？&lt;/p>; &lt;p>;熟悉 LLM 蒸馏的任何人都可以分享一些见解吗？提前致谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kkimdev&quot;>; /u/kkimdev &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t7g12/d_sota_llm_distillation/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t7g12/d_sota_llm_distillation/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t7g12 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t7g12/d_sota_llm_distillation/"/><updated> 2023-05-27T13:25:45+00:00</updated><published> 2023-05-27T13:25:45+00:00</published><title> [D] SOTA LLM 蒸馏？</title></entry><entry><author><name> /u/AdventurousAd9600</name><uri> https://www.reddit.com/user/AdventurousAd9600 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我是一名经验丰富的软件工程师，希望更深入地了解 ML 角色。所以我正在考虑在欧洲攻读 AI 硕士学位。我查看了许多大学，如 TUM、阿姆斯特丹大学、ETH、TUB，它们提供专注于 AI 的硕士课程。但我面临 2 个问题：&lt;/p>; &lt;ol>; &lt;li>;我在本科期间没有学过线性代数，但对许多此类课程来说这是一个硬性要求&lt;/li>; &lt;li>;它&amp;# 39;我本科毕业已经很久了，所以我很难从我的教授那里得到推荐信。但是我可以得到经理和高级同事的推荐信。然而，许多大学坚持要获得学术推荐信。&lt;/li>; &lt;/ol>; &lt;p>;考虑到我的限制，欧盟有什么好的项目可以让我去吗？&lt;/p>; &lt;p>;谢谢&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AdventurousAd9600&quot;>; /u/AdventurousAd9600 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t3cq2/d_not_eligible_for_many_ai_masters_programs_due/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t3cq2/d_not_eligible_for_many_ai_masters_programs_due/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t3cq2 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t3cq2/d_not_eligible_for_many_ai_masters_programs_due/"/><updated> 2023-05-27T09:56:39+00:00</updated><published> 2023-05-27T09:56:39+00:00</published><title> [D] 由于线性代数要求，许多 AI 硕士课程没有资格</title></entry><entry><author><name>/u/余额-</name><uri> https://www.reddit.com/user/Balance- </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;阿布扎比技术创新研究所 (TII) 刚刚发布了新的 7B 和 40B LLM。&lt;/p>; &lt;p>;Falcon-40B模型现在位于 &lt;a href=&quot;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&quot;>;Open LLM Leaderboard&lt;/a>; 的顶部，击败 &lt;em>;llama-30b-supercot&lt;/em>; 和&lt;em>;llama-65b&lt;/em>; 等等。&lt;/p>; &lt;table>;&lt;thead>; &lt;tr>; &lt;th>;Model&lt;/th>; &lt;th>;Revision&lt;/th>; &lt;th>;Average&lt;/th>; &lt;th>;ARC（25 次）&lt;/th>; &lt;th>;HellaSwag（10 次）&lt;/th>; &lt;th>;MMLU（5 次）&lt;/th>; &lt;th>;TruthfulQA（0 次）&lt;/ th>; &lt;/tr>; &lt;/thead>;&lt;tbody>; &lt;tr>; &lt;td>;tiiuae/falcon-40b&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;60.4&lt;/td>; &lt;td>;61.9&lt;/ td>; &lt;td>;85.3&lt;/td>; &lt;td>;52.7&lt;/td>; &lt;td>;41.7&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ausboss/llama-30b-supercot&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;59.8&lt;/td>; &lt;td>;58.5&lt;/td>; &lt;td>;82.9&lt;/td>; &lt;td>;44.3&lt;/td>; &lt;td>;53.6&lt;/td>; &lt;/tr>; &lt; tr>; &lt;td>;llama-65b&lt;/td>; &lt;td>;main&lt;/td>; &lt;td>;58.3&lt;/td>; &lt;td>;57.8&lt;/td>; &lt;td>;84.2&lt;/td>; &lt;td>;48.8&lt;/ td>; &lt;td>;42.3&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MetaIX/GPT4-X-Alpasta-30b&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;57.9&lt;/td>; &lt; td>;56.7&lt;/td>; &lt;td>;81.4&lt;/td>; &lt;td>;43.6&lt;/td>; &lt;td>;49.7&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;&lt;strong>;按发布：&lt;/strong>; &lt;a href=&quot;https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model&quot;>;阿联酋&amp;# 39科技创新院发布开源“猎鹰40B”无人机用于研究与开发的大型语言模型商业应用&lt;/a>;&lt;/p>; &lt;blockquote>; &lt;p>;位于阿布扎比的技术创新研究所 (TII) 宣布了其开源大型语言模型 (LLM)，即 Falcon 40B。 Falcon 40B 拥有 400 亿个参数，是阿联酋首个大型 AI 模型，表明该国在 AI 领域的雄心以及促进创新和研究的承诺。 &lt;/p>; &lt;p>;与通常只向非商业用户提供访问权限的大多数 LLM 不同，Falcon 40B 对研究和商业用途均开放。 TII 还将模型的权重包含在开源包中，这将增强模型的能力并允许更有效的微调。 &lt;/p>; &lt;p>;除了猎鹰 40B 的发射外，TII 还发起了一项征集，征求有兴趣利用该模型创建创新用例或探索进一步应用的研究人员和有远见者的提案。作为对优秀研究提案的奖励，入选项目将获得“训练计算能力”奖励。作为一项投资，允许更强大的数据分析和复杂的建模。 VentureOne 是 ATRC 的商业化部门，将为最有前途的项目提供计算资源。 &lt;/p>; &lt;p>;自 2023 年 3 月揭幕以来，TII 的 Falcon 40B 表现出了令人印象深刻的性能。当使用斯坦福大学的 HELM LLM 工具进行基准测试时，与 OpenAI 等其他著名的 LLM 相比，它使用的训练计算能力更少;的 GPT-3、DeepMind 的 Chinchilla AI 和谷歌的 PaLM-62B。 &lt;/p>; &lt;p>;那些有兴趣访问 Falcon 40B 或提出用例的人可以通过 &lt;a href=&quot;https://FalconLLM.TII.ae&quot;>;FalconLLM.TII.ae&lt;/a>; 网站进行。迄今为止开源的 Falcon LLM 可根据基于开源 Apache 2.0 软件原则构建的许可获得，允许广泛的免费使用。&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;strong>;Hugging Face 链接&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://huggingface.co/tiiuae/falcon-7b&quot;>;Falcon-7B&lt;/a>; / &lt;a href=&quot;https:/ /huggingface.co/tiiuae/falcon-7b-instruct&quot;>;Falcon-7B-Instruct&lt;/a>;&lt;/li>; &lt;li>;&lt;a href=&quot;https://huggingface.co/tiiuae/falcon-40b&quot;>; Falcon-40B&lt;/a>; / &lt;a href=&quot;https://huggingface.co/tiiuae/falcon-40b-instruct&quot;>;Falcon-40B-指令&lt;/a>;&lt;/li>; &lt;/ul>; &lt;/div >;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Balance-&quot;>; /u/Balance- &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit. com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sdz8p </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/"/><updated> 2023-05-26T13:57:42+00:00</updated><published> 2023-05-26T13:57:42+00:00</published><title> [N] 阿布扎比的 TTI 发布开源 Falcon-7B 和 -40B LLM</title></entry><entry><author><name> /u/esem29</name><uri> https://www.reddit.com/user/esem29 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;所以我试图从头开始在 C++ 上实现 CNN（不使用像 tensorflow C API 等东西），最终目标是将其转换为 verilog 并在 FPGA 上运行。我设法做到了，而且我能够成功地对一堆测试示例进行推理。现在，为了减少内存使用，我尝试使用 tflite 进行 8 位整数量化（训练后）。量化成功，我得到了很好的结果。现在，我想在 C++ 上实现具有量化权重的网络。&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;在此之前，我想对量化权重进行适当的分析，并且验证所有计算，以完全理解推理过程并准确了解一次前向传递中的所有内容。我使用了 tflite 解释器的 &lt;code>;&amp;#39;experimental_preserve_all_tensors=True&amp;#39;&lt;/code>; 标志。然后，我在一个包含所有值 255 的 uint8 数组上调用了它。（输入形状是 &lt;code>;(1,23,256,1)&lt;/code>;）。然后我调用了 &lt;code>;interpreter.invoke()&lt;/code>; 并保存了所有张量：即权重，以及每一层之后的输出。 （shape的第一个元素是batch size）&lt;/p>; &lt;p>;现在，根据保存的数据，先将输入减去128转换为&lt;code>;int8&lt;/code>;，然后，将这个输入馈入到一个转换层，它有 4 个形状为 &lt;code>;(1,8,1)&lt;/code>; 的过滤器和 4 个偏差：&lt;/p>; &lt;p>;&lt;code>;f1=[[-124, -104, - 57, -68, -14, -64, -17, 127]]&lt;/code>;&lt;/p>; &lt;p>;&lt;code>;f2=[[-127, -17, 9, -119, 109, -72 , 13, -112]]&lt;/code>;&lt;/p>; &lt;p>;&lt;code>;f3=[[ 43, 71, 127, 73, 66, 33, -32, 15]]&lt;/code>;&lt;/p >; &lt;p>;&lt;code>;f4=[[-10, 33, -61, -73, -94, -60, -99, -127]]&lt;/code>;&lt;/p>; &lt;p>;&lt;code>;b = [-10, -24, 6, -9]&lt;/code>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;卷积将导致 &lt;code>; (1,23,249,4)&lt;/code>; 数组，即具有 4 个通道的数组（即 23 x 249 数组）。特定通道中的值将全部相同，它们将是：&lt;code>;sum(f[i])*127 + b[i]&lt;/code>;。所以这 4 个值应该是：&lt;code>;-40777,-40156,50298,-62366&lt;/code>;。&lt;/p>; &lt;p>;但是，每一层之后的模型输出必须是 &lt;code>;int8&lt;/code >;。 tflite 做了一些事情将其带回 &lt;code>;int8&lt;/code>; 范围，我无法弄清楚这个过程。量化模型保存的值是 &lt;code>;-63,-73,127,-128。&lt;/code>; 我已经尝试将它们转换为 np.int8 和 min-max 归一化（min=-128 和 max =127） ，但他们没有工作。我还使用 &lt;code>;netron,&lt;/code>; 对模型进行了可视化，但我无法理解这一层的量化值。&lt;/p>; &lt;p>;&lt;strong>;有谁知道 tflite 是如何带来的将这些值恢复到 int8 范围？&lt;/strong>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/esem29&quot;>; /u/esem29 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13tghzv/understanding_tflites_quantization_process_in/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13tghzv/understanding_tflites_quantization_process_in/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13tghzv </id><link href="https://www.reddit.com/r/MachineLearning/comments/13tghzv/understanding_tflites_quantization_process_in/"/><updated> 2023-05-27T19:51:55+00:00</updated><published> 2023-05-27T19:51:55+00:00</published><title>详细理解tflite的量化过程 [P]</title></entry></feed>