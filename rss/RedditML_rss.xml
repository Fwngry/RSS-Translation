<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-26T14:13:11+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /u/让-波特</name><uri>https://www.reddit.com/user/Jean-Porte </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 法尔se 模仿专有 LLM 的承诺&quot; title=&quot;[R] 模仿的虚假承诺专有法学硕士&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Jean-Porte&quot;>; /u/Jean-Porte &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv. org/abs/2305.15717&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13s6pb7 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/"/><updated> 2023-05-26T07:49:29+00:00</updated><published> 2023-05-26T07:49:29+00:00</published><title> [R] 模仿专有 LLM 的虚假承诺</title></entry><entry><author><name>/你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/neuralink-receives-fda-approval-to-launch-first -in-human-clinical-trials-e373e7b5fcf1&quot;>;https://medium.com/@tiago-mesquita/neuralink-receives-fda-approval-to-launch-first-in-human-clinical-trials-e373e7b5fcf1&lt;/ a>;&lt;/p>; &lt;p>;Neuralink 表示尚未招募参与者，更多信息将很快公布。&lt;/p>; &lt;p>;想法？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s85rb </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/"/><updated> 2023-05-26T09:20:57+00:00</updated><published> 2023-05-26T09:20:57+00:00</published><title> [N] Neuralink 刚刚获得 FDA 的绿灯，可以继续其首次人体临床试验</title></entry><entry><author><name>/u/I_will_delete_myself</name><uri> https://www.reddit.com/user/I_will_delete_myself </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我犹豫了一会儿，但听到这个消息后虚伪让我发疯。&lt;/p>; &lt;p>;SMH 这家公司就像白衣骑士一样，他们认为他们凌驾于所有人之上。他们想要监管，但他们希望不受该监管的影响。只想伤害其他人，但不想伤害“全能的”Sam 和朋友。&lt;/p>; &lt;p>;向国会撒谎说建议在欧盟采取类似的做法，但现在开始抱怨他们。在任何政治领域都不应该认真对待这个家伙。&lt;/p>; &lt;p>;我的观点是，这家公司通过锁定与其品牌名称相悖的东西来反对 AI 进步。如果他们甚至不能忠于这样简单的事情，我们怎么能指望他们忠于更难的 AI 安全？&lt;/p>; &lt;p>;我很高兴他们现在改变了立场，但我很高兴他们如何他们认为他们有权为了自己的利益而腐败。 SMH!!!!!!!!&lt;/p>; &lt;p>;你有什么想法？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/I_will_delete_myself&quot;>; /u/I_will_delete_myself &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rie0e </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/"/><updated> 2023-05-25T13:51:58+00:00</updated><published> 2023-05-25T13:51:58+00:00</published><title> OpenAI 现在抱怨人工智能的监管 [D]</title></entry><entry><author><name> /u/theoneandonlypatriot</name><uri> https://www.reddit.com/user/theoneandonlypatriot </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我正在参加 SWE 工作的面试过程，有几个人直接评判我，甚至公然说他们不是 AI 的粉丝因为我在 AI / ML 工作方面的背景。&lt;/p>; &lt;p>;发这篇文章是为了让人们知道工程社区中存在这种观点和负面看法。&lt;/p>; &lt;p>;考虑到我也分享了很多东西，感觉很糟糕人工智能的伦理问题。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/theoneandonlypatriot&quot;>; /u/theoneandonlypatriot&lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s32d4 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/"/><updated> 2023-05-26T04:25:57+00:00</updated><published> 2023-05-26T04:25:57+00:00</published><title> [D] 对 AI 的负面评价</title></entry><entry><author><name>/u/Mr_Whispers</name><uri> https://www.reddit.com/user/Mr_Whispers </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s8z41/deepmind_model_evaluation_for_extreme_risks/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;DeepMind: 模型极端风险评估&quot; title=&quot;DeepMind：极端风险模型评估&quot; />; &lt; /a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Mr_Whispers&quot;>; /u/Mr_Whispers &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.15324&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s8z41/deepmind_model_evaluation_for_extreme_risks/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13s8z41 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13s8z41/deepmind_model_evaluation_for_extreme_risks/"/><updated> 2023-05-26T10:08:23+00:00</updated><published> 2023-05-26T10:08:23+00:00</published><title> DeepMind：极端风险的模型评估</title></entry><entry><author><name>/u/余额-</name><uri> https://www.reddit.com/user/Balance- </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;阿布扎比技术创新研究所 (TII) 刚刚发布了新的 7B 和 40B LLM。&lt;/p>; &lt;p>;Falcon-40B模型现在位于 &lt;a href=&quot;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&quot;>;Open LLM Leaderboard&lt;/a>; 的顶部，击败 &lt;em>;llama-30b-supercot&lt;/em>; 和&lt;em>;llama-65b&lt;/em>; 等等。&lt;/p>; &lt;table>;&lt;thead>; &lt;tr>; &lt;th>;Model&lt;/th>; &lt;th>;Revision&lt;/th>; &lt;th>;Average&lt;/th>; &lt;th>;ARC（25 次）&lt;/th>; &lt;th>;HellaSwag（10 次）&lt;/th>; &lt;th>;MMLU（5 次）&lt;/th>; &lt;th>;TruthfulQA（0 次）&lt;/ th>; &lt;/tr>; &lt;/thead>;&lt;tbody>; &lt;tr>; &lt;td>;tiiuae/falcon-40b&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;60.4&lt;/td>; &lt;td>;61.9&lt;/ td>; &lt;td>;85.3&lt;/td>; &lt;td>;52.7&lt;/td>; &lt;td>;41.7&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ausboss/llama-30b-supercot&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;59.8&lt;/td>; &lt;td>;58.5&lt;/td>; &lt;td>;82.9&lt;/td>; &lt;td>;44.3&lt;/td>; &lt;td>;53.6&lt;/td>; &lt;/tr>; &lt; tr>; &lt;td>;llama-65b&lt;/td>; &lt;td>;main&lt;/td>; &lt;td>;58.3&lt;/td>; &lt;td>;57.8&lt;/td>; &lt;td>;84.2&lt;/td>; &lt;td>;48.8&lt;/ td>; &lt;td>;42.3&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MetaIX/GPT4-X-Alpasta-30b&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;57.9&lt;/td>; &lt; td>;56.7&lt;/td>; &lt;td>;81.4&lt;/td>; &lt;td>;43.6&lt;/td>; &lt;td>;49.7&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;&lt;strong>;按发布：&lt;/strong>; &lt;a href=&quot;https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model&quot;>;阿联酋&amp;# 39科技创新院发布开源“猎鹰40B”无人机用于研究与开发的大型语言模型商业应用&lt;/a>;&lt;/p>; &lt;blockquote>; &lt;p>;位于阿布扎比的技术创新研究所 (TII) 宣布了其开源大型语言模型 (LLM)，即 Falcon 40B。 Falcon 40B 拥有 400 亿个参数，是阿联酋首个大型 AI 模型，表明该国在 AI 领域的雄心以及促进创新和研究的承诺。 &lt;/p>; &lt;p>;与通常只向非商业用户提供访问权限的大多数 LLM 不同，Falcon 40B 对研究和商业用途均开放。 TII 还将模型的权重包含在开源包中，这将增强模型的能力并允许更有效的微调。 &lt;/p>; &lt;p>;除了猎鹰 40B 的发射外，TII 还发起了一项征集，征求有兴趣利用该模型创建创新用例或探索进一步应用的研究人员和有远见者的提案。作为对优秀研究提案的奖励，入选项目将获得“训练计算能力”奖励。作为一项投资，允许更强大的数据分析和复杂的建模。 VentureOne 是 ATRC 的商业化部门，将为最有前途的项目提供计算资源。 &lt;/p>; &lt;p>;自 2023 年 3 月揭幕以来，TII 的 Falcon 40B 表现出了令人印象深刻的性能。当使用斯坦福大学的 HELM LLM 工具进行基准测试时，与 OpenAI 等其他著名的 LLM 相比，它使用的训练计算能力更少;的 GPT-3、DeepMind 的 Chinchilla AI 和谷歌的 PaLM-62B。 &lt;/p>; &lt;p>;那些有兴趣访问 Falcon 40B 或提出用例的人可以通过 &lt;a href=&quot;https://FalconLLM.TII.ae&quot;>;FalconLLM.TII.ae&lt;/a>; 网站进行。迄今为止开源的 Falcon LLM 可根据基于开源 Apache 2.0 软件原则构建的许可获得，允许广泛的免费使用。&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;strong>;Hugging Face 链接&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://huggingface.co/tiiuae/falcon-7b&quot;>;Falcon-7B&lt;/a>; / &lt;a href=&quot;https:/ /huggingface.co/tiiuae/falcon-7b-instruct&quot;>;Falcon-7B-Instruct&lt;/a>;&lt;/li>; &lt;li>;&lt;a href=&quot;https://huggingface.co/tiiuae/falcon-40b&quot;>; Falcon-40B&lt;/a>; / &lt;a href=&quot;https://huggingface.co/tiiuae/falcon-40b-instruct&quot;>;Falcon-40B-指令&lt;/a>;&lt;/li>; &lt;/ul>; &lt;/div >;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Balance-&quot;>; /u/Balance- &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit. com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sdz8p </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/"/><updated> 2023-05-26T13:57:42+00:00</updated><published> 2023-05-26T13:57:42+00:00</published><title> [N] 阿布扎比的 TTI 发布开源 Falcon-7B 和 -40B LLM</title></entry><entry><author><name> /你/阿杜纳托</name><uri>https://www.reddit.com/user/adunato </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;最近，我一直在处理几个使用 PyTorch 的 GitHub 项目。对于每个项目，我维护一个单独的 Conda 环境（我通过艰难的方式了解到为什么这很重要）。&lt;/p>; &lt;p>;但是，我遇到的一个持续存在的问题涉及 PyTorch 与我的 CUDA 的兼容性版本。具体来说，通过 requirements.txt 文件安装的 PyTorch 版本通常与我的 CUDA 版本不兼容，导致无法识别 CUDA 设备。&lt;/p>; &lt;p>;为了解决这个问题，我采用了一种做法我从 requirements.txt 文件中删除了对 PyTorch（以及相关库，如 torchvision、torchaudio）的任何提及，并从官方 PyTorch 站点手动安装它。&lt;/p>; &lt;p>;这是一种常见做法吗？或者我错过了一个更简化的工作流程来确保 PyTorch 和 CUDA 的兼容性？我很想听听其他人是如何处理这个问题的。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/adunato&quot;>; /u/adunato &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s6x3b </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/"/><updated> 2023-05-26T08:02:39+00:00</updated><published> 2023-05-26T08:02:39+00:00</published><title> [D] 安装 PyTorch 以与特定 CUDA 版本保持一致的最佳实践</title></entry><entry><author><name>/u/Simple-Respect-1937</name><uri> https://www.reddit.com/user/Simple-Respect-1937 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，大家好！&lt;/p>; &lt;p>;我和我的团队正在进行人脸识别项目。我们所做的是，我们从实时摄像机中提取人脸图像，然后使用 Facenet 为每张脸进行嵌入。这些嵌入是向量。因此，通过测量两个向量（两个人脸图像的嵌入）之间的距离，我们可以判断这两张图像是否来自同一个人。这是我们阅读论文时人脸识别的正常程序。 &lt;/p>; &lt;p>;但是我们遇到的是，我们为印度人脸运行程序设置的阈值对东亚（中国）人脸不起作用，尽管它对印度人脸有效。所以我们也尝试阅读一些研究论文。那些论文也是如此，承认存在这样的问题。 &lt;/p>; &lt;p>;&lt;strong>;我只是想知道以前是否有人遇到过完全相同的问题。如果有的话，那么你采用了什么方法？&lt;/strong>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;我对 Reddit 有点陌生，所以如果我做了任何提问时出错，请见谅。谢谢大家！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Simple-Respect-1937&quot;>; /u/Simple-Respect-1937 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13s80ev/face_recognition_models_require_different/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s80ev/face_recognition_models_require_different/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s80ev </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s80ev/face_recognition_models_require_different/"/><updated> 2023-05-26T09:11:37+00:00</updated><published> 2023-05-26T09:11:37+00:00</published><title>人脸识别模型对不同种族需要不同的阈值？ [D]</title></entry><entry><author><name> /u/知道杰罗姆</name><uri>https://www.reddit.com/user/iknowjerome </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;表>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sd4ku/r_samadrivescalifornia_automotive_semantic/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/6mKRKGxChyyetC3FAKtYmWYw3zxEdQM6gd3Tjw1DuwI.jpg&quot; alt=&quot;[R] sama-drives-california：汽车语义分割数据集（25k 帧）现已可用&quot; title=&quot;[R] sama-drives-california：汽车语义分割数据集（25k 帧）现在可用&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;Sama 刚刚发布了另一个数据集根据 Creative Commons 4.0 许可。它可以在 Hugging Face 上找到。您可以查看 Hugging Face &lt;a href=&quot;https://huggingface.co/datasets/SamaAI/sama-drives-california&quot;>;数据集卡&lt;/a>;了解更多详细信息。如果您想直接下载 BDD100K 格式而不通过 Hugging Face，这里是 &lt;a href=&quot;https://sama-documentation-assets.s3.amazonaws.com/sama-drives -california/zipped/sama-drives-california.zip&quot;>;zip 文件&lt;/a>; (2.3GB)。请随时告诉我您的想法。&lt;/p>; &lt;p>;&lt;em>;免责声明：我为 Sama 工作&lt;/em>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/op4hdkqjf62b1.png?width=2239&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb3b66a194fc29c34fe42167d6b78af537b4bc7&quot;>;样本帧&lt;/a>;&lt;/p>; &lt;/div>;&lt;! -- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/iknowjerome&quot;>; /u/iknowjerome &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sd4ku/r_samadrivescalifornia_automotive_semantic/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sd4ku/r_samadrivescalifornia_automotive_semantic/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13sd4ku </id><media:thumbnail url="https://b.thumbs.redditmedia.com/6mKRKGxChyyetC3FAKtYmWYw3zxEdQM6gd3Tjw1DuwI.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13sd4ku/r_samadrivescalifornia_automotive_semantic/"/><updated> 2023-05-26T13:21:34+00:00</updated><published> 2023-05-26T13:21:34+00:00</published><title> [R] sama-drives-california：汽车语义分割数据集（25k 帧）现已可用</title></entry><entry><author><name>/u/rwill128</name><uri> https://www.reddit.com/user/rwill128 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;有人知道与此主题相关的论文吗？ &lt;/p>; &lt;p>;看起来像法学硕士，尤其是即将成为多模态的，可以与传感器和相机输入密切相关，可以成为规划和高级考虑的强大工具，例如识别某些任务的机会等.&lt;/p>; &lt;p>;从我在 HuggingFace 论文等中看到的情况来看，LLM 的进展可能还没来得及深入机器人技术，但我想我会问。&lt;/p>; &lt;/ div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/rwill128&quot;>; /u/rwill128 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13scb1b/d_llms_in_robotics/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13scb1b/d_llms_in_robotics/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13scb1b </id><link href="https://www.reddit.com/r/MachineLearning/comments/13scb1b/d_llms_in_robotics/"/><updated> 2023-05-26T12:47:28+00:00</updated><published> 2023-05-26T12:47:28+00:00</published><title> [D] 机器人学法学硕士</title></entry><entry><author><name>/u/Mr_Whispers</name><uri> https://www.reddit.com/user/Mr_Whispers </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;航海者：一个Minecraft 中由 LLM 驱动的学习代理” title=&quot;航海者：一个由 LLM 驱动的学习代理在我的世界中&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Mr_Whispers&quot;>; /u/Mr_Whispers &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.16291&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13sc0pp </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/"/><updated> 2023-05-26T12:34:50+00:00</updated><published> 2023-05-26T12:34:50+00:00</published><title> Voyager：Minecraft 中由 LLM 驱动的学习代理</title></entry><entry><author><name>/u/天眼2006</name><uri> https://www.reddit.com/user/dayeye2006 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我以前是一名机器学习工程师。但我已经多年没有接触 Pytorch（我在自己的初创公司工作，作为一名全栈工程师）。&lt;/p>; &lt;p>;有哪些好的资源可以刷新我的 PyTorch 技能？&lt;/p>; &lt;p>;我喜欢以“愚蠢的方式”学习东西。我计划从头开始实现一些最经典的模型（ResNet、TextCNN、transformers，...）。&lt;/p>; &lt;p>;当我学习编程语言时，我最喜欢参考的资源是 &lt;a href=&quot;https://github.com/topics/koans&quot;>;公案&lt;/a>;。这有助于我快速熟悉新语言。深度学习界有对应的吗？&lt;/p>; &lt;p>;谢谢&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/dayeye2006&quot;>; /u/dayeye2006 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13royi6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/"/><updated> 2023-05-25T18:13:41+00:00</updated><published> 2023-05-25T18:13:41+00:00</published><title> [D] 有哪些资源可以提高我的 PyTorch 技能？</title></entry><entry><author><name> /u/奇异语2501</name><uri> https://www.reddit.com/user/Singularian2501 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>; &lt;img src=&quot;https://a.thumbs.redditmedia .com/pwokgMFTSRP9bRbBlTbOA1gPSTlPoECDQdwoNNPMuG0.jpg&quot; alt=&quot;[R] Gorilla：连接大量 API 的大型语言模型 - Microsoft Research 2023 - 在编写 API 调用方面超越 GPT-4 的性能。 title=&quot;[R] Gorilla：连接大量 API 的大型语言模型 - Microsoft Research 2023 - 在编写 API 调用方面超越 GPT-4 的性能。&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.15334 &quot;>;https://arxiv.org/abs/2305.15334&lt;/a>; &lt;/p>; &lt;p>;Github：&lt;a href=&quot;https://github.com/ShishirPatil/gorilla&quot;>;https://github。 com/ShishirPatil/gorilla&lt;/a>; &lt;/p>; &lt;p>;博客：&lt;a href=&quot;https://gorilla.cs.berkeley.edu/&quot;>;https://gorilla.cs.berkeley.edu/&lt; /a>; &lt;/p>; &lt;p>;摘要：&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 最近出现了令人印象深刻的进步浪潮，模型现在在各种任务中表现出色，例如数学推理和程序综合。然而，它们通过 API 调用有效使用工具的潜力仍未实现。即使对于当今最先进的 LLM（例如 GPT-4）而言，这也是一项具有挑战性的任务，这主要是因为它们无法生成准确的输入参数，并且它们倾向于产生错误的 API 调用用法。我们发布了 Gorilla，这是一种经过微调的基于 LLaMA 的模型，在编写 API 调用方面超越了 GPT-4 的性能。当与文档检索器结合使用时，Gorilla 展示了适应测试时文档更改的强大能力，支持灵活的用户更新或版本更改。 &lt;strong>;它还大大减轻了直接提示 LLM 时经常遇到的幻觉问题。&lt;/strong>;为了评估模型的能力，我们引入了 APIBench，这是一个由 HuggingFace、TorchHub 和 TensorHub API 组成的综合数据集。 &lt;strong>;检索系统与 Gorilla 的成功集成表明 LLM 有潜力更准确地使用工具，跟上经常更新的文档，从而提高其输出的可靠性和适用性。&lt;/strong>;&lt;/p>; &lt; /blockquote>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/n5ezjchbg12b1.jpg?width=872&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eb5b7e11a22abe59d49504fad7278006a2b878a6&quot;>;https://preview.redd。它/n5ezjchbg12b1.jpg?width=872&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eb5b7e11a22abe59d49504fad7278006a2b878a6&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/e2xhpfhbg12b1.jpg ?width=1075&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b3c0f6ed7a6d72c93e681266977a0ec0f129ba6d&quot;>;https://preview.redd.it/e2xhpfhbg12b1.jpg?width=1075&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b 3c0f6ed7a6d72c93e681266977a0ec0f129ba6d&lt;/a >;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/i7i7bfhbg12b1.jpg?width=1213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5a287aba81199b66d1334457c6e8a12b3b5881c0&quot;>;https://预览。 redd.it/i7i7bfhbg12b1.jpg?width=1213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5a287aba81199b66d1334457c6e8a12b3b5881c0&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Singularian2501&quot;>; /u/Singularian2501 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13rl3v9 </id><media:thumbnail url="https://a.thumbs.redditmedia.com/pwokgMFTSRP9bRbBlTbOA1gPSTlPoECDQdwoNNPMuG0.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/"/><updated> 2023-05-25T15:42:26+00:00</updated><published> 2023-05-25T15:42:26+00:00</published><title> [R] Gorilla: Large Language Model Connected with Massive APIs - Microsoft Research 2023 - 在编写 API 调用方面超越了 GPT-4 的性能。</title></entry><entry><author><name> /u/Ok_Bank_2217</name><uri> https://www.reddit.com/user/Ok_Bank_2217 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们需要为我们的平台获取大量 YouTube 数据并训练自定义 ML 模型，但除了 YouTube 之外找不到任何有用的东西8M Dataset，相当过时，信息非常有限。官方的 YouTube 数据 API 也被限制在大约 10.000 个积分，这远远不能满足我们需要的数量。&lt;/p>; &lt;p>;这就是为什么我们说去他妈的，并决定自己构建一个巨大的 YouTube 数据集。在为超过 1 亿个视频编制索引并构建自定义 API 来访问它之后，我们决定公开 API 并允许人们购买访问权限！&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.blizzy -data.com/&quot;>;链接到网站&lt;/a>;&lt;/p>; &lt;p>;我们很乐意听到我们的 ML 工程师和数据科学家的反馈，并希望解决您和我们遇到的问题!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Ok_Bank_2217&quot;>; /u/Ok_Bank_2217 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rh9yj </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/"/><updated> 2023-05-25T13:04:43+00:00</updated><published> 2023-05-25T13:04:43+00:00</published><title> [P] 我们创建了一个大型 YouTube 视频数据集来替换 YouTube 数据 API</title></entry><entry><author><name> /u/我是布兰妮</name><uri>https://www.reddit.com/user/ISpearedBritney </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你的很多工作都涉及 AI 或 ML（无论标题如何），你能分享一下你的典型工作日是怎样的吗？您将时间花在什么上，最终经常使用哪些工具或资源？其中有多少是数据争论，你使用了多少数学？谢谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ISpearedBritney&quot;>; /u/ISpearedBritney &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rct07 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/"/><updated> 2023-05-25T09:21:06+00:00</updated><published> 2023-05-25T09:21:06+00:00</published><title> [D] 对于那些从事 ML/AI 工作的人，您的工作和工作日是什么样的？</title></entry><entry><author><name> /u/arg_max</name><uri> https://www.reddit.com/user/arg_max </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨，&lt;/p>; &lt;p>;我为我的 neurips 提交深入研究了扩散器，发现了一些我认为有点奇怪但不#39;真的没有人可以与之讨论，所以我想我只是把它贴在这里，看看是否有人知道发生了什么，这是否是一个众所周知的现象。&lt;/p >; &lt;p>;所以调节稳定扩散。你有一个提示，类似于“狗的图像”。该提示通过 Clip 模型编码为条件矩阵，该矩阵通过交叉注意力输入 U-Net。此剪辑编码包括一个标记器，它将提示拆分为标记及其连续表示。这个分词器还包括一个“句子开头”放在每个标记化序列开头的标记（以及重复的“句子结尾”标记，直到达到最大标记数，对于稳定扩散为 77）。在交叉注意层中，然后将作为当前潜在 z_t 的 U-Net 编码版本的视觉特征投影到查询矩阵 Q 中。条件（即剪辑编码提示）被转换为键和值矩阵K 和 V。然后乘以 Q * K^T 并在行上取 softmax 以获得注意力概率矩阵。该矩阵中的每一行对应一个视觉特征，每一列对应文本条件中的一个标记。由于 softmax，行总和为 1，对于每个空间位置，您可以在标记上进行分布。基本上，它告诉提示中的一个标记/单词对某个空间位置的影响有多大。现在，我希望所有权重都集中在提示中的重要标记上（例如“狗”），但我发现平均而言，90-99% 的概率质量被放入“句子开头” ;令牌。然后这也意味着对应于“句子开头”的值矩阵中的条目是“句子的开始”。 token会支配交叉注意力层的输出，不管你写什么提示。对我来说，这很奇怪，显然，这不是手工编码的，而是学习的，因此优化发现 XA 层的输出具有较小的可变性，而是始终接近对应于“&amp;quot;”的值矩阵条目。句首”令牌在某种程度上是最好的。此外，这种行为在不同的时间步都是相同的，所以它发生在扩散过程的开始和结束时。&lt;/p>; &lt;p>;也许其他人经历过类似的事情或者知道这里发生了什么？&lt; /p>; &lt;p>;TLDR：稳定扩散中的注意力概率集中在 90-99% 的句子标记的一般开头，而不是来自实际提示的标记，与扩散时间步长、交叉注意力头或 U 无关-网络层。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/arg_max&quot;>; /u/arg_max &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_thinks_this_behavior/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_that_thinks_this_behavior/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rwh1c </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_that_thinks_this_behavior/"/><updated> 2023-05-25T23:10:42+00:00</updated><published> 2023-05-25T23:10:42+00:00</published><title> [D] 只有我认为这种行为（交叉注意力层）很奇怪吗？</title></entry><entry><author><name> /u/奇异语2501</name><uri> https://www.reddit.com/user/Singularian2501 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/huTwcaqVaNrBZzikekNSL9XJi3tfEGZsggLCg50LMYM.jpg&quot; alt=&quot;[R] 用语言模型推理就是用世界模型进行规划 - Shibo Hao 等加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，计划相对改进了 33%世代设定！” title=&quot;[R] 用语言模型推理就是用世界模型进行规划 - Shibo Hao 等加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，在计划生成设置中相对改进了 33%！” />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.14992 &quot;>;https://arxiv.org/abs/2305.14992&lt;/a>; &lt;/p>; &lt;p>;摘要：&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 已显示出非凡的推理能力，尤其是当提示生成中间推理步骤（例如，Chain-of-Thought，CoT）。然而，LLM 仍然难以解决对人类来说很容易的问题，例如为在给定环境中执行任务生成行动计划，或者执行复杂的数学、逻辑和常识推理。缺陷源于一个关键事实，即 LLM 缺乏内部&lt;em>;世界模型&lt;/em>;来预测世界&lt;em>;状态&lt;/em>;（例如，环境状态、中间变量值）并模拟长期结果动作。这可以防止 LLM 执行类似于人脑的深思熟虑的计划，这涉及探索替代推理路径、预测未来状态和奖励，以及迭代改进现有推理步骤。为了克服这些限制，我们提出了一个新的 LLM 推理框架，&lt;strong>;&lt;em>;R&lt;/em>;&lt;/strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;easoning via&lt;/em>; /strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;P&lt;/em>;&lt;/strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;规划&lt;/em>;&lt;/strong >; &lt;strong>;(RAP).&lt;/strong>; RAP 将 &lt;strong>;LLM 重新定位为世界模型和推理代理，并结合原则性规划算法（基于蒙托卡罗树搜索）在广阔的推理中进行战略探索&lt;/strong>; 在推理过程中，LLM（作为代理）在LLM（作为世界模型）和任务特定奖励的指导下，增量构建推理树，并在适当的平衡下高效地获得高奖励推理路径在探索 &lt;em>; 与 &lt;/em>; 开发之间。我们将 RAP 应用于各种具有挑战性的推理问题，包括计划生成、数学推理和逻辑推理。这些任务的实证结果证明了 RAP 优于各种强大的基线，包括 CoT 和自洽性从最少到最多的提示。 &lt;strong>;LLAMA-33B 上的 RAP 超过了 GPT-4 上的 CoT，计划生成设置相对改进了 33%。&lt;/strong>;&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;a href=&quot;https://preview .redd.it/jaoiil2mc12b1.jpg?width=747&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53&quot;>;https://preview.redd.it/jaoiil2mc12b1.jpg?width=747&amp;amp;format= pjpg&amp;auto= webp&amp;amp;s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7389d75d0ff7 d1d8787c7c5f9add4787b02b47be &quot;>;https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7389d75d0ff7d1d8787c7c5f9add4787b02b47be&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https:/ /preview.redd.it/ykpqvp2mc12b1.jpg?width=980&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=834c39fb3e549418b8396725e86ddff3c6584077&quot;>;https://preview.redd.it/ykpqvp2mc12b1.jpg?width=9 80&amp;amp;格式=pjpg&amp;amp; auto=webp&amp;amp;s=834c39fb3e549418b8396725e86ddff3c6584077&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s =9c5192d6c012bfe4390fa67b010580b8e4508daa&quot;>;https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9c5192d6c012bfe4390fa67b010580 b8e4508daa&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https //preview.redd.it/qd8pjo2mc12b1.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea&quot;>;https://preview.redd.it/qd8pjo2mc12b1.jpg?width=140 0&amp;格式= pjpg&amp;amp;auto=webp&amp;amp;s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Singularian2501&quot;>; /u/Singularian2501 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13rkhzx </id><media:thumbnail url="https://b.thumbs.redditmedia.com/huTwcaqVaNrBZzikekNSL9XJi3tfEGZsggLCg50LMYM.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/"/><updated> 2023-05-25T15:17:59+00:00</updated><published> 2023-05-25T15:17:59+00:00</published><title> [R] 使用语言模型进行推理就是使用世界模型进行规划 - Shibo Hao 等人，加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，计划生成设置相对改进了 33%！</title></entry><entry><author><name> /u/钢铁侠马克20</name><uri> https://www.reddit.com/user/IronManMark20 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/IronManMark20&quot;>; /u/IronManMark20 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://gorilla.cs.berkeley. edu/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rpvgn/gorilla_large_language_model_connected_with/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rpvgn </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rpvgn/gorilla_large_language_model_connected_with/"/><updated> 2023-05-25T18:49:35+00:00</updated><published> 2023-05-25T18:49:35+00:00</published><title> Gorilla：连接大量 API 的大型语言模型</title></entry><entry><author><name>/u/南巴达尔</name><uri>https://www.reddit.com/user/Nambardaar </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨&lt;/p>; &lt;p>;我正在使用 libtorch 将一些 python tensorflow 代码翻译成 cpp。我想为文本数据生成嵌入。在 python 中，我们使用 tensorflow hub 来预处理数据，然后生成像这样的嵌入&lt;/p>; &lt;p>;preprocessor = hub.load(preprocessor) Bert = hub.load(BertModel)&lt;/p>; &lt;p>;Tokens = preprocessor (text) Embeddings = Bert (Tokens)&lt;/p>; &lt;p>;现在我想把它转换成c++。我可以通过火炬脚本进行跟踪来转换 Bert 模型。但是如何转换预处理器/标记化部分？ &lt;/p>; &lt;p>;欢迎提供任何指导或线索&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Nambardaar&quot;>; /u/Nambardaar &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s7df6/bert_embeddings_in_c_discussion/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s7df6/bert_embeddings_in_c_discussion/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s7df6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s7df6/bert_embeddings_in_c_discussion/"/><updated> 2023-05-26T08:31:11+00:00</updated><published> 2023-05-26T08:31:11+00:00</published><title> C++ 中的 Bert 嵌入 [讨论]</title></entry><entry><author><name> /u/内部-Industry758</name><uri> https://www.reddit.com/user/Internal-Industry758 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你在没有在顶级会议上发表任何论文的情况下完成了博士学位，你现在在做什么？你仍然觉得博士学位值得吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Internal-Industry758&quot;>; /u/Internal-Industry758 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rm0uf </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/"/><updated> 2023-05-25T16:18:09+00:00</updated><published> 2023-05-25T16:18:09+00:00</published><title> [D] 没有顶级出版物的博士：你现在在做什么？</title></entry><entry><author><name> /u/Artistic_Artist_9891</name><uri> https://www.reddit.com/user/Artistic_Artist_9891 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;你好&lt;/p>; &lt;p>;我想训练一个 LSTM 模型来预测 True 或 False 的输出。但是，当我部署模型时，我将不会获得之前时间步长的实际输出。我不确定在训练过程中，模型是否将先前实际输出的信息存储在隐藏状态或内存中。如果是这样，我想知道是否有任何方法可以考虑到这一点来训练模型。&lt;/p>; &lt;p>;在此先感谢您的帮助。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Artistic_Artist_9891&quot;>; /u/Artistic_Artist_9891 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s9v7x/d_lstm/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s9v7x/d_lstm/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s9v7x</id><link href="https://www.reddit.com/r/MachineLearning/comments/13s9v7x/d_lstm/"/><updated> 2023-05-26T10:54:01+00:00</updated><published> 2023-05-26T10:54:01+00:00</published><title> [D] 长短期记忆网络</title></entry><entry><author><name>/u/恩里科希波尔</name><uri>https://www.reddit.com/user/EnricoShippole </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;很高兴发布 FLAN V2 数据集的开源复制品。&lt;/p>; &lt;p>;可以在此处找到完整的数据集：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/FLAN_2022&quot;>;https://huggingface.co/datasets/conceptofmind/FLAN_2022&lt;/a>;&lt;/p>; &lt;p>;我和主要作者 Shayne Longpre 一起工作FLAN 合集重现他的伟大作品并公开发布高质量的指令调优数据。我们修复了编码问题并将序列长度增加到 4096：&lt;a href=&quot;https://twitter.com/EnricoShippole/status/1661756166248996867?s=20&quot;>;https://twitter.com/EnricoShippole/status/1661756166248996867 ?s=20&lt;/a>;&lt;/p>; &lt;p>;每个单独的子混音也可以在 huggingface 上下载。子混音是 T0、FLAN2021、CoT、NIv2 和 Dialog。每个都包含相关的元数据，例如输入、目标、任务源、任务名称和模板类型。&lt;/p>; &lt;p>;T0 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/t0_submix_original&quot;>; https://huggingface.co/datasets/conceptofmind/t0_submix_original&lt;/a>;&lt;/p>; &lt;p>;Flan2021 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/flan2021_submix_original&quot;>;https:/ /huggingface.co/datasets/conceptofmind/flan2021_submix_original&lt;/a>;&lt;/p>; &lt;p>;CoT 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/cot_submix_original&quot;>;https://huggingface. co/datasets/conceptofmind/cot_submix_original&lt;/a>;&lt;/p>; &lt;p>;NIv2 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/niv2_submix_original&quot;>;https://huggingface.co/datasets /conceptofmind/niv2_submix_original&lt;/a>;&lt;/p>; &lt;p>;对话子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/dialog_submix_original&quot;>;https://huggingface.co/datasets/conceptofmind/ dialog_submix_original&lt;/a>;&lt;/p>; &lt;p>;您可以在这里找到原始的 FLAN 存储库和 Shayne Longpre 的所有令人难以置信的工作：&lt;a href=&quot;https://github.com/google-research/FLAN /tree/main/flan/v2#download&quot;>;https://github.com/google-research/FLAN/tree/main/flan/v2#download&lt;/a>;&lt;/p>; &lt;p>;一定要也通读 Shayne 关于 FLAN 集合的论文，以更好地了解数据是如何创建的：&lt;a href=&quot;https://arxiv.org/abs/2301.13688&quot;>;https://arxiv.org/abs /2301.13688&lt;/a>;&lt;/p>; &lt;p>;我们将很快发布一个包含数百 GB 高质量指令数据的大规模因果语言建模数据集。在不久的将来留意该版本。&lt;/p>; &lt;p>;我们在 FLAN V2 和相关项目的开放复制方面的工作都归功于 CarperAI 和 StabilityAI 的慷慨赞助。&lt;/p>; &lt;p>;你可以在此处了解有关 CarperAI 的更多信息：&lt;a href=&quot;https://carper.ai/&quot;>;https://carper.ai/&lt;/a>;&lt;/p>; &lt;p>;在此处了解 StabilityAI：&lt;a href=&quot; https://stability.ai/&quot;>;https://stability.ai/&lt;/a>;&lt;/p>; &lt;p>;非常感谢 Jason Phang 和 Fabrizio Milo 帮助构建数据集。&lt;/p >; &lt;p>;你可以在这里找到 Jason Phang 的推特：&lt;a href=&quot;https://twitter.com/zhansheng&quot;>;https://twitter.com/zhansheng&lt;/​​a>;&lt;/p>; &lt; p>;Fabrizio Milo 在这里：&lt;a href=&quot;https://twitter.com/fabmilo&quot;>;https://twitter.com/fabmilo&lt;/a>;&lt;/p>; &lt;p>;您可以查看在此处查看 Shayne 关于构建预训练数据集的新论文：&lt;a href=&quot;https://github.com/shayne-longpre/a-pretrainers-guide/blob/main/A%20Pretrainer&amp;#x27;s %20Guide%20To%20Training%20Data.pdf&quot;>;https://github.com/shayne-longpre/a-pretrainers-guide/blob/main/A%20Pretrainer&amp;#39;s%20Guide%20To%20Training%20Data。 pdf&lt;/a>;&lt;/p>; &lt;p>;这不是 Google 或 StabilityAI 的官方产品。&lt;/p>; &lt;p>;如果您对数据有任何疑问，请务必联系我们并询问！我会尽量及时回复：&lt;a href=&quot;https://twitter.com/EnricoShippole&quot;>;https://twitter.com/EnricoShippole&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON - ->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/EnricoShippole&quot;>; /u/EnricoShippole &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rmqx5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/"/><updated> 2023-05-25T16:46:42+00:00</updated><published> 2023-05-25T16:46:42+00:00</published><title> [P] FLAN V2 数据集的开源再现</title></entry><entry><author><name>/u/Shot-Button-9010</name><uri> https://www.reddit.com/user/Shot-Button-9010 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我以为 NeurIPS 有，但我在网站上看到的只有提交截止日期和通知日期。 NeurIPS 通常会跳过反驳吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Shot-Button-9010&quot;>; /u/Shot-Button-9010 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13rry8o/d_does_neurips_2023_have_rebuttal_phase/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rry8o/d_does_neurips_2023_have_rebuttal_phase/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rry8o </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rry8o/d_does_neurips_2023_have_rebuttal_phase/"/><updated> 2023-05-25T20:11:05+00:00</updated><published> 2023-05-25T20:11:05+00:00</published><title> [D] NeurIPS 2023 是否有反驳阶段？</title></entry><entry><author><name> /u/lanc9r</name><uri> https://www.reddit.com/user/lanc9r </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;你会相信 AI 数据驱动的美元涨跌来做出买入或卖出货币的决定吗？&lt;/p>; &lt;p >;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.reddit.com/poll/13sbim2&quot;>;查看投票&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/lanc9r&quot;>; /u/lanc9r &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sbim2/d_will_you_trust_ai_to_analyze_the_rise_or_fall/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sbim2/d_will_you_trust_ai_to_analyze_the_rise_or_fall/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sbim2 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sbim2/d_will_you_trust_ai_to_analyze_the_rise_or_fall/"/><updated> 2023-05-26T12:12:34+00:00</updated><published> 2023-05-26T12:12:34+00:00</published><title> [D] 你会信任人工智能来分析资产的涨跌吗？</title></entry><entry><author><name> /u/铁叶神经元</name><uri>https://www.reddit.com/user/tiedyeneuron </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;诚然，我在标题问题的措辞上略显幼稚和片面，以引发讨论。我认为从事深度学习研究的学术实验室有一定的优势。然而，许多重大突破现在似乎确实发生在工业实验室，而不是小型大学实验室。这可能是由于 DL 从新兴研究领域成熟为工业技术。&lt;/p>; &lt;p>;鉴于 DL 的最新发展，人们对在工业中进行深度学习研究的相对优点有何看法与学术界？例如，如果有人可以选择在顶级学术实验室（如麻省理工学院、斯坦福大学、加州大学伯克利分校等）担任研究员或加入 OpenAI/Anthropic/DeepMind 等，他们为什么要选择学术路径？&lt;/p >; &lt;p>;我理解有些人可能出于成为教授的愿望而选择学术界，但似乎越来越多的顶尖大学乐于让行业研究人员担任客座教授或担任兼职教授。许多行业科学家也接受实习生，因此他们仍然可以充当导师，就像他们是学术实验室的 PI 一样。仍然，显然留在 AI 学术界仍然有一些独特的价值，因为我能想到许多选择这样做的顶级研究人员。我很想知道人们认为与行业实验室相比有什么好处。&lt;/p>; &lt;p>;（我知道这是一个与职业相关的帖子，但它看起来不像 &lt;a href=&quot;https:// www.reddit.com/r/cscareerquestions/&quot;>;r/cscareerquestions&lt;/a>; 拥有合适的受众或专业知识来推动这一讨论。此外，我认为目前这一讨论非常针对跨行业/学术界的 ML 社区及时。）&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/tiedyeneuron&quot;>; /u/tiedyeneuron &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rl1q6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/"/><updated> 2023-05-25T15:40:05+00:00</updated><published> 2023-05-25T15:40:05+00:00</published><title> [D] 鉴于深度学习方法的扩展，作为 AI 研究人员留在学术界的剩余优点是什么？</title></entry></feed>