<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-26T20:15:10+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /u/余额-</name><uri> https://www.reddit.com/user/Balance- </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;阿布扎比技术创新研究所 (TII) 刚刚发布了新的 7B 和 40B LLM。&lt;/p>; &lt;p>;Falcon-40B模型现在位于 &lt;a href=&quot;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&quot;>;Open LLM Leaderboard&lt;/a>; 的顶部，击败 &lt;em>;llama-30b-supercot&lt;/em>; 和&lt;em>;llama-65b&lt;/em>; 等等。&lt;/p>; &lt;table>;&lt;thead>; &lt;tr>; &lt;th>;Model&lt;/th>; &lt;th>;Revision&lt;/th>; &lt;th>;Average&lt;/th>; &lt;th>;ARC（25 次）&lt;/th>; &lt;th>;HellaSwag（10 次）&lt;/th>; &lt;th>;MMLU（5 次）&lt;/th>; &lt;th>;TruthfulQA（0 次）&lt;/ th>; &lt;/tr>; &lt;/thead>;&lt;tbody>; &lt;tr>; &lt;td>;tiiuae/falcon-40b&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;60.4&lt;/td>; &lt;td>;61.9&lt;/ td>; &lt;td>;85.3&lt;/td>; &lt;td>;52.7&lt;/td>; &lt;td>;41.7&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ausboss/llama-30b-supercot&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;59.8&lt;/td>; &lt;td>;58.5&lt;/td>; &lt;td>;82.9&lt;/td>; &lt;td>;44.3&lt;/td>; &lt;td>;53.6&lt;/td>; &lt;/tr>; &lt; tr>; &lt;td>;llama-65b&lt;/td>; &lt;td>;main&lt;/td>; &lt;td>;58.3&lt;/td>; &lt;td>;57.8&lt;/td>; &lt;td>;84.2&lt;/td>; &lt;td>;48.8&lt;/ td>; &lt;td>;42.3&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MetaIX/GPT4-X-Alpasta-30b&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;57.9&lt;/td>; &lt; td>;56.7&lt;/td>; &lt;td>;81.4&lt;/td>; &lt;td>;43.6&lt;/td>; &lt;td>;49.7&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;&lt;strong>;按发布：&lt;/strong>; &lt;a href=&quot;https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model&quot;>;阿联酋&amp;# 39科技创新院发布开源“猎鹰40B”无人机用于研究与开发的大型语言模型商业应用&lt;/a>;&lt;/p>; &lt;blockquote>; &lt;p>;位于阿布扎比的技术创新研究所 (TII) 宣布了其开源大型语言模型 (LLM)，即 Falcon 40B。 Falcon 40B 拥有 400 亿个参数，是阿联酋首个大型 AI 模型，表明该国在 AI 领域的雄心以及促进创新和研究的承诺。 &lt;/p>; &lt;p>;与通常只向非商业用户提供访问权限的大多数 LLM 不同，Falcon 40B 对研究和商业用途均开放。 TII 还将模型的权重包含在开源包中，这将增强模型的能力并允许更有效的微调。 &lt;/p>; &lt;p>;除了猎鹰 40B 的发射外，TII 还发起了一项征集，征求有兴趣利用该模型创建创新用例或探索进一步应用的研究人员和有远见者的提案。作为对优秀研究提案的奖励，入选项目将获得“训练计算能力”奖励。作为一项投资，允许更强大的数据分析和复杂的建模。 VentureOne 是 ATRC 的商业化部门，将为最有前途的项目提供计算资源。 &lt;/p>; &lt;p>;自 2023 年 3 月揭幕以来，TII 的 Falcon 40B 表现出了令人印象深刻的性能。当使用斯坦福大学的 HELM LLM 工具进行基准测试时，与 OpenAI 等其他著名的 LLM 相比，它使用的训练计算能力更少;的 GPT-3、DeepMind 的 Chinchilla AI 和谷歌的 PaLM-62B。 &lt;/p>; &lt;p>;那些有兴趣访问 Falcon 40B 或提出用例的人可以通过 &lt;a href=&quot;https://FalconLLM.TII.ae&quot;>;FalconLLM.TII.ae&lt;/a>; 网站进行。迄今为止开源的 Falcon LLM 可根据基于开源 Apache 2.0 软件原则构建的许可获得，允许广泛的免费使用。&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;strong>;Hugging Face 链接&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://huggingface.co/tiiuae/falcon-7b&quot;>;Falcon-7B&lt;/a>; / &lt;a href=&quot;https:/ /huggingface.co/tiiuae/falcon-7b-instruct&quot;>;Falcon-7B-Instruct&lt;/a>;&lt;/li>; &lt;li>;&lt;a href=&quot;https://huggingface.co/tiiuae/falcon-40b&quot;>; Falcon-40B&lt;/a>; / &lt;a href=&quot;https://huggingface.co/tiiuae/falcon-40b-instruct&quot;>;Falcon-40B-指令&lt;/a>;&lt;/li>; &lt;/ul>; &lt;/div >;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Balance-&quot;>; /u/Balance- &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit. com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sdz8p </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/"/><updated> 2023-05-26T13:57:42+00:00</updated><published> 2023-05-26T13:57:42+00:00</published><title> [N] 阿布扎比的 TTI 发布开源 Falcon-7B 和 -40B LLM</title></entry><entry><author><name> /u/让-波特</name><uri>https://www.reddit.com/user/Jean-Porte </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 法尔se 模仿专有 LLM 的承诺&quot; title=&quot;[R] 模仿的虚假承诺专有法学硕士&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Jean-Porte&quot;>; /u/Jean-Porte &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv. org/abs/2305.15717&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13s6pb7 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/"/><updated> 2023-05-26T07:49:29+00:00</updated><published> 2023-05-26T07:49:29+00:00</published><title> [R] 模仿专有 LLM 的虚假承诺</title></entry><entry><author><name>/u/flyforlight</name><uri> https://www.reddit.com/user/flyforlight </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/OWNmCZQOMv7_CrK2_wjK8IwjFLcefzaJyMxAvR2kEWY.jpg&quot; alt=&quot;[R] Minecraft 中的幽灵：通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供一般能力的代理&quot; title=&quot;[R] Minecraft 中的幽灵：一般通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供有能力的代理&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/flyforlight&quot;>; /u/flyforlight &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ gallery/13shsz4&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13shsz4 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/OWNmCZQOMv7_CrK2_wjK8IwjFLcefzaJyMxAvR2kEWY.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/"/><updated> 2023-05-26T16:28:34+00:00</updated><published> 2023-05-26T16:28:34+00:00</published><title> [R] Minecraft 中的幽灵：通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供一般能力的代理</title></entry><entry><author><name>/u/Mr_Whispers</name><uri> https://www.reddit.com/user/Mr_Whispers </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;航海者：一个Minecraft 中由 LLM 驱动的学习代理” title=&quot;航海者：一个由 LLM 驱动的学习代理在我的世界中&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Mr_Whispers&quot;>; /u/Mr_Whispers &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.16291&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13sc0pp </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/"/><updated> 2023-05-26T12:34:50+00:00</updated><published> 2023-05-26T12:34:50+00:00</published><title> Voyager：Minecraft 中由 LLM 驱动的学习代理</title></entry><entry><author><name>/你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/neuralink-receives-fda-approval-to-launch-first -in-human-clinical-trials-e373e7b5fcf1&quot;>;https://medium.com/@tiago-mesquita/neuralink-receives-fda-approval-to-launch-first-in-human-clinical-trials-e373e7b5fcf1&lt;/ a>;&lt;/p>; &lt;p>;Neuralink 表示尚未招募参与者，更多信息将很快公布。&lt;/p>; &lt;p>;想法？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s85rb </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/"/><updated> 2023-05-26T09:20:57+00:00</updated><published> 2023-05-26T09:20:57+00:00</published><title> [N] Neuralink 刚刚获得 FDA 的绿灯，可以继续其首次人体临床试验</title></entry><entry><author><name>/u/theoneandonlypatriot</name><uri> https://www.reddit.com/user/theoneandonlypatriot </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我正在参加 SWE 工作的面试过程，有几个人直接评判我，甚至公然说他们不是 AI 的粉丝因为我在 AI / ML 工作方面的背景。&lt;/p>; &lt;p>;发这篇文章是为了让人们知道工程社区中存在这种观点和负面看法。&lt;/p>; &lt;p>;考虑到我也分享了很多东西，感觉很糟糕人工智能的伦理问题。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/theoneandonlypatriot&quot;>; /u/theoneandonlypatriot&lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s32d4 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/"/><updated> 2023-05-26T04:25:57+00:00</updated><published> 2023-05-26T04:25:57+00:00</published><title> [D] 对 AI 的负面评价</title></entry><entry><author><name>/u/Mr_Whispers</name><uri> https://www.reddit.com/user/Mr_Whispers </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s8z41/deepmind_model_evaluation_for_extreme_risks/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;DeepMind: 模型极端风险评估&quot; title=&quot;DeepMind：极端风险模型评估&quot; />; &lt; /a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Mr_Whispers&quot;>; /u/Mr_Whispers &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.15324&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s8z41/deepmind_model_evaluation_for_extreme_risks/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13s8z41 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13s8z41/deepmind_model_evaluation_for_extreme_risks/"/><updated> 2023-05-26T10:08:23+00:00</updated><published> 2023-05-26T10:08:23+00:00</published><title> DeepMind：极端风险的模型评估</title></entry><entry><author><name>/u/I_will_delete_myself</name><uri> https://www.reddit.com/user/I_will_delete_myself </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我犹豫了一会儿，但听到这个消息后虚伪让我发疯。&lt;/p>; &lt;p>;SMH 这家公司就像白衣骑士一样，他们认为他们凌驾于所有人之上。他们想要监管，但他们希望不受该监管的影响。只想伤害其他人，但不想伤害“全能的”Sam 和朋友。&lt;/p>; &lt;p>;向国会撒谎说建议在欧盟采取类似的做法，但现在开始抱怨他们。在任何政治领域都不应该认真对待这个家伙。&lt;/p>; &lt;p>;我的观点是，这家公司通过锁定与其品牌名称相悖的东西来反对 AI 进步。如果他们甚至不能忠于这样简单的事情，我们怎么能指望他们忠于更难的 AI 安全？&lt;/p>; &lt;p>;我很高兴他们现在改变了立场，但我很高兴他们如何他们认为他们有权为了自己的利益而腐败。 SMH!!!!!!!!&lt;/p>; &lt;p>;你有什么想法？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/I_will_delete_myself&quot;>; /u/I_will_delete_myself &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rie0e </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/"/><updated> 2023-05-25T13:51:58+00:00</updated><published> 2023-05-25T13:51:58+00:00</published><title> OpenAI 现在抱怨人工智能的监管 [D]</title></entry><entry><author><name> /u/ThePanArchitect</name><uri> https://www.reddit.com/user/ThePanArchitect </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好&lt;/p>; &lt;p>;我想知道是否有人有兴趣讨论一些关于进一步为建筑师开发人工智能工具的话题。在您阅读之前，我必须声明，我对 AI 和 Transformer 模型的了解非常浅薄。请原谅我的无知，尽管如此，我还是非常感兴趣。&lt;/p>; &lt;p>;所以...如果尚未发生，人工智能在建筑中的集成已经得到了广泛的讨论。然而，从我的角度来看，它似乎是在一个相对表面的层面上实现的。即通过使用 Midjourney 或 ControlNET 等文本提示生成图像。但是，我还没有看到真正可以理解几何或 3D 形状的工具或模型。尽管从技术上讲，几何可以通过文本或数学公式来表示更复杂的表面和形状。如果几何可以转换成文本，它就可以被理解和预训练，&lt;em>;对吗？&lt;/em>;&lt;/p>; &lt;p>;已经有一篇优秀的研究论文对这种想法进行了概念验证，论文被称为“Architext”而且我认为，深入研究将几何图形表示为文本，将墙壁、窗户、门等表示为文本或任何其他可以预训练的格式的想法肯定会有所收获。&lt;/p>; &lt;p>;也许墙可以用元组表示，例如：&lt;br/>; (&lt;em>;baselineL1[Startpoint(x1,y1),Endpoint(x2,y2)], thickness=250 mm, height=2800)&lt;/em>;&lt;/ p>; &lt;p>;事实上，实际上有一种名为 IFC 的文件格式，它基本上是将整个 BIM 转换为文本。也许 IFC 可以用作“训练集”？&lt;/p>; &lt;p>;我可能有点超前了，但前景真的很诱人，如果我的热情似乎被误导了，请原谅我的热情，尤其是我的无知。我对这个话题的理解很肤浅。&lt;/p>; &lt;p>;我真的很期待听到你们的声音&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ThePanArchitect&quot;>; /u/ThePanArchitect &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sloh2/first_post_the_exciting_prospect_of_ai_in/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sloh2/first_post_the_exciting_prospect_of_ai_in/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sloh2 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sloh2/first_post_the_exciting_prospect_of_ai_in/"/><updated> 2023-05-26T19:07:02+00:00</updated><published> 2023-05-26T19:07:02+00:00</published><title>第一次发帖！人工智能在建筑和施工中令人兴奋的前景[讨论]</title></entry><entry><author><name> /u/知道杰罗姆</name><uri>https://www.reddit.com/user/iknowjerome </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;表>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sd4ku/r_samadrivescalifornia_automotive_semantic/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/6mKRKGxChyyetC3FAKtYmWYw3zxEdQM6gd3Tjw1DuwI.jpg&quot; alt=&quot;[R] sama-drives-california：汽车语义分割数据集（25k 帧）现已可用&quot; title=&quot;[R] sama-drives-california：汽车语义分割数据集（25k 帧）现在可用&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;Sama 刚刚发布了另一个数据集根据 Creative Commons 4.0 许可。它可以在 Hugging Face 上找到。您可以查看 Hugging Face &lt;a href=&quot;https://huggingface.co/datasets/SamaAI/sama-drives-california&quot;>;数据集卡&lt;/a>;了解更多详细信息。如果您想直接下载 BDD100K 格式而不通过 Hugging Face，这里是 &lt;a href=&quot;https://sama-documentation-assets.s3.amazonaws.com/sama-drives -california/zipped/sama-drives-california.zip&quot;>;zip 文件&lt;/a>; (2.3GB)。请随时告诉我您的想法。&lt;/p>; &lt;p>;&lt;em>;免责声明：我为 Sama 工作&lt;/em>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/op4hdkqjf62b1.png?width=2239&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb3b66a194fc29c34fe42167d6b78af537b4bc7&quot;>;样本帧&lt;/a>;&lt;/p>; &lt;/div>;&lt;! -- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/iknowjerome&quot;>; /u/iknowjerome &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sd4ku/r_samadrivescalifornia_automotive_semantic/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sd4ku/r_samadrivescalifornia_automotive_semantic/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13sd4ku </id><media:thumbnail url="https://b.thumbs.redditmedia.com/6mKRKGxChyyetC3FAKtYmWYw3zxEdQM6gd3Tjw1DuwI.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13sd4ku/r_samadrivescalifornia_automotive_semantic/"/><updated> 2023-05-26T13:21:34+00:00</updated><published> 2023-05-26T13:21:34+00:00</published><title> [R] sama-drives-california：汽车语义分割数据集（25k 帧）现已可用</title></entry><entry><author><name>/u/ironborn123</name><uri> https://www.reddit.com/user/ironborn123 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;看起来有点雄心勃勃，但有点有趣。&lt;/p>; &lt;p>;&lt;a href=&quot;https://kommonmann.wordpress.com /2023/05/26/a-new-academic-citation-system-based-on-semantic-understanding-with-llms/&quot;>;https://kommonmann.wordpress.com/2023/05/26/a-new -academic-citation-system-based-on-semantic-understanding-with-llms/&lt;/a>;&lt;/p>; &lt;p>;作者提供了基本几何学的例子，这似乎是一个很好的开始。但这大规模可行吗？有人在构建这样的框架吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ironborn123&quot;>; /u/ironborn123 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sigt7/d_overhauling_research_citations_with_gpt4/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sigt7/d_overhauling_research_citations_with_gpt4/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sigt7 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sigt7/d_overhauling_research_citations_with_gpt4/"/><updated> 2023-05-26T16:54:08+00:00</updated><published> 2023-05-26T16:54:08+00:00</published><title> [D] 使用 GPT4 彻底修改研究引用？</title></entry><entry><author><name> /你/阿杜纳托</name><uri>https://www.reddit.com/user/adunato </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;最近，我一直在处理几个使用 PyTorch 的 GitHub 项目。对于每个项目，我维护一个单独的 Conda 环境（我通过艰难的方式了解到为什么这很重要）。&lt;/p>; &lt;p>;但是，我遇到的一个持续存在的问题涉及 PyTorch 与我的 CUDA 的兼容性版本。具体来说，通过 requirements.txt 文件安装的 PyTorch 版本通常与我的 CUDA 版本不兼容，导致无法识别 CUDA 设备。&lt;/p>; &lt;p>;为了解决这个问题，我采用了一种做法我从 requirements.txt 文件中删除了对 PyTorch（以及相关库，如 torchvision、torchaudio）的任何提及，并从官方 PyTorch 站点手动安装它。&lt;/p>; &lt;p>;这是一种常见做法吗？或者我错过了一个更简化的工作流程来确保 PyTorch 和 CUDA 的兼容性？我很想听听其他人是如何处理这个问题的。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/adunato&quot;>; /u/adunato &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s6x3b </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/"/><updated> 2023-05-26T08:02:39+00:00</updated><published> 2023-05-26T08:02:39+00:00</published><title> [D] 安装 PyTorch 以与特定 CUDA 版本保持一致的最佳实践</title></entry><entry><author><name>/u/Simple-Respect-1937</name><uri> https://www.reddit.com/user/Simple-Respect-1937 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，大家好！&lt;/p>; &lt;p>;我和我的团队正在进行人脸识别项目。我们所做的是，我们从实时摄像机中提取人脸图像，然后使用 Facenet 为每张脸进行嵌入。这些嵌入是向量。因此，通过测量两个向量（两个人脸图像的嵌入）之间的距离，我们可以判断这两张图像是否来自同一个人。这是我们阅读论文时人脸识别的正常程序。 &lt;/p>; &lt;p>;但是我们遇到的是，我们为印度人脸运行程序设置的阈值对东亚（中国）人脸不起作用，尽管它对印度人脸有效。所以我们也尝试阅读一些研究论文。那些论文也是如此，承认存在这样的问题。 &lt;/p>; &lt;p>;&lt;strong>;我只是想知道以前是否有人遇到过完全相同的问题。如果有的话，那么你采用了什么方法？&lt;/strong>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;我对 Reddit 有点陌生，所以如果我做了任何提问时出错，请见谅。谢谢大家！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Simple-Respect-1937&quot;>; /u/Simple-Respect-1937 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13s80ev/face_recognition_models_require_different/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s80ev/face_recognition_models_require_different/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s80ev </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s80ev/face_recognition_models_require_different/"/><updated> 2023-05-26T09:11:37+00:00</updated><published> 2023-05-26T09:11:37+00:00</published><title>人脸识别模型对不同种族要求不同的阈值？ [D]</title></entry><entry><author><name> /u/rwill128</name><uri> https://www.reddit.com/user/rwill128 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;有人知道与此主题相关的论文吗？ &lt;/p>; &lt;p>;看起来像法学硕士，尤其是即将成为多模态的，可以与传感器和相机输入密切相关，可以成为规划和高级考虑的强大工具，例如识别某些任务的机会等.&lt;/p>; &lt;p>;从我在 HuggingFace 论文等中看到的情况来看，LLM 的进展可能还没来得及深入机器人技术，但我想我会问。&lt;/p>; &lt;/ div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/rwill128&quot;>; /u/rwill128 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13scb1b/d_llms_in_robotics/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13scb1b/d_llms_in_robotics/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13scb1b </id><link href="https://www.reddit.com/r/MachineLearning/comments/13scb1b/d_llms_in_robotics/"/><updated> 2023-05-26T12:47:28+00:00</updated><published> 2023-05-26T12:47:28+00:00</published><title> [D] 机器人学法学硕士</title></entry><entry><author><name>/u/Hot-Heron4388</name><uri> https://www.reddit.com/user/Hot-Heron4388 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我很好奇是否有一种方法可以让模型根据用户的需求访问不同的知识集卷;除了训练不同的模型？例如，如果我有一个通常需要订阅的数据集，是否有一种方法可以让单个 LLM 仅在提供用户的订阅信息时才能访问此知识？我能想到的最接近的事情是：&lt;/p>; &lt;p>;A）根本不要在数据集上改进 LLM，只需通过增强提示合并额外的数据集信息&lt;/p>; &lt;p>;B）为每个可能的订阅数据集组合训练不同的 LLM，并且基于一个人的订阅，它们链接到不同的 LLM（这是我想避免的）。 &lt;/p>; &lt;p>;C) 根据用户的订阅对允许的提示实施限制。&lt;/p>; &lt;p>;理想情况下，我想知道是否有办法让我不需要做增强提示的单一法学硕士（因为我的数据集不小，所以我遇到了上下文窗口问题），而且我不想拥有无数不同的法学硕士稍微不一样。我读到的所有关于试图限制提示本身的内容（这样没有订阅的人就不能问相关问题）似乎相当困难并且经常被聪明的提示技术规避，或者需要一个大量的幕后工作来关闭任何给定的漏洞（这也只有在发现正在访问的额外信息后才有效）。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Hot-Heron4388&quot;>; /u/Hot-Heron4388 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/MachineLearning/comments/13shu7k/d_roles_based_model_knowledge/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13shu7k/d_roles_based_model_knowledge/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13shu7k </id><link href="https://www.reddit.com/r/MachineLearning/comments/13shu7k/d_roles_based_model_knowledge/"/><updated> 2023-05-26T16:29:58+00:00</updated><published> 2023-05-26T16:29:58+00:00</published><title> [D] 基于角色的模型知识？</title></entry><entry><author><name> /u/deviantkindle</name><uri> https://www.reddit.com/user/deviantkindle </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;IIUC，通过 chatGPT 接口发送的任何数据都可以（并且将会？）用于训练。相反，通过 API 提交的任何数据都不会用于训练。正确吗？&lt;/p>; &lt;p>;如果是这样，以下情况的可行性如何：InternA 无意中通过 chatGPT 提示上传了有关 CompanyA 的机密信息。为什么 EvilCompetitor 不能使用 chatGPT/API 来搜索此类机密信息？&lt;/p>; &lt;p>;我（目前）不是在寻找解决这个问题的方法；我正在查看它是否 &lt;em>;&lt;/em>; 是一个问题。因此，没有本地 LLM 或特殊的企业级护栏（“每月仅需 10,000 美元！但是等等！还有更多！”），或“IT 部门应该……”的建议。&lt; /p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/deviantkindle&quot;>; /u/deviantkindle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13shrc6/d_mining_openai_for_competitor_data/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13shrc6/d_mining_openai_for_competitor_data/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13shrc6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13shrc6/d_mining_openai_for_competitor_data/"/><updated> 2023-05-26T16:26:54+00:00</updated><published> 2023-05-26T16:26:54+00:00</published><title> [D] 为竞争对手数据挖掘 OpenAI</title></entry><entry><author><name> /u/博士疯子</name><uri>https://www.reddit.com/user/drBonkers </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;这是对在该领域使用 ML 的控诉吗？&lt;/p>; &lt;p>;Spotify、SoundCloud、和 Netflix 都这么烂？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/drBonkers&quot;>; /u/drBonkers &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13si63b/mlpowered_content_recommendation_are_mostly/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13si63b/mlpowered_content_recommendation_are_mostly/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13si63b </id><link href="https://www.reddit.com/r/MachineLearning/comments/13si63b/mlpowered_content_recommendation_are_mostly/"/><updated> 2023-05-26T16:42:39+00:00</updated><published> 2023-05-26T16:42:39+00:00</published><title>机器学习驱动的内容推荐大多很糟糕 [D]</title></entry><entry><author><name> /u/天眼2006</name><uri> https://www.reddit.com/user/dayeye2006 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我以前是一名机器学习工程师。但我已经多年没有接触 Pytorch（我在自己的初创公司工作，作为一名全栈工程师）。&lt;/p>; &lt;p>;有哪些好的资源可以刷新我的 PyTorch 技能？&lt;/p>; &lt;p>;我喜欢以“愚蠢的方式”学习东西。我计划从头开始实现一些最经典的模型（ResNet、TextCNN、transformers，...）。&lt;/p>; &lt;p>;当我学习编程语言时，我最喜欢参考的资源是 &lt;a href=&quot;https://github.com/topics/koans&quot;>;公案&lt;/a>;。这有助于我快速熟悉新语言。深度学习界有对应的吗？&lt;/p>; &lt;p>;谢谢&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/dayeye2006&quot;>; /u/dayeye2006 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13royi6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/"/><updated> 2023-05-25T18:13:41+00:00</updated><published> 2023-05-25T18:13:41+00:00</published><title> [D] 有哪些资源可以提高我的 PyTorch 技能？</title></entry><entry><author><name> /u/Facilex_zyzz</name><uri> https://www.reddit.com/user/Facilex_zyzz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嘿，AI 爱好者们！ 👋 我很高兴向您介绍 TypeNinja，这是一款改变游戏规则的工具，可在您在计算机上打字时将 OpenAI ChatGPT 的强大功能直接带到您的指尖。&lt;/p>; &lt;p>;使用 TypeNinja，您可以无缝访问来自任何应用程序的 OpenAI ChatGPT，使其成为您日常任务的多功能且不可或缺的伴侣。它实时监控您的输入并响应您的提示，在您需要的地方提供即时 AI 帮助。&lt;/p>; &lt;p>;但是 TypeNinja 真正独一无二的是它能够理解和响应您的自定义命令提示配置。您可以自由设置个性化提示，以触发来自 ChatGPT 的特定操作或行为。&lt;/p>; &lt;p>;例如，假设您配置命令提示符“gen:”描述为“响应我的请求，因为你是我的私人助理。”然后您可以设置一个发送键，例如“.”，它表示您的提示结束。现在，无论您在何处书写，无论是文档、电子邮件，甚至是聊天窗口，您都可以简单地键入“gen：你好，我的助手。”该消息将自动发送到 ChatGPT，它会在您输入的同一字段中响应，充当您的私人助理。&lt;/p>; &lt;p>;另一个例子是“twt”提示符，代表推特。您可以将其提示配置设置为“使用流行的主题标签发布关于主题的推文”。现在，无论何时你想生成关于特定主题的推文，你都可以写“twt：AI Revolution”。在任何文本字段中，TypeNinja 将自动生成一条关于该主题的推文，并带有相关和流行的主题标签。&lt;/p>; &lt;p>;TypeNinja 的用户友好界面使您可以轻松配置和监控您的提示使用情况。您可以查看聊天记录、微调提示并调整行为以符合您的喜好。这种级别的定制让您可以控制您的 AI 交互，允许您定制 TypeNinja 以满足您的独特需求。&lt;/p>; &lt;p>;无论您是编码、写电子邮件还是参与在线对话，TypeNinja 集成与您最喜爱的应用程序和工作流程一起顺利进行。告别为了获得 AI 帮助而在网站或应用程序之间切换的麻烦。 TypeNinja 可提高您的工作效率并全面简化您的工作流程。&lt;/p>; &lt;p>;隐私和安全对于 TypeNinja 至关重要。所有交互都在您的计算机上本地处理，确保您的敏感信息的机密性。 OpenAI 强大的安全措施进一步保护您的数据，让您在利用 TypeNinja 的强大功能时高枕无忧。&lt;/p>; &lt;p>;我迫不及待地想在未来的日子。准备好提升您的打字游戏水平，并通过 TypeNinja 释放 OpenAI ChatGPT 的全部潜力。请继续关注激动人心的发展并为打字革命做好准备！&lt;/p>; &lt;p>;网站：&lt;a href=&quot;https://www.typeninja.io&quot;>;https://www.typeninja.io&lt;/a>; &lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Facilex_zyzz&quot;>; /u/Facilex_zyzz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sh82x/p_my_project_typeninjaio_your_ai_companion_for/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sh82x/p_my_project_typeninjaio_your_ai_companion_for/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sh82x </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sh82x/p_my_project_typeninjaio_your_ai_companion_for/"/><updated> 2023-05-26T16:06:24+00:00</updated><published> 2023-05-26T16:06:24+00:00</published><title> [P] 我的项目 TypeNinja.io 增强打字的 AI 伴侣</title></entry><entry><author><name>/u/奇异语2501</name><uri> https://www.reddit.com/user/Singularian2501 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>; &lt;img src=&quot;https://a.thumbs.redditmedia .com/pwokgMFTSRP9bRbBlTbOA1gPSTlPoECDQdwoNNPMuG0.jpg&quot; alt=&quot;[R] Gorilla：连接大量 API 的大型语言模型 - Microsoft Research 2023 - 在编写 API 调用方面超越 GPT-4 的性能。 title=&quot;[R] Gorilla：连接大量 API 的大型语言模型 - Microsoft Research 2023 - 在编写 API 调用方面超越 GPT-4 的性能。&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.15334 &quot;>;https://arxiv.org/abs/2305.15334&lt;/a>; &lt;/p>; &lt;p>;Github：&lt;a href=&quot;https://github.com/ShishirPatil/gorilla&quot;>;https://github。 com/ShishirPatil/gorilla&lt;/a>; &lt;/p>; &lt;p>;博客：&lt;a href=&quot;https://gorilla.cs.berkeley.edu/&quot;>;https://gorilla.cs.berkeley.edu/&lt; /a>; &lt;/p>; &lt;p>;摘要：&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 最近出现了令人印象深刻的进步浪潮，模型现在在各种任务中表现出色，例如数学推理和程序综合。然而，它们通过 API 调用有效使用工具的潜力仍未实现。即使对于当今最先进的 LLM（例如 GPT-4）而言，这也是一项具有挑战性的任务，这主要是因为它们无法生成准确的输入参数，并且它们倾向于产生错误的 API 调用用法。我们发布了 Gorilla，这是一种经过微调的基于 LLaMA 的模型，在编写 API 调用方面超越了 GPT-4 的性能。当与文档检索器结合使用时，Gorilla 展示了适应测试时文档更改的强大能力，支持灵活的用户更新或版本更改。 &lt;strong>;它还大大减轻了直接提示 LLM 时经常遇到的幻觉问题。&lt;/strong>;为了评估模型的能力，我们引入了 APIBench，这是一个由 HuggingFace、TorchHub 和 TensorHub API 组成的综合数据集。 &lt;strong>;检索系统与 Gorilla 的成功集成表明 LLM 有潜力更准确地使用工具，跟上经常更新的文档，从而提高其输出的可靠性和适用性。&lt;/strong>;&lt;/p>; &lt; /blockquote>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/n5ezjchbg12b1.jpg?width=872&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eb5b7e11a22abe59d49504fad7278006a2b878a6&quot;>;https://preview.redd。它/n5ezjchbg12b1.jpg?width=872&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eb5b7e11a22abe59d49504fad7278006a2b878a6&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/e2xhpfhbg12b1.jpg ?width=1075&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b3c0f6ed7a6d72c93e681266977a0ec0f129ba6d&quot;>;https://preview.redd.it/e2xhpfhbg12b1.jpg?width=1075&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b 3c0f6ed7a6d72c93e681266977a0ec0f129ba6d&lt;/a >;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/i7i7bfhbg12b1.jpg?width=1213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5a287aba81199b66d1334457c6e8a12b3b5881c0&quot;>;https://预览。 redd.it/i7i7bfhbg12b1.jpg?width=1213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5a287aba81199b66d1334457c6e8a12b3b5881c0&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Singularian2501&quot;>; /u/Singularian2501 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13rl3v9 </id><media:thumbnail url="https://a.thumbs.redditmedia.com/pwokgMFTSRP9bRbBlTbOA1gPSTlPoECDQdwoNNPMuG0.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/"/><updated> 2023-05-25T15:42:26+00:00</updated><published> 2023-05-25T15:42:26+00:00</published><title> [R] Gorilla: Large Language Model Connected with Massive APIs - Microsoft Research 2023 - 在编写 API 调用方面超越了 GPT-4 的性能。</title></entry><entry><author><name> /u/arg_max</name><uri> https://www.reddit.com/user/arg_max </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨，&lt;/p>; &lt;p>;我为我的 neurips 提交深入研究了扩散器，发现了一些我认为有点奇怪但不#39;真的没有人可以与之讨论，所以我想我只是把它贴在这里，看看是否有人知道发生了什么，这是否是一个众所周知的现象。&lt;/p >; &lt;p>;所以调节稳定扩散。你有一个提示，类似于“狗的图像”。该提示通过 Clip 模型编码为条件矩阵，该矩阵通过交叉注意力输入 U-Net。此剪辑编码包括一个标记器，它将提示拆分为标记及其连续表示。这个分词器还包括一个“句子开头”放在每个标记化序列开头的标记（以及重复的“句子结尾”标记，直到达到最大标记数，对于稳定扩散为 77）。在交叉注意层中，然后将作为当前潜在 z_t 的 U-Net 编码版本的视觉特征投影到查询矩阵 Q 中。条件（即剪辑编码提示）被转换为键和值矩阵K 和 V。然后乘以 Q * K^T 并在行上取 softmax 以获得注意力概率矩阵。该矩阵中的每一行对应一个视觉特征，每一列对应文本条件中的一个标记。由于 softmax，行总和为 1，对于每个空间位置，您可以在标记上进行分布。基本上，它告诉提示中的一个标记/单词对某个空间位置的影响有多大。现在，我希望所有权重都集中在提示中的重要标记上（例如“狗”），但我发现平均而言，90-99% 的概率质量被放入“句子开头” ;令牌。然后这也意味着对应于“句子开头”的值矩阵中的条目是“句子的开始”。 token会支配交叉注意力层的输出，不管你写什么提示。对我来说，这很奇怪，显然，这不是手工编码的，而是学习的，因此优化发现 XA 层的输出具有较小的可变性，而是始终接近对应于“&amp;quot;”的值矩阵条目。句首”令牌在某种程度上是最好的。此外，这种行为在时间步长上是相同的，所以它发生在扩散过程的开始和结束时。&lt;/p>; &lt;p>;也许其他人经历过类似的事情或者知道这里发生了什么？&lt; /p>; &lt;p>;TLDR：稳定扩散中的注意力概率集中在 90-99% 的句子标记的一般开头，而不是来自实际提示的标记，与扩散时间步长、交叉注意力头或 U 无关-网络层。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/arg_max&quot;>; /u/arg_max &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_thinks_this_behavior/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_that_thinks_this_behavior/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rwh1c </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_that_thinks_this_behavior/"/><updated> 2023-05-25T23:10:42+00:00</updated><published> 2023-05-25T23:10:42+00:00</published><title> [D] 只有我认为这种行为（交叉注意力层）很奇怪吗？</title></entry><entry><author><name> /u/Ok_Bank_2217</name><uri> https://www.reddit.com/user/Ok_Bank_2217 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们需要为我们的平台获取大量 YouTube 数据并训练自定义 ML 模型，但除了 YouTube 之外找不到任何有用的东西8M Dataset，相当过时，信息非常有限。官方的 YouTube 数据 API 也被限制在大约 10.000 个积分，这远远不能满足我们需要的数量。&lt;/p>; &lt;p>;这就是为什么我们说去他妈的，并决定自己构建一个巨大的 YouTube 数据集。在为超过 1 亿个视频编制索引并构建自定义 API 来访问它之后，我们决定公开 API 并允许人们购买访问权限！&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.blizzy -data.com/&quot;>;链接到网站&lt;/a>;&lt;/p>; &lt;p>;我们很乐意听到我们的 ML 工程师和数据科学家的反馈，并希望解决您和我们遇到的问题!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Ok_Bank_2217&quot;>; /u/Ok_Bank_2217 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rh9yj </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/"/><updated> 2023-05-25T13:04:43+00:00</updated><published> 2023-05-25T13:04:43+00:00</published><title> [P] 我们创建了一个大型 YouTube 视频数据集来替换 YouTube 数据 API</title></entry><entry><author><name> /u/我是布兰妮</name><uri>https://www.reddit.com/user/ISpearedBritney </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你的很多工作都涉及 AI 或 ML（无论标题如何），你能分享一下你的典型工作日是怎样的吗？您将时间花在什么上，最终经常使用哪些工具或资源？其中有多少是数据争论，你使用了多少数学？谢谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ISpearedBritney&quot;>; /u/ISpearedBritney &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rct07 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/"/><updated> 2023-05-25T09:21:06+00:00</updated><published> 2023-05-25T09:21:06+00:00</published><title> [D] 对于那些从事 ML/AI 工作的人，您的工作和工作日是什么样的？</title></entry><entry><author><name> /u/奇异语2501</name><uri> https://www.reddit.com/user/Singularian2501 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/huTwcaqVaNrBZzikekNSL9XJi3tfEGZsggLCg50LMYM.jpg&quot; alt=&quot;[R] 用语言模型推理就是用世界模型进行规划 - Shibo Hao 等加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，计划相对改进了 33%世代设定！” title=&quot;[R] 用语言模型推理就是用世界模型进行规划 - Shibo Hao 等加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，在计划生成设置中相对改进了 33%！” />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.14992 &quot;>;https://arxiv.org/abs/2305.14992&lt;/a>; &lt;/p>; &lt;p>;摘要：&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 已显示出非凡的推理能力，尤其是当提示生成中间推理步骤（例如，Chain-of-Thought，CoT）。然而，LLM 仍然难以解决对人类来说很容易的问题，例如为在给定环境中执行任务生成行动计划，或者执行复杂的数学、逻辑和常识推理。缺陷源于一个关键事实，即 LLM 缺乏内部&lt;em>;世界模型&lt;/em>;来预测世界&lt;em>;状态&lt;/em>;（例如，环境状态、中间变量值）并模拟长期结果动作。这可以防止 LLM 执行类似于人脑的深思熟虑的计划，这涉及探索替代推理路径、预测未来状态和奖励，以及迭代改进现有推理步骤。为了克服这些限制，我们提出了一个新的 LLM 推理框架，&lt;strong>;&lt;em>;R&lt;/em>;&lt;/strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;easoning via&lt;/em>; /strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;P&lt;/em>;&lt;/strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;规划&lt;/em>;&lt;/strong >; &lt;strong>;(RAP).&lt;/strong>; RAP 将 &lt;strong>;LLM 重新定位为世界模型和推理代理，并结合原则性规划算法（基于蒙托卡罗树搜索）在广阔的推理中进行战略探索&lt;/strong>; 在推理过程中，LLM（作为代理）在LLM（作为世界模型）和任务特定奖励的指导下，增量构建推理树，并在适当的平衡下高效地获得高奖励推理路径在探索 &lt;em>; 与 &lt;/em>; 开发之间。我们将 RAP 应用于各种具有挑战性的推理问题，包括计划生成、数学推理和逻辑推理。这些任务的实证结果证明了 RAP 优于各种强大的基线，包括 CoT 和自洽性从最少到最多的提示。 &lt;strong>;LLAMA-33B 上的 RAP 超过了 GPT-4 上的 CoT，计划生成设置相对改进了 33%。&lt;/strong>;&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;a href=&quot;https://preview .redd.it/jaoiil2mc12b1.jpg?width=747&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53&quot;>;https://preview.redd.it/jaoiil2mc12b1.jpg?width=747&amp;amp;format= pjpg&amp;auto= webp&amp;amp;s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7389d75d0ff7 d1d8787c7c5f9add4787b02b47be &quot;>;https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7389d75d0ff7d1d8787c7c5f9add4787b02b47be&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https:/ /preview.redd.it/ykpqvp2mc12b1.jpg?width=980&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=834c39fb3e549418b8396725e86ddff3c6584077&quot;>;https://preview.redd.it/ykpqvp2mc12b1.jpg?width=9 80&amp;amp;格式=pjpg&amp;amp; auto=webp&amp;amp;s=834c39fb3e549418b8396725e86ddff3c6584077&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s =9c5192d6c012bfe4390fa67b010580b8e4508daa&quot;>;https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9c5192d6c012bfe4390fa67b010580 b8e4508daa&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https //preview.redd.it/qd8pjo2mc12b1.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea&quot;>;https://preview.redd.it/qd8pjo2mc12b1.jpg?width=140 0&amp;格式= pjpg&amp;amp;auto=webp&amp;amp;s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Singularian2501&quot;>; /u/Singularian2501 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13rkhzx </id><media:thumbnail url="https://b.thumbs.redditmedia.com/huTwcaqVaNrBZzikekNSL9XJi3tfEGZsggLCg50LMYM.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/"/><updated> 2023-05-25T15:17:59+00:00</updated><published> 2023-05-25T15:17:59+00:00</published><title> [R] 使用语言模型进行推理就是使用世界模型进行规划 - Shibo Hao 等人，加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，计划生成设置相对改进了 33%！</title></entry><entry><author><name> /u/钢铁侠马克20</name><uri> https://www.reddit.com/user/IronManMark20 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/IronManMark20&quot;>; /u/IronManMark20 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://gorilla.cs.berkeley. edu/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rpvgn/gorilla_large_language_model_connected_with/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rpvgn </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rpvgn/gorilla_large_language_model_connected_with/"/><updated> 2023-05-25T18:49:35+00:00</updated><published> 2023-05-25T18:49:35+00:00</published><title> Gorilla：连接大量 API 的大型语言模型</title></entry></feed>