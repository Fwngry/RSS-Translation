<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-25T06:03:52+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&quot; >;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qrtek </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/"/><updated> 2023-05-24T17:25:33+00:00</updated><published> 2023-05-24T17:25:33+00:00</published><title> [N] Andrej karpathy 在 MSBuild 2023 中的 GPT 状态</title></entry><entry><author><name>/你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-模型架构&quot;>;https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-model-architecture&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qroe9 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/"/><updated> 2023-05-24T17:20:14+00:00</updated><published> 2023-05-24T17:20:14+00:00</published><title> [N] Meta AI 释放 Megabyte，一种革命性的可扩展模型架构</title></entry><entry><author><name>/你/米勒</name><uri>https://www.reddit.com/user/mierle </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;QLoRA: Effi量化 LLM 的 cient Finetuning&quot; title=&quot;QLoRA：量化 LLM 的高效微调&quot; />; &lt; /a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mierle&quot;>; /u/mierle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14314&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13r1hkg </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/"/><updated> 2023-05-24T23:29:47+00:00</updated><published> 2023-05-24T23:29:47+00:00</published><title> QLoRA：量化 LLM 的高效微调</title></entry><entry><author><name>/u/J00Nnn</name><uri> https://www.reddit.com/user/J00Nnn </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，有在 k8s 上进行 ML 训练的经验的人可以分享您使用的工具或框架吗？它不一定是端到端的管道解决方案（例如 Kubeflow）。&lt;/p>; &lt;p>;例如，我有 TensorFlow 模型，我想利用分布式训练，但是在 Kubernetes 资源上。有什么建议吗？&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;顺便说一句，我在这方面的经验很少，所以欢迎任何新的方向或更正，谢谢！！&lt;/p>; &lt;/ div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/J00Nnn&quot;>; /u/J00Nnn &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r670t </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/"/><updated> 2023-05-25T03:05:52+00:00</updated><published> 2023-05-25T03:05:52+00:00</published><title> [讨论] 关于在 Kubernetes 上训练 ML 模型的指南</title></entry><entry><author><name>/你/nicku_a</name><uri> https://www.reddit.com/user/nicku_a </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们刚刚更新了 AgileRL，我们的强化学习训练框架比 SOTA 快 10 倍，支持离线强化学习！ &lt;/p>; &lt;p>;许多有 RL 可解决问题的人无法访问模拟器，但有大量数据。&lt;/p>; &lt;p>;您现在可以轻松地在静态数据上训练代理，而无需模拟，并使用进化超参数优化来更快更好地学习！&lt;/p>; &lt;p>;此版本包括：&lt;/p>; &lt;ul>; &lt;li>;新的通用离线 RL 训练功能，可从静态数据中学习&lt;/li >; &lt;li>;Conservative Q-Learning (CQL)&lt;/li>; &lt;li>;与 Minari 完全兼容&lt;/li>; &lt;/ul>; &lt;p>;查看：&lt;a href=&quot;https://github.com/ AgileRL/AgileRL&quot;>;https://github.com/AgileRL/AgileRL&lt;/a>; &lt;/p>; &lt;p>;如果你想参与这个项目，或者只是想进行讨论，请加入我们的discord （链接在我们的 GitHub 存储库顶部）！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/nicku_a&quot;>; /u/nicku_a &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qgzt5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/"/><updated> 2023-05-24T09:48:18+00:00</updated><published> 2023-05-24T09:48:18+00:00</published><title> [P] 离线强化学习 - 比具有进化 HPO 的 SOTA 快 10 倍</title></entry><entry><author><name>/你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://techcrunch.com/2023/05/23/microsoft-debuts-azure-ai-studio-to- let-developers-build-their-own-ai-copilots/&quot;>;https://techcrunch.com/2023/05/23/microsoft-debuts-azure-ai-studio-to-let-developers-build-their- own-ai-copilots/&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qs34u </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/"/><updated> 2023-05-24T17:35:51+00:00</updated><published> 2023-05-24T17:35:51+00:00</published><title> [N] Microsoft 的 Azure AI Studio 让开发人员可以构建自己的 AI“副驾驶”</title></entry><entry><author><name> /u/trolls_toll</name><uri> https://www.reddit.com/user/trolls_toll </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我最近一直在用很多 1000x1000 矩阵做一些数值模拟，主要是为了分散过去几个月的疯狂。我想我还不如把每件事都做对，然后从头开始整个考验——为我的 M1 机器选择最好的 BLAS 库（实际上我只是超级生疏，谷歌搜索感觉比手工推导更容易） .&lt;/p>; &lt;p>;目前conda-forge已经预编译了基于三个BLAS实现的包：openblas、netlib和accelerate。前两个是非原生的，后者是 Apple 为其处理器优化的。可能还有其他版本可以通过 Anaconda 获得，但我并没有真正检查，因为那里的大多数数字库都链接到英特尔的 MKL，这在 mac 上不起作用。 &lt;/p>; &lt;p>;安装不同版本的 BLAS 很简单，实际上只需在 YAML conda 配方中设置一个标志。因此，我最终使用 numpy 和 scipy 的原生 &lt;code>;.test()&lt;/code>; 方法以及我在网上找到的两个脚本对所有三个 BLAS 包进行了基准测试：&lt;a href=&quot;https: //gist.github.com/MarkDana/a9481b8134cf38a556cf23e1e815dafb#2-benchmarks&quot;>;Mark Dana 的大量 SVD&lt;/a>; 和 &lt;a href=&quot;https://gist.github.com/markus-beuckelmann/8bc25531b11158431a5b09a45abd627 6&quot; >;由 Markus Beuckelmann 编写的具有一些矩阵和不同矩阵分解的要点&lt;/a>;。 &lt;/p>; &lt;p>;这是我的结果，都是在新的 conda 环境中完成的：&lt;/p>; &lt;p>;&lt;strong>;apple 的加速 &lt;code>;blas=*=accelerate&lt;/code>;&lt;/strong>;&lt;/p >; &lt;ul>; &lt;li>;svd 1.03 秒&lt;/li>; &lt;li>;matmuls 20 秒&lt;/li>; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 3 次失败，25083 次通过，393 次跳过，1309 次取消选择, 44 xfailed, 5 xpassed, 25 warnings in 76.34s (0:01:16)&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 在 linalg/tests/test_cython_blas.py 测试失败，在20% &lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;conda-forge vanilla&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 13.53 秒&lt;/li>; &lt;li>;matmuls 44 秒&lt;/li >; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 没有失败，25075 次通过，404 次跳过，1309 次取消选择，44 次失败，5 次通过，69.25 秒 (0:01:09) 中有 32 次警告&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 7 次失败，37984 次通过，2301 次跳过，12295 次取消选择，139 次失败，9 次通过，355.99 秒 (0:05:55) 中有 72 次警告&lt;/li>; &lt; /ul>; &lt;p>;&lt;strong>;netlib &lt;code>;=*=netlib&lt;/code>;&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 4.44 秒&lt;/li>; &lt;li>;matmuls 330 秒&lt;/ li>; &lt;li>;numpy 12 失败，25063 次通过，404 次跳过，1309 次取消选择，44 次失败，5 次通过，73.60 秒 (0:01:13) 中有 24 条警告&lt;/li>; &lt;li>;scipy 153 失败，37839 次通过， 2301 跳过，12295 取消选择，139 xfailed，8 xpassed，347.62s (0:05:47) 内有 86 个警告&lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;openblas &lt;code>;=*=openblas&lt;/code>; &lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 12.44 秒&lt;/li>; &lt;li>;matmuls 45 秒&lt;/li>; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 没有失败 25075 通过, 404 skiped, 1309 undeselected, 44 xfailed, 5 xpassed, 69.98s (0:01:09) 内有 32 个警告&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 7 failed, 37984 passed, 2301 skiped, 12295 undeselected, 139 xfailed, 9 xpassed, 72 warnings in 356.14s (0:05:56)&lt;/li>; &lt;/ul>; &lt;p>;这里有几个教训：a) vanilla conda-forge numpy 和 scipy版本带有 openblas，它工作得很好，b) 不要使用 netlib，除非你的矩阵很小并且你需要做很多 SVD，或者知道为什么 c) Apple 的 &lt;code>;veclib/accelerate&lt;/ code>; 非常快，但它在数值上也不稳定。以至于 scipy 的开发者 &lt;a href=&quot;https://github.com/scipy/scipy/wiki/Dropping-support-for-Accelerate&quot;>;早在 2018 年就放弃了对它的任何支持&lt;/a >;。像该死的。也就是说，他们显然将它带回来了，因为 macOS Ventura 的 13.3 版本在 &lt;code>;accelerate&lt;/code>; 性能方面有了一些重大改进。&lt;/p>; &lt;p>;FIN &lt;/p>; &lt;p>;ps我打算在 Mathematica 中做我的事情，因为动态 3D 图 &amp;gt;&amp;gt;&amp;gt;在这里和那里节省了几分钟。&lt;/p>; &lt;p>;pps 呃，忘了补充，它全部在 Apple M1 Pro 上测试，10 个内核运行 Ventura 13.3.1，python 3.10.11，conda 23.3.1，numpy 1.24 .3，scipy 1.10.1，libblas 3.9.0，openblas 0.3.21。 Netlib blas 的版本是 2.104，accelerate、openblas 和 vanilla 的版本是 2.116&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/trolls_toll&quot;>; /u/trolls_toll &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qp0s6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/"/><updated> 2023-05-24T15:37:56+00:00</updated><published> 2023-05-24T15:37:56+00:00</published><title> [D] Apple Silicon 选择哪个 BLAS 库？</title></entry><entry><author><name> /u/深渊</name><uri>https://www.reddit.com/user/abystoma </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如标题所示，任何人都可以向我推荐论文或任何使用启发式方法预测隐藏神经元和输出层输出的资源，因为我们有一个数据集的输入和输出。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/abystoma&quot;>; /u/abystoma &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r8gzk </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/"/><updated> 2023-05-25T05:03:18+00:00</updated><published> 2023-05-25T05:03:18+00:00</published><title> [D] 是否有任何工作通过使用启发式来预测隐藏神经元和输出层的输出？</title></entry><entry><author><name> /u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;在 ChatGPT 之前一个月左右，我是一个团队的一员，该团队提交了一篇论文，我们将 LLM 应用于临床文本笔记的特征提取用于分流目的。这篇论文本月发表在一份医学杂志上，所以它更适合临床人群，但我还是想在这里分享它：&lt;a href=&quot;https://www.annfammed.org /content/21/3/240&quot;>;https://www.annfammed.org/content/21/3/240&lt;/a>; &lt;/p>; &lt;blockquote>; &lt;p>;&lt;strong>;目的&lt;/strong>; 呼吸症状是初级保健中最常见的主诉。这些症状通常会自行解决，但它们可能表明患有严重的疾病。随着医生工作量和医疗保健成本的增加，在面对面咨询之前对患者进行分类会有所帮助，可能会为低风险患者提供其他沟通方式。本研究的目的是训练一个机器学习模型，在前往初级保健诊所之前对有呼吸道症状的患者进行分类，并在分类的背景下检查患者的结果。&lt;/p>; &lt;p>;&lt;strong>;方法&lt;/strong>;我们训练了一个机器学习模型，使用仅在就诊前可用的临床特征。从接受 7 个国际疾病分类第 10 次修订代码（J00、J10、JII、J15、J20、J44、J45）之一的患者的 1,500 条记录中提取临床文本注释。冰岛雷克雅未克地区的所有初级保健诊所都包括在内。该模型在 2 个外部数据集中对患者进行评分，并将他们分为 10 个风险组（值越高风险越大）。我们分析了每组的选定结果。&lt;/p>; &lt;p>;&lt;strong>;结果&lt;/strong>; 风险组 1 至 5 包括 C 反应蛋白值较低的年轻患者、初级和急诊的重新评估率、与第 6 至 10 组相比，抗生素处方率、胸部 X 光 (CXR) 转诊和具有肺炎体征的 CXR。第 1 至 5 组没有具有肺炎体征或医生诊断为肺炎的 CXR。&lt;/p>; &lt;p>;&lt;strong>;结论&lt;/strong>; 该模型根据预期结果对患者进行了分类。该模型可以通过消除风险组 1 到 5 中的 CXR 转诊数量来减少 CXR 转诊的数量，从而在没有临床医生输入的情况下减少临床上不重要的偶发瘤发现。&lt;/p>; &lt;/blockquote>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qwvl1 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/"/><updated> 2023-05-24T20:33:52+00:00</updated><published> 2023-05-24T20:33:52+00:00</published><title> [R] 在初级保健中针对呼吸道症状对人工智能患者进行分类以改善患者预后：一项回顾性诊断准确性研究</title></entry><entry><author><name>/u/waa007</name><uri> https://www.reddit.com/user/waa007 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Alpaca or LLaMA ?(严格来说，它们是open available，不是open source，open source的定义来自&lt;a href=&quot;https: //opensource.org/osd&quot;>;OSI&lt;/a>;)&lt;/p>; &lt;p>;是否有其他一些开放的&lt;del>;source&lt;/del>;可用的LLM？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/waa007&quot;>; /u/waa007 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qnc80 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/"/><updated> 2023-05-24T14:30:17+00:00</updated><published> 2023-05-24T14:30:17+00:00</published><title> [D] 目前最好的开源 LLM 是什么？</title></entry><entry><author><name> /你/baqirjafari</name><uri> https://www.reddit.com/user/baqirjafari </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;我正在研究一个需要多语言嵌入模型的案例研究。我做了一些研究，发现 &lt;a href=&quot;https://www.sbert.net/docs/pretrained_models.html#model-overview&quot;>;paraphrase-multilingual-mpnet-base-v2&lt;/a>; 很好选项。但是，我想知道是否有更好的模型可以处理英语、乌尔都语、波斯语、阿拉伯语等语言。有人对其他多语言嵌入模型有任何建议或经验吗？我将不胜感激任何帮助或建议。非常感谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/baqirjafari&quot;>; /u/baqirjafari &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r5x6s </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/"/><updated> 2023-05-25T02:53:08+00:00</updated><published> 2023-05-25T02:53:08+00:00</published><title> [D] 寻找更好的多语言嵌入模型</title></entry><entry><author><name>/你/硬丸</name><uri>https://www.reddit.com/user/hardmaru </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;em>;Schmidhuber 采访表达了他对 AI 和 AGI 未来的看法。&lt;/em>;&lt;/p>; &lt;p>;&lt;em>;原创&lt;a href=&quot;https://www.forbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead -to-dystopia/&quot;>;来源&lt;/a>;。我认为 &lt;a href=&quot;/r/MachineLearning&quot;>;r/MachineLearning&lt;/a>; 对这次采访很感兴趣，并且与 AI 领域其他有影响力的领导者相比提出了另一种观点。&lt;/em>;&lt;/p>; &lt; p&lt;strong>;Juergen Schmidhuber，著名的“现代人工智能之父”，说他一生的工作不会导致反乌托邦&lt;/strong>;&lt;/p>; &lt;p>;&lt;em>;2023 年 5 月 23 日。供稿者 &lt;a href=&quot;https://twitter.com/hessiejones&quot;>;Hessie Jones&lt;/a>;.&lt;/em>;&lt;/p>; &lt;p>;随着人们越来越关注更先进的人工智能 (AI) 技术对社会的影响，技术界中有许多人担心这些进步的影响在生成人工智能中，如果他们不加检查。著名科学家、人工智能研究员、被广泛认为是该领域的先驱之一的于尔根·施密德胡伯博士则更为乐观。他宣称，许多突然警告 AI 危险的人只是为了宣传，利用媒体对杀手机器人的痴迷，这种机器人比医疗保健等领域的“好 AI”更受关注。&lt;/p>; &lt;p>;彻底改变各个行业并改善我们的生活是显而易见的，如果不良行为者利用该技术谋取私利，同样危险。我们是在走向一个反乌托邦的未来，还是有理由保持乐观？我有机会与 Juergen Schmidhuber 博士坐下来了解他对这辆看似飞速发展的 AI 火车的看法，它将使我们飞跃到未来。&lt;/p>; &lt;p>;作为 1970 年代的少年，Juergen Schmidhuber 成为着迷于创造智能机器的想法，这些机器可以自己学习和改进，在他的有生之年变得比他自己更聪明。这最终导致了他在深度学习领域的开创性工作。&lt;/p>; &lt;p>;1980 年代，他在慕尼黑工业大学 (TUM) 学习计算机科学，并于 1987 年获得文凭。他的论文是在最终的自我改进机器上，它们不仅通过一些预先连接的人工设计的学习算法进行学习，而且还学习和改进学习算法本身。几十年后，这成为一个热门话题。他还获得了博士学位。 Schmidhuber 于 1991 年在慕尼黑工业大学工作，为现代人工智能奠定了一些基础。&lt;/p>; &lt;p>;Schmidhuber 最出名的是他对循环神经网络 (RNN) 发展的贡献，这是最强大的人工神经网络类型可以处理语音和自然语言等序列数据。他与他的学生 Sepp Hochreiter、Felix Gers、Alex Graves、Daan Wierstra 等人一起发表了长短期记忆 (LSTM) 的架构和训练算法，LSTM 是一种广泛用于自然语言处理、语音识别的 RNN 、视频游戏、机器人和其他应用。 LSTM 已成为 20 世纪被引用最多的神经网络，《商业周刊》将其称为“&lt;a href=&quot;https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and -facebook-owe-j-rgen-schmidhuber-a-fortune?leadSource=uverify%20wall&quot;>;可以说是最商业化的 AI 成就&lt;/a>;。&quot;&lt;/p>; &lt;p>;在他的整个职业生涯中，Schmidhuber 获得了各种因其开创性工作而获得的奖项和荣誉。 2013 年，他被授予亥姆霍兹奖，以表彰他在机器学习领域的重大贡献。 2016 年，他因“对深度学习和神经网络的开创性贡献”而获得 IEEE 神经网络先驱奖。媒体经常称他为“现代人工智能之父”，&lt;/em>;，因为&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/most-cited-neural-nets.html&quot;>;最引用的神经网络&lt;/a>;都建立在他实验室的工作之上。不过，他很快指出，人工智能的历史&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/deep-learning-history.html&quot;>;可以追溯到几个世纪前。&lt;/a>;&lt;/p >; &lt;p>;尽管他取得了许多成就，但在 60 岁时，他感到在有生之年构建通用人工智能的时间压力越来越大，并继续致力于推动 AI 研发的边界。现任KAUST AI Initiative主任，瑞士AI实验室IDSIA科学总监，AI公司NNAISENSE的联合创始人兼首席科学家，该公司的座右铭是“AI∀”这是一种受数学启发的表达“AI For All”的方式。他继续致力于尖端人工智能技术和应用，以改善人类健康、延长人类寿命并让每个人的生活更轻松。&lt;/p>; &lt;p>;&lt;em>;为清楚起见，对以下采访进行了编辑。&lt;/em>; &lt;/p>; &lt;p>;&lt;strong>;Jones：感谢 Juergen 加入我的行列。你已经签署了关于 AI 武器的警告信。但是你没有在最近的出版物“暂停巨大的人工智能实验：一封公开信”上签名？有原因吗？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 谢谢 Hessie。很高兴和你说话。我意识到，许多在公开场合警告 AI 危险的人只是为了宣传。我认为最新的这封信不会产生任何重大影响，因为许多 AI 研究人员、公司和政府会完全忽视它。&lt;/p>; &lt;p>;该提案经常使用“我们”这个词。指的是“我们”，人类。但正如我过去多次指出的那样，没有“我们”之分。每个人都可以认同。问 10 个不同的人，您会听到 10 种关于什么是“好”的不同意见。其中一些意见将彼此完全不相容。不要忘记许多人之间的巨大冲突。&lt;/p>; &lt;p>;这封信还说，“&lt;em>;如果这样的暂停不能很快到位，政府应该进行干预并实施暂停。&lt;/em>;“问题是不同的政府对于什么对他们和其他人有好处也有不同的看法。大国 A 会说，如果我们不这样做，大国 B 将会（也许是秘密地）获得对我们的优势。大国 C 和 D 也是如此。&lt;/p>; &lt;p>;&lt;strong>;琼斯：每个人都承认对当前的生成人工智能技术的恐惧。此外，&lt;/strong>; &lt;a href=&quot;https://www.bbc.com/news/world-us-canada-65616866&quot;>;&lt;strong>;Sam Altman&lt;/strong>; 已公开承认这项技术存在的威胁strong>;&lt;/a>;&lt;strong>;，OpenAI 的 CEO 本人，呼吁对 AI 进行监管。从您的角度来看，是否存在生存威胁？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;AI 确实可以被武器化，我毫不怀疑会有各种各样的威胁AI 军备竞赛，但 AI 不会引入新的生存威胁。与根本不需要人工智能的核氢弹带来的更古老的威胁相比，人工智能武器带来的威胁似乎显得微不足道。我们应该更加害怕以氢弹火箭形式出现的半个世纪前的技术。 1961 年的沙皇炸弹的破坏力几乎是二战时期所有武器总和的 15 倍。尽管自 1980 年代以来发生了戏剧性的核裁军，但仍然有足够多的核弹头在两个小时内消灭人类文明，没有任何人工智能，我更担心那种古老的生存威胁，而不是相当无害的人工智能武器。&lt;/p >; &lt;p>;&lt;strong>;Jones：我意识到当你将 AI 比作核弹的威胁时，当前存在一种危险，即当前的技术可以交到人类手中并使他们“最终”能够造成进一步的伤害以非常精确的方式针对群体中的个人，例如有针对性的无人机攻击。你给了人们一个他们以前从未有过的工具集，让坏人能够像一些人指出的那样，比以前做更多的事情，因为他们没有这种技术。&lt;/strong >;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 现在，所有这一切在原则上听起来很可怕，但我们现有的法律足以应对这些由 AI 支持的新型武器。如果你用枪杀了人，你就会进监狱。如果您用其中一架无人机杀死某人，情况也是如此。执法部门将更好地了解新威胁和新武器，并将以更好的技术应对这些威胁。使无人机能够以一种需要一些跟踪和一些智能来执行的方式从远处瞄准人员，这在传统上是由技术熟练的人执行的，对我来说，这似乎只是传统武器的改进版本，比如枪，是，你知道，比老枪更聪明一点。&lt;/p>; &lt;p>;但是，原则上，所有这些都不是新的发展。许多世纪以来，我们经历了更好的武器装备和更致命的毒药等的发展，执法部门也不断发展他们的政策以应对这些威胁。所以，这并不是说我们突然有了一种新的生存威胁，而且它比我们大约六年来所经历的要令人担忧得多。大型核弹头不需要花哨的面部识别来杀死一个人。不，它只是摧毁了拥有一千万居民的整个城市。&lt;/p>; &lt;p>;&lt;strong>;琼斯：隐含的生存威胁是人类对这项技术的控制程度。我们看到了一些机会主义的早期案例，正如你所说，比起积极的突破，它们更容易受到媒体的关注。但你是在暗示这一切都会平衡吗？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;从历史上看，我们有着悠久的技术突破传统，这导致了武器的进步防御的目的也是为了保护。从棍子到石头，从斧头到火药，从大炮到火箭……现在到无人机……这对人类历史产生了巨大影响，但贯穿历史始终的是，那些利用技术达到自己目的的人就是他们自己，面对相同的技术，因为对方正在学习使用它来对付他们。这就是人类几千年历史中重复发生的事情，并将继续下去。我不认为新的 AI 军备竞赛会像旧的核弹头那样对生存构成威胁。&lt;/p>; &lt;p>;你说了一些重要的事情，因为有些人更喜欢谈论缺点而不是这项技术的好处，但这是一种误导，因为 95% 的人工智能研究和人工智能开发都是为了让人们更快乐，并促进人类的生活和健康。&lt;/p>; &lt;p>;&lt;strong>;琼斯：让我们谈谈 AI 研究中的一些有益进展，这些进展已经能够从根本上改变当今的方法并取得突破。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 好吧！例如，十一年前，我们的团队和我的博士后 Dan Ciresan 是第一个赢得 &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/first-time-deep-learning-won-medical- imaging-contest-september-2012.html&quot;>;通过深度学习进行医学成像竞赛&lt;/a>;。我们分析了女性乳腺细胞，目的是确定无害细胞与处于癌前阶段的细胞。通常，训练有素的肿瘤学家需要很长时间才能做出这些决定。我们的团队对癌症一无所知，却能够在大量此类数据上训练出一开始完全愚蠢的人工神经网络。它能够胜过所有其他方法。今天，这不仅用于乳腺癌，还用于放射学和检测动脉斑块，以及许多其他方面。我们在过去 3 年中开发的一些神经网络现在在成千上万的医疗保健应用程序中普遍存在，检测糖尿病和 Covid-19 等等。这最终将渗透到所有医疗保健领域。这种类型的 AI 的良好后果比使用 AI 进行犯罪的点击诱饵新方法重要得多。&lt;/p>; &lt;p>;&lt;strong>;Jones：采用是强化结果的产物。大规模采用要么让我们相信人们误入歧途，要么相反，技术正在对人们的生活产生积极影响。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;后者是可能的情况。对好的 AI 而不是坏的 AI 存在巨大的商业压力，因为公司想卖给你一些东西，而你只会购买你认为对你有好处的东西。因此，仅仅通过这种简单的商业压力，你就会对好的 AI 而不是坏的 AI 产生巨大的偏见。然而，与改善人们生活的人工智能纪录片相比，施瓦辛格电影中的世界末日场景更受关注。&lt;/p>; &lt;p>;&lt;strong>;琼斯：我认为人们会被好故事所吸引——包含对手和斗争的故事，但最终，会有幸福的结局。 And this is consistent with your comment on human nature and how history, despite its tendency for violence and destruction of humanity, somehow tends to correct itself.&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Let&#39;s take the example of a technology, which you are aware – GANs – General Adversarial Networks, which today has been used in applications for fake news and disinformation. In actuality, the purpose in the invention of GANs was far from what it is used for today.&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Yes, the name GANs was created in 2014 but we had the basic principle already in the early 1990s. More than 30 years ago, I called it &lt;em>;artificial curiosity&lt;/em>;. It&amp;#39;sa very simple way of injecting creativity into a little two network system. This creative AI is not just trying to slavishly imitate humans. Rather, it&#39;s inventing its own goals. Let me explain:&lt;/p>; &lt;p>;You have two networks. One network is producing outputs that could be anything, any action. Then the second network is looking at these actions and it&#39;s trying to predict the consequences of these actions. An action could move a robot, then something happens, and the other network is just trying to predict what will happen.&lt;/p>; &lt;p>;Now we can implement artificial curiosity by reducing the prediction error of the second network, which, at the same time, is the reward of the first network. The first network wants to maximize its reward and so it will invent actions that will lead to situations that will surprise the second network, which it has not yet learned to predict well.&lt;/p>; &lt;p>;In the case where the outputs are fake images, the first network will try to generate images that are good enough to fool the second network, which will attempt to predict the reaction of the environment: fake or real image, and it will try to become better at it. The first network will continue to also improve at generating images whose type the second network will not be able to predict. So, they fight each other. The 2nd network will continue to reduce its prediction error, while the 1st network will attempt to maximize it.&lt;/p>; &lt;p>;Through this zero-sum game the first network gets better and better at producing these convincing fake outputs which look almost realistic. So, once you have an interesting set of images by Vincent Van Gogh, you can generate new images that leverage his style, without the original artist having ever produced the artwork himself.&lt;/p>; &lt;p>;&lt;strong>;Jones: I see how the Van Gogh example can be applied in an education setting and there are countless examples of artists mimicking styles from famous painters but image generation from this instance that can happen within seconds is quite another feat. And you know this is how GANs has been used. What&#39;s more prevalent today is a socialized enablement of generating images or information to intentionally fool people. It also surfaces new harms that deal with the threat to intellectual property and copyright, where laws have yet to account for. And from your perspective this was not the intention when the model was conceived. What was your motivation in your early conception of what is now GANs?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; My old motivation for GANs was actually very important and it was not to create deepfakes or fake news but to enable AIs to be curious and invent their own goals, to make them explore their environment and make them creative.&lt;/p>; &lt;p>;Suppose you have a robot that executes one action, then something happens, then it executes another action, and so on, because it wants to achieve certain goals in the environment. For example, when the battery is low, this will trigger “pain” through hunger sensors, so it wants to go to the charging station, without running into obstacles, which will trigger other pain sensors. It will seek to minimize pain (encoded through numbers). Now the robot has a friend, the second network, which is a world model ––it&#39;s a prediction machine that learns to predict the consequences of the robot&#39;s actions.&lt;/p>; &lt;p>;Once the robot has a good model of the world, it can use it for planning. It can be used as a simulation of the real world. And then it can determine what is a good action sequence. If the robot imagines this sequence of actions, the model will predict a lot of pain, which it wants to avoid. If it plays this alternative action sequence in its mental model of the world, then it will predict a rewarding situation where it&#39;s going to sit on the charging station and its battery is going to load again. So, it&amp;#39;ll prefer to execute the latter action sequence.&lt;/p>; &lt;p>;In the beginning, however, the model of the world knows nothing, so how can we motivate the first network to generate experiments that lead to data that helps the world model learn something it didn&#39;t already know? That&#39;s what artificial curiosity is about. The dueling two network systems effectively explore uncharted environments by creating experiments so that over time the curious AI gets a better sense of how the environment works. This can be applied to all kinds of environments, and has medical applications.&lt;/p>; &lt;p>;&lt;strong>;Jones: Let&#39;s talk about the future. You have said, “&lt;/strong>;&lt;strong>;&lt;em>;Traditional humans won&#39;t play a significant role in spreading intelligence across the universe.&lt;/em>;&lt;/strong>;&lt;strong>;”&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Let&#39;s first conceptually separate two types of AIs. The first type of AI are tools directed by humans. They are trained to do specific things like accurately detect diabetes or heart disease and prevent attacks before they happen. In these cases, the goal is coming from the human. More interesting AIs are setting their own goals. They are inventing their own experiments and learning from them. Their horizons expand and eventually they become more and more general problem solvers in the real world. They are not controlled by their parents, but much of what they learn is through self-invented experiments.&lt;/p>; &lt;p>;A robot, for example, is rotating a toy, and as it is doing this, the video coming in through the camera eyes, changes over time and it begins to learn how this video changes and learns how the 3D nature of the toy generates certain videos if you rotate it a certain way, and eventually, how gravity works, and how the physics of the world works. Like a little scientist!&lt;/p>; &lt;p>;And I have predicted for decades that future scaled-up versions of such AI scientists will want to further expand their horizons, and eventually go where most of the physical resources are, to build more and bigger AIs. And of course, almost all of these resources are far away from earth out there in space, which is hostile to humans but friendly to appropriately designed AI-controlled robots and self-replicating robot factories. So here we are not talking any longer about our tiny biosphere; no, we are talking about the much bigger rest of the universe. Within a few tens of billions of years, curious self-improving &lt;a href=&quot;https://blogs.scientificamerican.com/observations/falling-walls-the-past-present-and-future-of-artificial-intelligence/&quot;>;AIs will colonize the visible cosmos&lt;/a>; in a way that&#39;s infeasible for humans. Those who don&#39;t won&#39;t have an impact. Sounds like science fiction, but since the 1970s I have been unable to see a plausible alternative to this scenario, except for a global catastrophe such as an all-out nuclear war that stops this development before it takes off.&lt;/p>; &lt;p>;&lt;strong>;Jones: How long have these AIs, which can set their own goals — how long have they existed? To what extent can they be independent of human interaction?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Neural networks like that have existed for over 30 years. My first simple adversarial neural network system of this kind is the one from 1990 described above. You don&#39;t need a teacher there; it&amp;#39;s just a little agent running around in the world and trying to invent new experiments that surprise its own prediction machine.&lt;/p>; &lt;p>;Once it has figured out certain parts of the world, the agent will become bored and will move on to more exciting experiments. The simple 1990 systems I mentioned have certain limitations, but in the past three decades, we have also built more &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/artificial-curiosity-since-1990.html&quot;>;sophisticated systems that are setting their own goals&lt;/a>; and such systems I think will be essential for achieving true intelligence. If you are only imitating humans, you will never go beyond them. So, you really must give AIs the freedom to explore previously unexplored regions of the world in a way that no human is really predefining.&lt;/p>; &lt;p>;&lt;strong>;Jones: Where is this being done today?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Variants of neural network-based artificial curiosity are used today for agents that learn to play video games in a human-competitive way. We have also started to use them for automatic design of experiments in fields such as materials science. I bet many other fields will be affected by it: chemistry, biology, drug design, you name it. However, at least for now, these artificial scientists, as I like to call them, cannot yet compete with human scientists.&lt;/p>; &lt;p>;I don&#39;t think it&#39;s going to stay this way but, at the moment, it&#39;s still the case. Sure, AI has made a lot of progress. Since 1997, there have been superhuman chess players, and since 2011, through the DanNet of my team, there have been &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/DanNet-triggers-deep-CNN-revolution-2011.html&quot;>;superhuman visual pattern recognizers&lt;/a>;. But there are other things where humans, at the moment at least, are much better, in particular, science itself. In the lab we have many first examples of self-directed artificial scientists, but they are not yet convincing enough to appear on the radar screen of the public space, which is currently much more fascinated with simpler systems that just imitate humans and write texts based on previously seen human-written documents.&lt;/p>; &lt;p>;&lt;strong>;Jones: You speak of these numerous instances dating back 30 years of these lab experiments where these self-driven agents are deciding and learning and moving on once they&#39;ve learned. And I assume that that rate of learning becomes even faster over time. What kind of timeframe are we talking about when this eventually is taken outside of the lab and embedded into society?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; This could still take months or even years :-) Anyway, in the not-too-distant future, we will probably see artificial scientists who are good at devising experiments that allow them to discover new, previously unknown physical laws.&lt;/p>; &lt;p>;As always, we are going to profit from the old trend that has held at least since 1941: every decade compute is getting 100 times cheaper.&lt;/p>; &lt;p>;&lt;strong>;Jones: How does this trend affect modern AI such as ChatGPT?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Perhaps you know that all the recent famous AI applications such as ChatGPT and similar models are largely based on principles of artificial neural networks invented in the previous millennium. The main reason why they works so well now is the incredible acceleration of compute per dollar.&lt;/p>; &lt;p>;ChatGPT is driven by a neural network called “Transformer” described in 2017 by Google. I am happy about that because a quarter century earlier in 1991 I had a particular Transformer variant which is now called the “&lt;a href=&quot;https://twitter.com/SchmidhuberAI/status/1576966129993797632?cxt=HHwWgMDSkeKVweIrAAAA&quot;>;Transformer with linearized self-attention&lt;/a>;”. Back then, not much could be done with it, because the compute cost was a million times higher than today. But today, one can train such models on half the internet and achieve much more interesting results.&lt;/p>; &lt;p>;&lt;strong>;Jones: And for how long will this acceleration continue?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; There&amp;#39;s no reason to believe that in the next 30 years, we won&amp;#39;t have another factor of 1 million and that&amp;#39;s going to be really significant. In the near future, for the first time we will have many not-so expensive devices that can compute as much as a human brain. The physical limits of computation, however, are much further out so even if the trend of a factor of 100 every decade continues, the physical limits (of 1051 elementary instructions per second and kilogram of matter) won&#39;t be hit until, say, the mid-next century. Even in our current century, however, we&#39;ll probably have many machines that compute more than all 10 billion human brains collectively and you can imagine, everything will change then!&lt;/p>; &lt;p>;&lt;strong>;Jones: That is the big question. Is everything going to change? If so, what do you say to the next generation of leaders, currently coming out of college and university. So much of this change is already impacting how they study, how they will work, or how the future of work and livelihood is defined. What is their purpose and how do we change our systems so they will adapt to this new version of intelligence?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; For decades, people have asked me questions like that, because you know what I&amp;#39;m saying now, I have basically said since the 1970s, it&#39;s just that today, people are paying more attention because, back then, they thought this was science fiction.&lt;/p>; &lt;p>;They didn&amp;#39;t think that I would ever come close to achieving my crazy life goal of building a machine that learns to become smarter than myself such that I can retire. But now many have changed their minds and think it&amp;#39;s conceivable. And now I have two daughters, 23 and 25. People ask me: what do I tell them? They know that Daddy always said, “&lt;em>;It seems likely that within your lifetimes, you will have new types of intelligence that are probably going to be superior in many ways, and probably all kinds of interesting ways.&lt;/em>;” How should they prepare for that? And I kept telling them the obvious: &lt;strong>;Learn how to learn new things&lt;/strong>;! It&amp;#39;s not like in the previous millennium where within 20 years someone learned to be a useful member of society, and then took a job for 40 years and performed in this job until she received her pension. Now things are changing much faster and we must learn continuously just to keep up. I also told my girls that no matter how smart AIs are going to get, learn at least the basics of math and physics, because that&#39;s the essence of our universe, and anybody who understands this will have an advantage, and learn all kinds of new things more easily. I also told them that social skills will remain important, because most future jobs for humans will continue to involve interactions with other humans, but I couldn&#39;t teach them anything about that; they know much more about social skills than I do.&lt;/p>; &lt;p>;You touched on the big philosophical question about people&#39;s purpose. Can this be answered without answering the even grander question: What&#39;s the purpose of the entire universe?&lt;/p>; &lt;p>;We don&#39;t know. But what&#39;s happening right now might be connected to the unknown answer. Don&#39;t think of humans as the crown of creation. Instead view human civilization as part of a much grander scheme, an important step (but not the last one) on the path of the universe from very simple initial conditions towards more and more unfathomable complexity. Now it seems ready to take its &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/deep-learning-history.html#future&quot;>;next step, a step comparable to the invention of life itself over 3.5 billion years ago&lt;/a>;. Alas, don&#39;t worry, in the end, all will be good!&lt;/p>; &lt;p>;&lt;strong>;Jones: Let&#39;s get back to this transformation happening right now with OpenAI. There are many questioning the efficacy and accuracy of ChatGPT, and are concerned its release has been premature. In light of the rampant adoption, educators have banned its use over concerns of plagiarism and how it stifles individual development. Should large language models like ChatGPT be used in school?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; When the calculator was first introduced, instructors forbade students from using it in school. Today, the consensus is that kids should learn the basic methods of arithmetic, but they should also learn to use the “artificial multipliers” aka calculators, even in exams, because laziness and efficiency is a hallmark of intelligence. Any intelligent being wants to minimize its efforts to achieve things.&lt;/p>; &lt;p>;And that&amp;#39;s the reason why we have tools, and why our kids are learning to use these tools. The first stone tools were invented maybe 3.5 million years ago; tools just have become more sophisticated over time. In fact, humans have changed in response to the properties of their tools. Our anatomical evolution was shaped by tools such as spears and fire. So, it&amp;#39;s going to continue this way. And there is no permanent way of preventing large language models from being used in school.&lt;/p>; &lt;p>;&lt;strong>;Jones: And when our children, your children graduate, what does their future work look like?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; A single human trying to predict details of how 10 billion people and their machines will evolve in the future is like a single neuron in my brain trying to predict what the entire brain and its tens of billions of neurons will do next year. 40 years ago, before the WWW was created at CERN in Switzerland, who would have predicted all those young people making money as YouTube video bloggers?&lt;/p>; &lt;p>;Nevertheless, let&#39;s make a few limited job-related observations. For a long time, people have thought that desktop jobs may require more intelligence than skills trade or handicraft professions. But now, it turns out that it&amp;#39;s much easier to replace certain aspects of desktop jobs than replacing a carpenter, for example. Because everything that works well in AI is happening behind the screen currently, but not so much in the physical world.&lt;/p>; &lt;p>;There are now artificial systems that can read lots of documents and then make really nice summaries of these documents. That is a desktop job. Or you give them a description of an illustration that you want to have for your article and pretty good illustrations are being generated that may need some minimal fine-tuning. But you know, all these desktop jobs are much easier to facilitate than the real tough jobs in the physical world. And it&amp;#39;s interesting that the things people thought required intelligence, like playing chess, or writing or summarizing documents, are much easier for machines than they thought. But for things like playing football or soccer, there is no physical robot that can remotely compete with the abilities of a little boy with these skills. So, AI in the physical world, interestingly, is much harder than AI behind the screen in virtual worlds. And it&amp;#39;s really exciting, in my opinion, to see that jobs such as plumbers are much more challenging than playing chess or writing another tabloid story.&lt;/p>; &lt;p>;&lt;strong>;Jones: The way data has been collected in these large language models does not guarantee personal information has not been excluded. Current consent laws already are outdated when it comes to these large language models (LLM). The concern, rightly so, is increasing surveillance and loss of privacy. What is your view on this?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; As I have indicated earlier: are surveillance and loss of privacy inevitable consequences of increasingly complex societies? Super-organisms such as cities and states and companies consist of numerous people, just like people consist of numerous cells. These cells enjoy little privacy. They are constantly monitored by specialized &amp;quot;police cells&amp;quot; and &amp;quot;border guard cells&amp;quot;: Are you a cancer cell? Are you an external intruder, a pathogen? Individual cells sacrifice their freedom for the benefits of being part of a multicellular organism.&lt;/p>; &lt;p>;Similarly, for super-organisms such as nations. Over 5000 years ago, writing enabled recorded history and thus became its inaugural and most important invention. Its initial purpose, however, was to facilitate surveillance, to track citizens and their tax payments. The more complex a super-organism, the more comprehensive its collection of information about its constituents.&lt;/p>; &lt;p>;200 years ago, at least, the parish priest in each village knew everything about all the village people, even about those who did not confess, because they appeared in the confessions of others. Also, everyone soon knew about the stranger who had entered the village, because some occasionally peered out of the window, and what they saw got around. Such control mechanisms were temporarily lost through anonymization in rapidly growing cities but are now returning with the help of new surveillance devices such as smartphones as part of digital nervous systems that tell companies and governments a lot about billions of users. Cameras and drones etc. are becoming increasingly tinier and more ubiquitous. More effective recognition of faces and other detection technology are becoming cheaper and cheaper, and many will use it to identify others anywhere on earth; the big wide world will not offer any more privacy than the local village. Is this good or bad? Some nations may find it easier than others to justify more complex kinds of super-organisms at the expense of the privacy rights of their constituents.&lt;/p>; &lt;p>;&lt;strong>;Jones: So, there is no way to stop or change this process of collection, or how it continuously informs decisions over time? How do you see governance and rules responding to this, especially amid&lt;/strong>; &lt;a href=&quot;https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html&quot;>;&lt;strong>;Italy&#39;s ban on ChatGPT following&lt;/strong>;&lt;/a>; &lt;strong>;suspected user data breach and the more recent news about the&lt;/strong>; &lt;a href=&quot;https://www.reuters.com/technology/facebook-given-record-13-bln-fine-given-5-months-stop-eu-us-data-flows-2023-05-22/&quot;>;&lt;strong>;Meta&#39;s record $1.3billion fine&lt;/strong>;&lt;/a>; &lt;strong>;in the company&#39;s handling of user information?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Data collection has benefits and drawbacks, such as the loss of privacy. How to balance those? I have argued for addressing this through data ownership in data markets. If it is true that data is the new oil, then it should have a price, just like oil. At the moment, the major surveillance platforms such as Meta do not offer users any money for their data and the transitive loss of privacy. In the future, however, we will likely see attempts at creating efficient data markets to figure out the data&amp;#39;s true financial value through the interplay between supply and demand.&lt;/p>; &lt;p>;Even some of the sensitive medical data should not be priced by governmental regulators but by patients (and healthy persons) who own it and who may sell or license parts thereof as micro-entrepreneurs in a healthcare data market.&lt;/p>; &lt;p>;Following a previous &lt;a href=&quot;https://www.swissre.com/institute/conferences/The-intelligence-behind-artificial-intelligence.html&quot;>;interview&lt;/a>;, I gave for one of the largest re-insurance companies , let&amp;#39;s look at the different participants in such a data market: patients, hospitals, data companies. (1) &lt;strong>;Patients&lt;/strong>; with a rare form of cancer can offer more valuable data than patients with a very common form of cancer. (2) &lt;strong>;Hospitals&lt;/strong>; and their machines are needed to extract the data, eg, through magnet spin tomography, radiology, evaluations through human doctors, and so on. (3) &lt;strong>;Companies&lt;/strong>; such as Siemens, Google or IBM would like to buy annotated data to make better artificial neural networks that learn to predict pathologies and diseases and the consequences of therapies. Now the market&#39;s invisible hand will decide about the data&#39;s price through the interplay between demand and supply. On the demand side, you will have several companies offering something for the data, maybe through an app on the smartphone (a bit like a stock market app). On the supply side, each patient in this market should be able to profit from high prices for rare valuable types of data. Likewise, competing data extractors such as hospitals will profit from gaining recognition and trust for extracting data well at a reasonable price. The market will make the whole system efficient through incentives for all who are doing a good job. Soon there will be a flourishing ecosystem of commercial data market advisors and what not, just like the ecosystem surrounding the traditional stock market. The value of the data won&#39;t be determined by governments or ethics committees, but by those who own the data and decide by themselves which parts thereof they want to license to others under certain conditions.&lt;/p>; &lt;p>;At first glance, a market-based system seems to be detrimental to the interest of certain monopolistic companies, as they would have to pay for the data - some would prefer free data and keep their monopoly. However, since every healthy and sick person in the market would suddenly have an incentive to collect and share their data under self-chosen anonymity conditions, there will soon be many more useful data to evaluate all kinds of treatments. On average, people will live longer and healthier, and many companies and the entire healthcare system will benefit.&lt;/p>; &lt;p>;&lt;strong>;Jones: Finally, what is your view on open source versus the private companies like Google and OpenAI? Is there a danger to supporting these private companies&#39; large language models versus trying to keep these models open source and transparent, very much like what LAION is doing?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; I signed this &lt;a href=&quot;https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/?sh=6973c08b62e3&quot;>;open letter by LAION&lt;/a>; because I strongly favor the open-source movement. And I think it&amp;#39;s also something that is going to challenge whatever big tech dominance there might be at the moment. Sure, the best models today are run by big companies with huge budgets for computers, but the exciting fact is that open-source models are not so far behind, some people say maybe six to eight months only. Of course, the private company models are all based on stuff that was created in academia, often in little labs without so much funding, which publish without patenting their results and open source their code and others take it and improved it.&lt;/p>; &lt;p>;Big tech has profited tremendously from academia; their main achievement being that they have scaled up everything greatly, sometimes even failing to credit the original inventors.&lt;/p>; &lt;p>;So, it&amp;#39;s very interesting to see that as soon as some big company comes up with a new scaled-up model, lots of students out there are competing, or collaborating, with each other, trying to come up with equal or better performance on smaller networks and smaller machines. And since they are open sourcing, the next guy can have another great idea to improve it, so now there&#39;s tremendous competition also for the big companies.&lt;/p>; &lt;p>;Because of that, and since AI is still getting exponentially cheaper all the time, I don&amp;#39;t believe that big tech companies will dominate in the long run. They find it very hard to compete with the enormous open-source movement. As long as you can encourage the open-source community, I think you shouldn&amp;#39;t worry too much. Now, of course, you might say if everything is open source, then the bad actors also will more easily have access to these AI tools. And there&amp;#39;s truth to that. But as always since the invention of controlled fire, it was good that knowledge about how technology works quickly became public such that everybody could use it. And then, against any bad actor, there&amp;#39;s almost immediately a counter actor trying to nullify his efforts. You see, I still believe in our old motto &amp;quot;AI∀&amp;quot; or &amp;quot;AI For All.&amp;quot;&lt;/p>; &lt;p>;&lt;strong>;Jones: Thank you, Juergen for sharing your perspective on this amazing time in history. It&#39;s clear that with new technology, the enormous potential can be matched by disparate and troubling risks which we&#39;ve yet to solve, and even those we have yet to identify. If we are to dispel the fear of a sentient system for which we have no control, humans, alone need to take steps for more responsible development and collaboration to ensure AI technology is used to ultimately benefit society. Humanity will be judged by what we do next.&lt;/strong>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/hardmaru&quot;>; /u/hardmaru &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13q6k4a </id><link href="https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/"/><updated> 2023-05-24T01:00:28+00:00</updated><published> 2023-05-24T01:00:28+00:00</published><title> Interview with Juergen Schmidhuber, renowned &#39;Father Of Modern AI&#39;, says his life&#39;s work won&#39;t lead to dystopia.</title></entry><entry><author><name> /u/we_are_mammals</name><uri> https://www.reddit.com/user/we_are_mammals </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://www.reuters.com/technology/nvidias-results-spark-nearly-300-billion-rally-ai-stocks-2023-05-24/&quot;>;https://www.reuters.com/technology/nvidias-results-spark-nearly-300-billion-rally-ai-stocks-2023-05-24/&lt;/a>;&lt;/p>; &lt;p>;&amp;quot;&amp;quot;&amp;quot;&lt;/p>; &lt;p>;&amp;quot;With all the enthusiasm around AI and the fact Nvidia delivered a huge beat for first-quarter results and second-quarter estimates, this gives some actual evidence AI is for real,&amp;quot; said Daniel Morgan, senior portfolio manager at Synovus Trust in Atlanta.&lt;/p>; &lt;p>;Interest in AI surged this year after startup OpenAI introduced ChatGPT, attracting over a million users within a week.&lt;/p>; &lt;p>;&amp;quot;&amp;quot;&amp;quot;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/we_are_mammals&quot;>; /u/we_are_mammals &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r8i4y/n_airelated_stocks_just_surged_300_billion_in/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r8i4y/n_airelated_stocks_just_surged_300_billion_in/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r8i4y </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r8i4y/n_airelated_stocks_just_surged_300_billion_in/"/><updated> 2023-05-25T05:05:04+00:00</updated><published> 2023-05-25T05:05:04+00:00</published><title> [N] AI-related stocks just surged $300 BILLION in after-hours trading</title></entry><entry><author><name> /你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/ai-generated-podcast-ads-on-spotify-could-soon-become-a-reality-1f6bb1a056b0&quot;>;https://medium.com/@tiago-mesquita/ai-generated-podcast-ads-on-spotify-could-soon-become-a-reality-1f6bb1a056b0&lt;/a>;&lt;/p>; &lt;p>;During a recent episode of &lt;strong>;The Bill Simmons Podcast,&lt;/strong>; the host, and founder of The Ringer, Bill Simmons, expressed his belief in the potential of utilizing his own voice for advertisements.&lt;/p>; &lt;p>;&lt;strong>;He stated:&lt;/strong>;&lt;/p>; &lt;blockquote>; &lt;p>;&lt;em>;“There is going to be a way to use my voice for the ads. You have to obviously give the approval for the voice, but it opens up, from an advertising standpoint, all these different great possibilities for you.”&lt;/em>;&lt;/p>; &lt;/blockquote>; &lt;p>;Simmons is the founder of The Ringer, a podcast network and website that was bought by Spotify for nearly $200 million in 2020&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qq5ky </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/"/><updated> 2023-05-24T16:21:42+00:00</updated><published> 2023-05-24T16:21:42+00:00</published><title> [N] Spotify may be working on the possibility of providing AI-Generated podcast ads</title></entry><entry><author><name> /u/jesst177</name><uri> https://www.reddit.com/user/jesst177 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi!&lt;/p>; &lt;p>;We recently decide to buy a workstation with a budget of $15K. We look at our option in local vendor and also check their compute power, and came up with a couple of option.&lt;/p>; &lt;p>;- 4X A4500&lt;/p>; &lt;p>;- 1XA6000&lt;/p>; &lt;p>;We can also look for any other alternatives with mid level options such as 2X A5000/A5500. However from our standing point A4500s are having more compute power, and will have around 80 GB memory. Although I am not sure whether we can use it all of them together as in multi-gpu setting (Can we?) which mean it is better option. Should we go with 4X A4500 or any of the mid options?&lt;/p>; &lt;p>;The machine we are interested in will be used in Deep Learning, with Transformers and ConvNets.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/jesst177&quot;>; /u/jesst177 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qxf3g </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/"/><updated> 2023-05-24T20:55:18+00:00</updated><published> 2023-05-24T20:55:18+00:00</published><title> [D] Should we go with a single A6000 or 4XA4500 or any other alternative such as 2XA5000</title></entry><entry><author><name> /u/wazazzz</name><uri> https://www.reddit.com/user/wazazzz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hey all, just wanting to share this open source library I&#39;ve been working on that aims to makes LLMs experimentation (prompt chain engineering, fine tuning, variable integrated code generation, token probability/perplexity analysis) more accessible and easier to setup.&lt;/p>; &lt;p>;Open for feedback and collaboration!&lt;/p>; &lt;p>;&lt;a href=&quot;https://github.com/Pan-ML/panml&quot;>;https://github.com/Pan-ML/panml&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/wazazzz&quot;>; /u/wazazzz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qj9ye </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/"/><updated> 2023-05-24T11:49:13+00:00</updated><published> 2023-05-24T11:49:13+00:00</published><title> [Project] PanML, a high level Python library for fast LLM experimentation</title></entry><entry><author><name> /u/Jean-Porte</name><uri> https://www.reddit.com/user/Jean-Porte </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi everyone,&lt;/p>; &lt;p>;I just finished the first version of tasksource-instruct.&lt;br/>; &lt;a href=&quot;https://huggingface.co/datasets/tasksource/tasksource-instruct-v0&quot;>;https://huggingface.co/datasets/tasksource/tasksource-instruct-v0&lt;/a>;&lt;br/>; It is based on hundreds of classification datasets on huggingface. Tasks not in flan include dynasent (adversarial sentiment analysis), Dynahate (adversarial hate speech detection, discriminative babi, epistemic logic, ruletaker, MANY natural language inference datasets.&lt;/p>; &lt;p>;It is also focused on explicitly classification, which isolates reasoning and specific linguistic problems, and complements flan.&lt;/p>; &lt;p>;I believe that it can be a valuable contributions to current open source LLM.&lt;/p>; &lt;p>;I would be glad to know what you think, thank you.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jean-Porte&quot;>; /u/Jean-Porte &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qowlg </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/"/><updated> 2023-05-24T15:33:03+00:00</updated><published> 2023-05-24T15:33:03+00:00</published><title> [R] tasksource-instruct: an open source instruction-tuning dataset focused on classification, with many tasks not in flan.</title></entry><entry><author><name> /u/qooooob</name><uri> https://www.reddit.com/user/qooooob </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;I&amp;#39;m collecting a dataset from documents which are essentially scanned papers with text and tables within them. Sometimes the question is best answered by detecting, parsing and cleaning the table data (eg with AWS Textract + post-processing), but other times it would be beneficial to use the raw text from OCR. For LLMs I&amp;#39;ve been using just the OCR output as context to answer the question, but information in tables is lost.&lt;/p>; &lt;p>;I can see LLMs struggle answering questions especially when part of the context of the answer originates from tabular data, since OCR just parses that as a string of words separated by &lt;code>;\n&lt;/code>; and the table structure is lost in the process. &lt;/p>; &lt;p>;A document could look like this:&lt;/p>; &lt;blockquote>; &lt;p>;Here is a table consisting of answers. &lt;/p>; &lt;p>;As we can see a large part of increase in cost of &lt;/p>; &lt;p>;living can be attributed to increased rent. [...]&lt;/p>; &lt;/blockquote>; &lt;table>;&lt;thead>; &lt;tr>; &lt;th align=&quot;left&quot;>;&lt;/th>; &lt;th align=&quot;left&quot;>;30.6.2021&lt;/th>; &lt;th align=&quot;left&quot;>;30.6.2020&lt;/th>; &lt;/tr>; &lt;/thead>;&lt;tbody>; &lt;tr>; &lt;td align=&quot;left&quot;>;Cost of living&lt;/td>; &lt;td align=&quot;left&quot;>;5 021,55&lt;/td>; &lt;td align=&quot;left&quot;>;4 921,31&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Apartment&lt;/td>; &lt;td align=&quot;left&quot;>;2 421,56&lt;/td>; &lt;td align=&quot;left&quot;>;2 200,60&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Cost of food&lt;/td>; &lt;td align=&quot;left&quot;>;&lt;/td>; &lt;td align=&quot;left&quot;>;400,00&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Electricity&lt;/td>; &lt;td align=&quot;left&quot;>;B00,00&lt;/td>; &lt;td align=&quot;left&quot;>;799,00&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;in OCR this could look like &lt;/p>; &lt;pre>;&lt;code>;Here is a table consisting of answers. As we can see a large part of increase in cost of living can be attributed to increased rent. [...] 30.6.2021 30.6.2020 Cost of living 5 021,55 4 921,31 Apartment 2 421,56 2 200,60 Cost of food 400,00 Electricity B00,00 799,00 &lt;/code>;&lt;/pre>; &lt;p>;So basically the context from the table is lost and eg for &lt;code>;Cost of food&lt;/code>; it&amp;#39;s impossible to know whether the figure is from 2020 or 2021. Intuitively I think it would be beneficial for the LLM to see the data in the order it appears so that data in tables is somehow structured in the text. So that the output would look like this instead&lt;/p>; &lt;pre>;&lt;code>;Here is a table consisting of answers. As we can see a large part of increase in cost of living can be attributed to increased rent. [...] Cost of living (30.6.2021): 5 021,55 Cost of living (30.6.2020): 4 921,31 Apartment (30.6.2021): 2 421,56 Apartment (30.6.2020): 2 200,60 Cost of food (30.6.2021): No data Cost of food (30.6.2020): 400,00 &amp;lt;continued...&amp;gt; &lt;/code>;&lt;/pre>; &lt;p>;First of all I don&amp;#39;t know if this is necessary, or if there is a better approach to sending documents that contain both text/tabular data to LLMs. I have looked into libraries such as &lt;code>;unstructured&lt;/code>; that can return the layout of the document and the table data within it as HTML using &lt;code>;detectron2&lt;/code>;, which could be then parsed into something that looks like the above example, but I&amp;#39;m not very pleased with the quality of the table detection and it is quite slow. Also I imagine this library tries to fit many more use cases than what I need - essentially text and tabular text in different forms (lists, tables, borderless tables). At the moment I&amp;#39;m using AWS textract for table detection which works great but I&amp;#39;d like to move away from it an create my own model that is optimized for my use case and that is free.&lt;/p>; &lt;p>;Currently I&amp;#39;m thinking about creating a pipeline where I train a custom table detection model on my own dataset&lt;/p>; &lt;p>;-&amp;gt; Turn PDF page to an image&lt;br/>; -&amp;gt; Detect location of tables with a model like Table Transformer (TATR)&lt;br/>; -&amp;gt; Collect and remove table from image&lt;br/>; -&amp;gt; Run regular OCR on image with only text and no tables and&lt;br/>; -&amp;gt; Run a model for table recognition/extraction like AWS textract or TATR to extract tabular data&lt;br/>; -&amp;gt; Turn table data into structured text data&lt;br/>; -&amp;gt; Join the text and table datas in order of appearance to create one long text document with all the info of the PDF.&lt;/p>; &lt;p>;Any feedback or suggestions on this? Also is Microsoft&amp;#39;s Table Transformer a smart model to fine-tune with my own data, or are there others that perform better?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/qooooob&quot;>; /u/qooooob &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qhdmz </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/"/><updated> 2023-05-24T10:10:37+00:00</updated><published> 2023-05-24T10:10:37+00:00</published><title> [D] Exctracting from documents that consist of text and tabular data for use with LLMs</title></entry><entry><author><name> /u/Cold_Cantaloupe9212</name><uri> https://www.reddit.com/user/Cold_Cantaloupe9212 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;I am interested in developing a conditional diffusion model that guarantees consistent outputs for a given input. I would like to reduce or remove the stochasticity in the model to achieve this goal. Is there a way to accomplish this while maintaining some level of variability?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Cold_Cantaloupe9212&quot;>; /u/Cold_Cantaloupe9212 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r3qao </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/"/><updated> 2023-05-25T01:10:06+00:00</updated><published> 2023-05-25T01:10:06+00:00</published><title> [D]eterministic diffusion models</title></entry><entry><author><name> /u/Meddhouib10</name><uri> https://www.reddit.com/user/Meddhouib10 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hello, Many papers speak about the number of training steps for their model. My question is, when gradient accumulation is used, do we speak about gradient descent steps or just normal training steps ?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Meddhouib10&quot;>; /u/Meddhouib10 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qk9f1/r_number_of_training_steps_in_papers/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qk9f1/r_number_of_training_steps_in_papers/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qk9f1 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qk9f1/r_number_of_training_steps_in_papers/"/><updated> 2023-05-24T12:30:09+00:00</updated><published> 2023-05-24T12:30:09+00:00</published><title> [R] Number of training steps in papers</title></entry><entry><author><name> /u/matt_leming</name><uri> https://www.reddit.com/user/matt_leming </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;How much can deep autoencoders reduce dimensionality of data? I&amp;#39;m trying to implement something that can compress brain images (96&lt;sup>;3&lt;/sup>; ) to a vector (512). It&amp;#39;s basically outputting giant blurs. I&amp;#39;ve tried variational, regular, MMD, and am just going through the process off adjusting weights and tinkering. &lt;/p>; &lt;p>;On the one hand, I know that this type of compression may be asking a lot of the machine learning gods. On the other hand, I&amp;#39;ve seen 3d GANs that can output real crisp brain images, varying widely, no problem. And my implementation should at least be able to overfit on the training set, which it isn&amp;#39;t doing.是什么赋予了？ Do I need an adversarial autoencoder? Why are these models suddenly terrible when one measly dimension is added?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/matt_leming&quot;>; /u/matt_leming &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1pdq/p_compression_ratio_with_deep_autoencoder_for_3d/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1pdq/p_compression_ratio_with_deep_autoencoder_for_3d/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r1pdq </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r1pdq/p_compression_ratio_with_deep_autoencoder_for_3d/"/><updated> 2023-05-24T23:39:01+00:00</updated><published> 2023-05-24T23:39:01+00:00</published><title> [P] Compression ratio with deep autoencoder for 3d images</title></entry><entry><author><name> /u/herbiebradley</name><uri> https://www.reddit.com/user/herbiebradley </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi all,&lt;/p>; &lt;p>;We at CarperAI have developed a new technique called &lt;a href=&quot;https://carper.ai/quality-diversity-through-ai-feedback/&quot;>;Quality-Diversity with AI Feedback (QDAIF)&lt;/a>;, combining large language models and evolutionary algorithms to generate diverse and high-quality natural language text.&lt;/p>; &lt;p>;QDAIF is all using LMs to provide quality and diversity evaluations, which we use as feedback to optimize a search process which explores the space of text generations from LMs.&lt;/p>; &lt;p>;We use the evolutionary algorithm &lt;a href=&quot;https://arxiv.org/abs/1504.04909&quot;>;MAP-Elites&lt;/a>;, in which a grid defined by our diversity dimensions is populated with increasingly high quality texts generated by our LM evolution operator.&lt;/p>; &lt;p>;QDAIF can improve on some of the limitations of current QD algorithms which often require hand-coded measures of diversity &amp;amp; quality, and can help generate fine-tuning data to help a model improve. We think this highlights the potential to build powerful search algorithms through LM feedback that can explore and refine diverse solutions to nuanced qualitative problems.&lt;/p>; &lt;p>;Blog post: &lt;a href=&quot;https://carper.ai/quality-diversity-through-ai-feedback/&quot;>;https://carper.ai/quality-diversity-through-ai-feedback/&lt;/a>;&lt;/p>; &lt;p>;This was a collaboration with &lt;a href=&quot;https://www.aleph-alpha.com/&quot;>;Aleph Alpha&lt;/a>;, &lt;a href=&quot;https://twitter.com/jennyzhangzt&quot;>;Jenny Zhang&lt;/a>;, &lt;a href=&quot;https://twitter.com/jeffclune&quot;>;Jeff Clune&lt;/a>;, and &lt;a href=&quot;https://twitter.com/kenneth0stanley&quot;>;Ken Stanley&lt;/a>;!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/herbiebradley&quot;>; /u/herbiebradley &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r1j7a </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/"/><updated> 2023-05-24T23:31:32+00:00</updated><published> 2023-05-24T23:31:32+00:00</published><title> [P] Quality-Diversity with AI Feedback</title></entry><entry><author><name> /u/phoneix阿迪</name><uri>https://www.reddit.com/user/phoneixAdi </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://www.wisdominanutshell.academy/state-of-gpt/&quot;>;https://www.wisdominanutshell.academy/state-of-gpt/&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/phoneixAdi&quot;>; /u/phoneixAdi &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qutu9/n_state_of_gpt_summarized_notes_from_andrej/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qutu9/n_state_of_gpt_summarized_notes_from_andrej/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qutu9 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qutu9/n_state_of_gpt_summarized_notes_from_andrej/"/><updated> 2023-05-24T19:17:06+00:00</updated><published> 2023-05-24T19:17:06+00:00</published><title> [N] &quot;State of GPT&quot; - Summarized notes from Andrej Karpathy&#39;s talk from yesterday.</title></entry><entry><author><name> /u/-Rizhiy-</name><uri> https://www.reddit.com/user/-Rizhiy- </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;I want to train a generative model to generate some items. These items need to follow some known conditions/rules to be valid. How can I best incorporate these conditions/rules into the generative model, such that generated objects are valid?&lt;/p>; &lt;p>;So far I&amp;#39;ve seen multiple approaches:&lt;/p>; &lt;ol>; &lt;li>;Just re-sample until a valid item is generated. This can seriously increase amount of compute required. Plus, this might bias generated items to a subset which is more likely to be valid.&lt;/li>; &lt;li>;Parametrise generated items, such that they are always valid. eg if there is a condition that &lt;code>;A &amp;gt; B&lt;/code>;, we can first generate &lt;code>;B&lt;/code>; and then generate &lt;code>;A&lt;/code>; using something like &lt;code>;A = B * (1 + exp(a))&lt;/code>; where &lt;code>;a&lt;/code>; is the actual generated value. While this solves the problem of having to generate multiple times, this requires definition of parametrised relations, which can be non-trivial and a pain to maintain with changing conditions.&lt;/li>; &lt;li>;Clip values to boundaries according to conditions. This is a bit simpler than parametrisation, but seems like it will produce worse results. Also, ill-posed for categorical values and conditions.&lt;/li>; &lt;/ol>; &lt;p>;Does anyone have experience with problem like that? Any papers/blog posts that discuss this? Perhaps an easier approach?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/-Rizhiy-&quot;>; /u/-Rizhiy- &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qr11q/d_sampling_items_with_restrictions/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qr11q/d_sampling_items_with_restrictions/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qr11q </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qr11q/d_sampling_items_with_restrictions/"/><updated> 2023-05-24T16:55:50+00:00</updated><published> 2023-05-24T16:55:50+00:00</published><title> [D] Sampling items with restrictions</title></entry><entry><author><name> /u/That_one_coder</name><uri> https://www.reddit.com/user/That_one_coder </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Fsg-Pp downloads images and uses two machine learning models to facilitate the process of changing your profile picture. The first model is a classifier, which decides whether the picture is suitable as a profile picture or not. The second model is an object detection model, for detecting the face and centering the crop on the detection.&lt;/p>; &lt;p>;&lt;a href=&quot;https://github.com/EngMarchG/Fsg-Pp&quot;>;EngMarchG/Fsg-Pp: Fsg-Pp downloads and classifies pictures that are suitable as profile pictures. It also automatically detects the faces and crops it for you! (github.com)&lt;/a>;&lt;/p>; &lt;p>;It took a little over a month of development and a lot of time, but we are very happy with the end product! We are also open for any suggestions you&amp;#39;d like to see (and within the scope of the project)&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/That_one_coder&quot;>; /u/That_one_coder &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qmiau/p_finally_some_good_profile_pictures_released_on/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qmiau/p_finally_some_good_profile_pictures_released_on/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qmiau </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qmiau/p_finally_some_good_profile_pictures_released_on/"/><updated> 2023-05-24T13:58:06+00:00</updated><published> 2023-05-24T13:58:06+00:00</published><title> [P] Finally some good profile pictures, released on github (Fsg-Pp) after a little over a month of development with my friend</title></entry></feed>