<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-26T08:18:35+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /u/I_will_delete_myself</name><uri> https://www.reddit.com/user/I_will_delete_myself </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我犹豫了一会儿，但听到这个消息后虚伪让我发疯。&lt;/p>; &lt;p>;SMH 这家公司就像白衣骑士一样，他们认为他们凌驾于所有人之上。他们想要监管，但他们希望不受该监管的影响。只想伤害其他人，而不是“全能的”Sam 和朋友。&lt;/p>; &lt;p>;他向国会直言不讳地谎称建议在欧盟采取类似措施，但现在开始抱怨他们。在任何政治领域都不应该认真对待这个家伙。&lt;/p>; &lt;p>;我的观点是，这家公司通过锁定与其品牌名称相悖的东西来反对 AI 进步。如果他们甚至不能忠于这样简单的事情，我们怎么能指望他们忠于更难的 AI 安全？&lt;/p>; &lt;p>;我很高兴他们现在改变了立场，但我很高兴他们如何他们认为他们有权为了自己的利益而腐败。 SMH!!!!!!!!&lt;/p>; &lt;p>;你有什么想法？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/I_will_delete_myself&quot;>; /u/I_will_delete_myself &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rie0e </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/"/><updated> 2023-05-25T13:51:58+00:00</updated><published> 2023-05-25T13:51:58+00:00</published><title> OpenAI 现在抱怨人工智能的监管 [D]</title></entry><entry><author><name> /u/theoneandonlypatriot</name><uri> https://www.reddit.com/user/theoneandonlypatriot </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我正在参加 SWE 工作的面试过程，有几个人直接评判我，甚至公然说他们不是 AI 的粉丝因为我在 AI / ML 工作方面的背景。&lt;/p>; &lt;p>;发这篇文章是为了让人们知道工程社区中存在这种观点和负面看法。&lt;/p>; &lt;p>;考虑到我也分享了很多东西，感觉很糟糕人工智能的伦理问题。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/theoneandonlypatriot&quot;>; /u/theoneandonlypatriot&lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s32d4 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/"/><updated> 2023-05-26T04:25:57+00:00</updated><published> 2023-05-26T04:25:57+00:00</published><title> [D] 对 AI 的负面评价</title></entry><entry><author><name>/u/天眼2006</name><uri> https://www.reddit.com/user/dayeye2006 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我以前是一名机器学习工程师。但我已经多年没有接触 Pytorch（我在自己的初创公司工作，作为一名全栈工程师）。&lt;/p>; &lt;p>;有哪些好的资源可以刷新我的 PyTorch 技能？&lt;/p>; &lt;p>;我喜欢以“愚蠢的方式”学习东西。我计划从头开始实现一些最经典的模型（ResNet、TextCNN、transformers，...）。&lt;/p>; &lt;p>;当我学习编程语言时，我最喜欢参考的资源是 &lt;a href=&quot;https://github.com/topics/koans&quot;>;公案&lt;/a>;。这有助于我快速熟悉新语言。深度学习界有对应的吗？&lt;/p>; &lt;p>;谢谢&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/dayeye2006&quot;>; /u/dayeye2006 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13royi6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13royi6/d_what_are_some_resources_to_brush_up_on_my/"/><updated> 2023-05-25T18:13:41+00:00</updated><published> 2023-05-25T18:13:41+00:00</published><title> [D] 有哪些资源可以提高我的 PyTorch 技能？</title></entry><entry><author><name> /u/奇异语2501</name><uri> https://www.reddit.com/user/Singularian2501 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>; &lt;img src=&quot;https://a.thumbs.redditmedia .com/pwokgMFTSRP9bRbBlTbOA1gPSTlPoECDQdwoNNPMuG0.jpg&quot; alt=&quot;[R] Gorilla：连接大量 API 的大型语言模型 - Microsoft Research 2023 - 在编写 API 调用方面超越 GPT-4 的性能。 title=&quot;[R] Gorilla：连接大量 API 的大型语言模型 - Microsoft Research 2023 - 在编写 API 调用方面超越 GPT-4 的性能。&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.15334 &quot;>;https://arxiv.org/abs/2305.15334&lt;/a>; &lt;/p>; &lt;p>;Github：&lt;a href=&quot;https://github.com/ShishirPatil/gorilla&quot;>;https://github。 com/ShishirPatil/gorilla&lt;/a>; &lt;/p>; &lt;p>;博客：&lt;a href=&quot;https://gorilla.cs.berkeley.edu/&quot;>;https://gorilla.cs.berkeley.edu/&lt; /a>; &lt;/p>; &lt;p>;摘要：&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 最近出现了令人印象深刻的进步浪潮，模型现在在各种任务中表现出色，例如数学推理和程序综合。然而，它们通过 API 调用有效使用工具的潜力仍未实现。即使对于当今最先进的 LLM（例如 GPT-4）而言，这也是一项具有挑战性的任务，这主要是因为它们无法生成准确的输入参数，并且它们倾向于产生错误的 API 调用用法。我们发布了 Gorilla，这是一种经过微调的基于 LLaMA 的模型，在编写 API 调用方面超越了 GPT-4 的性能。当与文档检索器结合使用时，Gorilla 展示了适应测试时文档更改的强大能力，支持灵活的用户更新或版本更改。 &lt;strong>;它还大大减轻了直接提示 LLM 时经常遇到的幻觉问题。&lt;/strong>;为了评估模型的能力，我们引入了 APIBench，这是一个由 HuggingFace、TorchHub 和 TensorHub API 组成的综合数据集。 &lt;strong>;检索系统与 Gorilla 的成功集成表明 LLM 有潜力更准确地使用工具，跟上经常更新的文档，从而提高其输出的可靠性和适用性。&lt;/strong>;&lt;/p>; &lt; /blockquote>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/n5ezjchbg12b1.jpg?width=872&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eb5b7e11a22abe59d49504fad7278006a2b878a6&quot;>;https://preview.redd。它/n5ezjchbg12b1.jpg?width=872&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eb5b7e11a22abe59d49504fad7278006a2b878a6&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/e2xhpfhbg12b1.jpg ?width=1075&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b3c0f6ed7a6d72c93e681266977a0ec0f129ba6d&quot;>;https://preview.redd.it/e2xhpfhbg12b1.jpg?width=1075&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b 3c0f6ed7a6d72c93e681266977a0ec0f129ba6d&lt;/a >;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/i7i7bfhbg12b1.jpg?width=1213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5a287aba81199b66d1334457c6e8a12b3b5881c0&quot;>;https://预览。 redd.it/i7i7bfhbg12b1.jpg?width=1213&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5a287aba81199b66d1334457c6e8a12b3b5881c0&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Singularian2501&quot;>; /u/Singularian2501 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13rl3v9 </id><media:thumbnail url="https://a.thumbs.redditmedia.com/pwokgMFTSRP9bRbBlTbOA1gPSTlPoECDQdwoNNPMuG0.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rl3v9/r_gorilla_large_language_model_connected_with/"/><updated> 2023-05-25T15:42:26+00:00</updated><published> 2023-05-25T15:42:26+00:00</published><title> [R] Gorilla: Large Language Model Connected with Massive APIs - Microsoft Research 2023 - 在编写 API 调用方面超越了 GPT-4 的性能。</title></entry><entry><author><name> /u/Ok_Bank_2217</name><uri> https://www.reddit.com/user/Ok_Bank_2217 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们需要为我们的平台获取大量 YouTube 数据并训练自定义 ML 模型，但除了 YouTube 之外找不到任何有用的东西8M Dataset，相当过时，信息非常有限。官方的 YouTube 数据 API 也被限制在大约 10.000 个积分，这远远不能满足我们需要的数量。&lt;/p>; &lt;p>;这就是为什么我们说去他妈的，并决定自己构建一个巨大的 YouTube 数据集。在为超过 1 亿个视频编制索引并构建自定义 API 来访问它之后，我们决定公开 API 并允许人们购买访问权限！&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.blizzy -data.com/&quot;>;链接到网站&lt;/a>;&lt;/p>; &lt;p>;我们很乐意听到我们的 ML 工程师和数据科学家的反馈，并希望解决您和我们遇到的问题!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Ok_Bank_2217&quot;>; /u/Ok_Bank_2217 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rh9yj </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/"/><updated> 2023-05-25T13:04:43+00:00</updated><published> 2023-05-25T13:04:43+00:00</published><title> [P] 我们创建了一个大型 YouTube 视频数据集来替换 YouTube 数据 API</title></entry><entry><author><name> /u/我是布兰妮</name><uri>https://www.reddit.com/user/ISpearedBritney </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你的很多工作都涉及 AI 或 ML（无论标题如何），你能分享一下你的典型工作日是怎样的吗？您将时间花在什么上，最终经常使用哪些工具或资源？其中有多少是数据争论，你使用了多少数学？谢谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ISpearedBritney&quot;>; /u/ISpearedBritney &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rct07 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/"/><updated> 2023-05-25T09:21:06+00:00</updated><published> 2023-05-25T09:21:06+00:00</published><title> [D] 对于那些从事 ML/AI 工作的人来说，你的工作和工作日是什么样的？</title></entry><entry><author><name> /u/arg_max</name><uri> https://www.reddit.com/user/arg_max </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨，&lt;/p>; &lt;p>;我为我的 neurips 提交深入研究了扩散器，发现了一些我认为有点奇怪但不#39;真的没有人可以与之讨论，所以我想我只是把它贴在这里，看看是否有人知道发生了什么，这是否是一个众所周知的现象。&lt;/p >; &lt;p>;所以调节稳定扩散。你有一个提示，类似于“狗的图像”。该提示通过 Clip 模型编码为条件矩阵，该矩阵通过交叉注意力输入 U-Net。此剪辑编码包括一个标记器，它将提示拆分为标记及其连续表示。这个分词器还包括一个“句子开头”放在每个标记化序列开头的标记（以及重复的“句子结尾”标记，直到达到最大标记数，对于稳定扩散为 77）。在交叉注意层中，然后将作为当前潜在 z_t 的 U-Net 编码版本的视觉特征投影到查询矩阵 Q 中。条件（即剪辑编码提示）被转换为键和值矩阵K 和 V。然后乘以 Q * K^T 并在行上取 softmax 以获得注意力概率矩阵。该矩阵中的每一行对应一个视觉特征，每一列对应文本条件中的一个标记。由于 softmax，行总和为 1，对于每个空间位置，您可以在标记上进行分布。基本上，它告诉提示中的一个标记/单词对某个空间位置的影响有多大。现在，我希望所有权重都集中在提示中的重要标记上（例如“狗”），但我发现平均而言，90-99% 的概率质量被放入“句子开头” ;令牌。然后这也意味着对应于“句子开头”的值矩阵中的条目是“句子的开始”。 token会支配交叉注意力层的输出，不管你写什么提示。对我来说，这很奇怪，显然，这不是手工编码的，而是学习的，因此优化发现 XA 层的输出具有较小的可变性，而是始终接近对应于“&amp;quot;”的值矩阵条目。句首”令牌在某种程度上是最好的。此外，这种行为在不同的时间步都是相同的，所以它发生在扩散过程的开始和结束时。&lt;/p>; &lt;p>;也许其他人经历过类似的事情或者知道这里发生了什么？&lt; /p>; &lt;p>;TLDR：稳定扩散中的注意力概率集中在 90-99% 的句子标记的一般开头，而不是来自实际提示的标记，与扩散时间步长、交叉注意力头或 U 无关-网络层。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/arg_max&quot;>; /u/arg_max &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_thinks_this_behavior/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_that_thinks_this_behavior/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rwh1c </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rwh1c/d_am_i_the_only_one_that_thinks_this_behavior/"/><updated> 2023-05-25T23:10:42+00:00</updated><published> 2023-05-25T23:10:42+00:00</published><title> [D] 只有我认为这种行为（交叉注意力层）很奇怪吗？</title></entry><entry><author><name> /u/奇异语2501</name><uri> https://www.reddit.com/user/Singularian2501 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/huTwcaqVaNrBZzikekNSL9XJi3tfEGZsggLCg50LMYM.jpg&quot; alt=&quot;[R] 用语言模型推理就是用世界模型进行规划 - Shibo Hao 等加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，计划相对改进了 33%世代设定！” title=&quot;[R] 用语言模型推理就是用世界模型进行规划 - Shibo Hao 等加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，在计划生成设置中相对改进了 33%！” />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;论文：&lt;a href=&quot;https://arxiv.org/abs/2305.14992 &quot;>;https://arxiv.org/abs/2305.14992&lt;/a>; &lt;/p>; &lt;p>;摘要：&lt;/p>; &lt;blockquote>; &lt;p>;大型语言模型 (LLM) 已显示出非凡的推理能力，尤其是当提示生成中间推理步骤（例如，Chain-of-Thought，CoT）。然而，LLM 仍然难以解决对人类来说很容易的问题，例如为在给定环境中执行任务生成行动计划，或者执行复杂的数学、逻辑和常识推理。缺陷源于一个关键事实，即 LLM 缺乏内部&lt;em>;世界模型&lt;/em>;来预测世界&lt;em>;状态&lt;/em>;（例如，环境状态、中间变量值）并模拟长期结果动作。这可以防止 LLM 执行类似于人脑的深思熟虑的计划，这涉及探索替代推理路径、预测未来状态和奖励，以及迭代改进现有推理步骤。为了克服这些限制，我们提出了一个新的 LLM 推理框架，&lt;strong>;&lt;em>;R&lt;/em>;&lt;/strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;easoning via&lt;/em>; /strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;P&lt;/em>;&lt;/strong>;&lt;strong>;––&lt;/strong>;&lt;strong>;&lt;em>;规划&lt;/em>;&lt;/strong >; &lt;strong>;(RAP).&lt;/strong>; RAP 将 &lt;strong>;LLM 重新定位为世界模型和推理代理，并结合原则性规划算法（基于蒙托卡罗树搜索）在广阔的推理中进行战略探索&lt;/strong>; 在推理过程中，LLM（作为代理）在LLM（作为世界模型）和任务特定奖励的指导下增量构建推理树，并在适当的平衡下高效地获得高奖励推理路径在探索 &lt;em>; 与 &lt;/em>; 开发之间。我们将 RAP 应用于各种具有挑战性的推理问题，包括计划生成、数学推理和逻辑推理。这些任务的实证结果证明了 RAP 优于各种强大的基线，包括 CoT 和自洽性从最少到最多的提示。 &lt;strong>;LLAMA-33B 上的 RAP 超过了 GPT-4 上的 CoT，计划生成设置相对改进了 33%。&lt;/strong>;&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;a href=&quot;https://preview .redd.it/jaoiil2mc12b1.jpg?width=747&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53&quot;>;https://preview.redd.it/jaoiil2mc12b1.jpg?width=747&amp;amp;format= pjpg&amp;auto= webp&amp;amp;s=28086d47e9c9ba38fdda8afbd9f15464bfb07a53&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7389d75d0ff7 d1d8787c7c5f9add4787b02b47be &quot;>;https://preview.redd.it/pq9c0o2mc12b1.jpg?width=1356&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7389d75d0ff7d1d8787c7c5f9add4787b02b47be&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https:/ /preview.redd.it/ykpqvp2mc12b1.jpg?width=980&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=834c39fb3e549418b8396725e86ddff3c6584077&quot;>;https://preview.redd.it/ykpqvp2mc12b1.jpg?width=9 80&amp;amp;格式=pjpg&amp;amp; auto=webp&amp;amp;s=834c39fb3e549418b8396725e86ddff3c6584077&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s =9c5192d6c012bfe4390fa67b010580b8e4508daa&quot;>;https://preview.redd.it/zlqb8q2mc12b1.jpg?width=1294&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9c5192d6c012bfe4390fa67b010580 b8e4508daa&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https //preview.redd.it/qd8pjo2mc12b1.jpg?width=1400&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea&quot;>;https://preview.redd.it/qd8pjo2mc12b1.jpg?width=140 0&amp;格式= pjpg&amp;amp;auto=webp&amp;amp;s=1017764e376aa1a8bfa9fd03eef22fb455bd7bea&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Singularian2501&quot;>; /u/Singularian2501 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13rkhzx </id><media:thumbnail url="https://b.thumbs.redditmedia.com/huTwcaqVaNrBZzikekNSL9XJi3tfEGZsggLCg50LMYM.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rkhzx/r_reasoning_with_language_model_is_planning_with/"/><updated> 2023-05-25T15:17:59+00:00</updated><published> 2023-05-25T15:17:59+00:00</published><title> [R] 使用语言模型进行推理就是使用世界模型进行规划 - Shibo Hao 等人，加州大学圣地亚哥分校 - LLAMA-33B 上的 RAP 超过 GPT-4 上的 CoT，计划生成设置相对改进了 33%！</title></entry><entry><author><name> /你/阿杜纳托</name><uri>https://www.reddit.com/user/adunato </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;最近，我一直在处理几个使用 PyTorch 的 GitHub 项目。对于每个项目，我维护一个单独的 Conda 环境（我通过艰难的方式了解到为什么这很重要）。&lt;/p>; &lt;p>;但是，我遇到的一个持续存在的问题涉及 PyTorch 与我的 CUDA 的兼容性版本。具体来说，通过 requirements.txt 文件安装的 PyTorch 版本通常与我的 CUDA 版本不兼容，导致无法识别 CUDA 设备。&lt;/p>; &lt;p>;为了解决这个问题，我采用了一种做法我从 requirements.txt 文件中删除了对 PyTorch（以及相关库，如 torchvision、torchaudio）的任何提及，并从官方 PyTorch 站点手动安装它。&lt;/p>; &lt;p>;这是一种常见做法吗？或者我错过了一个更简化的工作流程来确保 PyTorch 和 CUDA 的兼容性？我很想听听其他人是如何处理这个问题的。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/adunato&quot;>; /u/adunato &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s6x3b </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s6x3b/d_best_practices_for_installing_pytorch_to_align/"/><updated> 2023-05-26T08:02:39+00:00</updated><published> 2023-05-26T08:02:39+00:00</published><title> [D] 安装 PyTorch 以与特定 CUDA 版本保持一致的最佳实践</title></entry><entry><author><name>/u/钢铁侠马克20</name><uri> https://www.reddit.com/user/IronManMark20 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/IronManMark20&quot;>; /u/IronManMark20 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://gorilla.cs.berkeley. edu/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rpvgn/gorilla_large_language_model_connected_with/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rpvgn </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rpvgn/gorilla_large_language_model_connected_with/"/><updated> 2023-05-25T18:49:35+00:00</updated><published> 2023-05-25T18:49:35+00:00</published><title> Gorilla：连接大量 API 的大型语言模型</title></entry><entry><author><name>/u/让-波特</name><uri>https://www.reddit.com/user/Jean-Porte </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 法尔se 模仿专有 LLM 的承诺&quot; title=&quot;[R] 模仿的虚假承诺专有法学硕士&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Jean-Porte&quot;>; /u/Jean-Porte &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv. org/abs/2305.15717&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13s6pb7 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/"/><updated> 2023-05-26T07:49:29+00:00</updated><published> 2023-05-26T07:49:29+00:00</published><title> [R] 模仿专有 LLM 的虚假承诺</title></entry><entry><author><name>/u/内部-Industry758</name><uri> https://www.reddit.com/user/Internal-Industry758 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你在没有在顶级会议上发表任何论文的情况下完成了博士学位，你现在在做什么？你仍然觉得博士学位值得吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Internal-Industry758&quot;>; /u/Internal-Industry758 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rm0uf </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rm0uf/d_phds_without_tiptier_publications_what_are_you/"/><updated> 2023-05-25T16:18:09+00:00</updated><published> 2023-05-25T16:18:09+00:00</published><title> [D] 没有顶级出版物的博士：你现在在做什么？</title></entry><entry><author><name> /u/恩里科希波尔</name><uri>https://www.reddit.com/user/EnricoShippole </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;很高兴发布 FLAN V2 数据集的开源复制品。&lt;/p>; &lt;p>;可以在此处找到完整的数据集：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/FLAN_2022&quot;>;https://huggingface.co/datasets/conceptofmind/FLAN_2022&lt;/a>;&lt;/p>; &lt;p>;我和主要作者 Shayne Longpre 一起工作FLAN 合集重现他的伟大作品并公开发布高质量的指令调优数据。我们修复了编码问题并将序列长度增加到 4096：&lt;a href=&quot;https://twitter.com/EnricoShippole/status/1661756166248996867?s=20&quot;>;https://twitter.com/EnricoShippole/status/1661756166248996867 ?s=20&lt;/a>;&lt;/p>; &lt;p>;每个单独的子混音也可以在 huggingface 上下载。子混音是 T0、FLAN2021、CoT、NIv2 和 Dialog。每个都包含相关的元数据，例如输入、目标、任务源、任务名称和模板类型。&lt;/p>; &lt;p>;T0 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/t0_submix_original&quot;>; https://huggingface.co/datasets/conceptofmind/t0_submix_original&lt;/a>;&lt;/p>; &lt;p>;Flan2021 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/flan2021_submix_original&quot;>;https:/ /huggingface.co/datasets/conceptofmind/flan2021_submix_original&lt;/a>;&lt;/p>; &lt;p>;CoT 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/cot_submix_original&quot;>;https://huggingface. co/datasets/conceptofmind/cot_submix_original&lt;/a>;&lt;/p>; &lt;p>;NIv2 子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/niv2_submix_original&quot;>;https://huggingface.co/datasets /conceptofmind/niv2_submix_original&lt;/a>;&lt;/p>; &lt;p>;对话子混合：&lt;a href=&quot;https://huggingface.co/datasets/conceptofmind/dialog_submix_original&quot;>;https://huggingface.co/datasets/conceptofmind/ dialog_submix_original&lt;/a>;&lt;/p>; &lt;p>;您可以在这里找到原始的 FLAN 存储库和 Shayne Longpre 的所有令人难以置信的工作：&lt;a href=&quot;https://github.com/google-research/FLAN /tree/main/flan/v2#download&quot;>;https://github.com/google-research/FLAN/tree/main/flan/v2#download&lt;/a>;&lt;/p>; &lt;p>;一定要也通读 Shayne 关于 FLAN 集合的论文，以更好地了解数据是如何创建的：&lt;a href=&quot;https://arxiv.org/abs/2301.13688&quot;>;https://arxiv.org/abs /2301.13688&lt;/a>;&lt;/p>; &lt;p>;我们将很快发布一个包含数百 GB 高质量指令数据的大规模因果语言建模数据集。在不久的将来留意该版本。&lt;/p>; &lt;p>;我们在 FLAN V2 和相关项目的开放复制方面的工作都归功于 CarperAI 和 StabilityAI 的慷慨赞助。&lt;/p>; &lt;p>;你可以在此处了解有关 CarperAI 的更多信息：&lt;a href=&quot;https://carper.ai/&quot;>;https://carper.ai/&lt;/a>;&lt;/p>; &lt;p>;在此处了解 StabilityAI：&lt;a href=&quot; https://stability.ai/&quot;>;https://stability.ai/&lt;/a>;&lt;/p>; &lt;p>;非常感谢 Jason Phang 和 Fabrizio Milo 帮助构建数据集。&lt;/p >; &lt;p>;你可以在这里找到 Jason Phang 的推特：&lt;a href=&quot;https://twitter.com/zhansheng&quot;>;https://twitter.com/zhansheng&lt;/​​a>;&lt;/p>; &lt; p>;Fabrizio Milo 在这里：&lt;a href=&quot;https://twitter.com/fabmilo&quot;>;https://twitter.com/fabmilo&lt;/a>;&lt;/p>; &lt;p>;您可以查看在此处查看 Shayne 关于构建预训练数据集的新论文：&lt;a href=&quot;https://github.com/shayne-longpre/a-pretrainers-guide/blob/main/A%20Pretrainer&amp;#x27;s %20Guide%20To%20Training%20Data.pdf&quot;>;https://github.com/shayne-longpre/a-pretrainers-guide/blob/main/A%20Pretrainer&amp;#39;s%20Guide%20To%20Training%20Data。 pdf&lt;/a>;&lt;/p>; &lt;p>;这不是 Google 或 StabilityAI 的官方产品。&lt;/p>; &lt;p>;如果您对数据有任何疑问，请务必联系我们并询问！我会尽量及时回复：&lt;a href=&quot;https://twitter.com/EnricoShippole&quot;>;https://twitter.com/EnricoShippole&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON - ->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/EnricoShippole&quot;>; /u/EnricoShippole &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rmqx5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rmqx5/p_opensource_reproduction_of_the_flan_v2_dataset/"/><updated> 2023-05-25T16:46:42+00:00</updated><published> 2023-05-25T16:46:42+00:00</published><title> [P] FLAN V2 数据集的开源再现</title></entry><entry><author><name>/u/Shot-Button-9010</name><uri> https://www.reddit.com/user/Shot-Button-9010 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我以为 NeurIPS 有，但我在网站上看到的只有提交截止日期和通知日期。 NeurIPS 通常会跳过反驳吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Shot-Button-9010&quot;>; /u/Shot-Button-9010 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13rry8o/d_does_neurips_2023_have_rebuttal_phase/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rry8o/d_does_neurips_2023_have_rebuttal_phase/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rry8o </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rry8o/d_does_neurips_2023_have_rebuttal_phase/"/><updated> 2023-05-25T20:11:05+00:00</updated><published> 2023-05-25T20:11:05+00:00</published><title> [D] NeurIPS 2023 是否有反驳阶段？</title></entry><entry><author><name> /u/行星</name><uri>https://www.reddit.com/user/planetoryd </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;独裁政权（例如中国）一直在使用盲水印，以简单和隐写的方式，通过嵌入隐藏信息来迫害举报人/发起人在应用程序界面中。我不是专家，但我认为待办事项是：&lt;/p>; &lt;ul>; &lt;li>;用于局部盲水印去除的高效 ML 模型（或者，是否适合 ML） 去除（半）可见/盲水印，同时保留视觉/语义内容。&lt;/li>; &lt;li>;它的加速推理引擎，例如在 Rust 中。&lt;/li>; &lt;li>;开源移动和桌面应用程序界面。 （可能集成到现有的 EXIF 去除器工作流程中）&lt;/li>; &lt;/ul>; &lt;p>;现有方法包括拍照而不是屏幕截图。 （屏幕摄像头攻击）它可能不那么安全。 &lt;a href=&quot;https://ieeexplore.ieee.org/document/9136707&quot;>;paper1&lt;/a>; &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S1047320323000871&quot; >;paper2&lt;/a>;&lt;/p>; &lt;p>;它经常在中国异议 Reddit 社区中被提及。 （搜索 &lt;code>;reddit 盲水印&lt;/code>;）该技术也可能被出口。中国已经在与伊朗合作开发防火墙。我们需要做好准备。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/planetoryd&quot;>; /u/planetoryd &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rj2wp/d_a_call_to_implement_a_blind_watermark_removal/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rj2wp/d_a_call_to_implement_a_blind_watermark_removal/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rj2wp </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rj2wp/d_a_call_to_implement_a_blind_watermark_removal/"/><updated> 2023-05-25T14:20:08+00:00</updated><published> 2023-05-25T14:20:08+00:00</published><title> [D] 呼吁实施盲水印去除应用程序以捍卫公民自由。</title></entry><entry><author><name> /u/铁叶神经元</name><uri>https://www.reddit.com/user/tiedyeneuron </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;诚然，我在标题问题的措辞上略显幼稚和片面，以引发讨论。我认为从事深度学习研究的学术实验室有一定的优势。然而，许多重大突破现在似乎确实发生在工业实验室，而不是小型大学实验室。这可能是由于 DL 从新兴研究领域成熟为工业技术。&lt;/p>; &lt;p>;鉴于 DL 的最新发展，人们对在工业中进行深度学习研究的相对优点有何看法与学术界？例如，如果有人可以选择在顶级学术实验室（如麻省理工学院、斯坦福大学、加州大学伯克利分校等）担任研究员或加入 OpenAI/Anthropic/DeepMind 等，他们为什么要选择学术道路？&lt;/p >; &lt;p>;我理解有些人可能出于成为教授的愿望而选择学术界，但似乎越来越多的顶尖大学乐于让行业研究人员担任客座教授或担任兼职教授。许多行业科学家也接受实习生，因此他们仍然可以充当导师，就像他们是学术实验室的 PI 一样。仍然，显然留在 AI 学术界仍然有一些独特的价值，因为我能想到许多选择这样做的顶尖研究人员。我很想知道人们认为与行业实验室相比有什么好处。&lt;/p>; &lt;p>;（我知道这是一个与职业相关的帖子，但它看起来不像 &lt;a href=&quot;https:// www.reddit.com/r/cscareerquestions/&quot;>;r/cscareerquestions&lt;/a>; 拥有合适的受众或专业知识来推动这一讨论。此外，我认为目前这一讨论非常针对跨行业/学术界的 ML 社区及时。）&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/tiedyeneuron&quot;>; /u/tiedyeneuron &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rl1q6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rl1q6/d_given_the_scaling_up_of_deep_learning_methods/"/><updated> 2023-05-25T15:40:05+00:00</updated><published> 2023-05-25T15:40:05+00:00</published><title> [D] 鉴于深度学习方法的扩展，作为 AI 研究人员留在学术界的剩余优点是什么？</title></entry><entry><author><name> /u/panthsdger</name><uri> https://www.reddit.com/user/panthsdger </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;全文：&lt;a href=&quot;https://arxiv.org/abs/2305.11252v1&quot;>;https://arxiv.org/abs /2305.11252v1&lt;/a>;&lt;/p>; &lt;p>;人工神经网络 (ANN) 已成为机器学习的重要工具，在图像和语音生成、游戏和机器人技术等多个领域取得了显著成功。然而，人工神经网络之间存在根本差异。操作机制和生物大脑的机制，特别是关于学习过程。本文全面回顾了人工神经网络中当前受大脑启发的学习表征。我们研究整合更多生物学上合理的机制，例如突触可塑性，以增强这些网络的功能。能力。此外，我们深入研究了伴随这种方法的潜在优势和挑战。最终，我们为这个快速发展的领域的未来研究指出了有前途的途径，这可以使我们更接近理解智能的本质。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/panthsdger&quot;>; /u/panthsdger &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rinw8 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/"/><updated> 2023-05-25T14:02:59+00:00</updated><published> 2023-05-25T14:02:59+00:00</published><title> [r] 人工神经网络中的类脑学习：综述</title></entry><entry><author><name>/你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/transforming-youtube-shorts-google-deepminds-flamingo-reinvents -metadata-for-maximum-impact-f817e1141dde&quot;>;https://medium.com/@tiago-mesquita/transforming-youtube-shorts-google-deepminds-flamingo-reinvents-metadata-for-maximum-impact-f817e1141dde&lt;/ a>;&lt;br/>; ‍&lt;br/>; Google 的人工智能研究部门 DeepMind 最近与 Google Brain 合并，组成了一个专注于推进人工智能技术的强大团队。&lt;/p>; &lt;p>;他们的最新项目 Flamingo 是一种视觉语言模型 (VLM)，它被用于通过生成自动和准确的视频描述来提高 YouTube Shorts 的可发现性。&lt;/p>; &lt;p>;YouTube 短片创作者通常优先考虑快速制作而不是创建有用的标题，而 Flamingo 旨在解决&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rl49e/n_google_deepminds_flamingo_is_focusing_on/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rl49e/n_google_deepminds_flamingo_is_focusing_on/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rl49e </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rl49e/n_google_deepminds_flamingo_is_focusing_on/"/><updated> 2023-05-25T15:42:54+00:00</updated><published> 2023-05-25T15:42:54+00:00</published><title> [N] Google DeepMind 的 Flamingo 专注于改进 YouTube 短片的描述以提高可发现性</title></entry><entry><author><name>/你/米勒</name><uri>https://www.reddit.com/user/mierle </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;QLoRA: Effi量化 LLM 的 cient Finetuning&quot; title=&quot;QLoRA：量化 LLM 的高效微调&quot; />; &lt; /a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mierle&quot;>; /u/mierle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14314&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13r1hkg </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/"/><updated> 2023-05-24T23:29:47+00:00</updated><published> 2023-05-24T23:29:47+00:00</published><title> QLoRA：量化 LLM 的高效微调</title></entry><entry><author><name>/u/弗兰克米勒MC</name><uri> https://www.reddit.com/user/FrankMillerMC </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://huggingface.co/tiiuae&quot;>;https://huggingface.co/tiiuae&lt;/a>;&lt;/ p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/FrankMillerMC&quot;>; /u/FrankMillerMC &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rizo0/new_large_language_model_for_use_commercial/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rizo0/new_large_language_model_for_use_commercial/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rizo0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rizo0/new_large_language_model_for_use_commercial/"/><updated> 2023-05-25T14:16:29+00:00</updated><published> 2023-05-25T14:16:29+00:00</published><title>用于商业的新大型语言模型（开源）[N]</title></entry><entry><author><name> /u/_negativeonetwelfth</name><uri> https://www.reddit.com/user/_negativeonetwelfth </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;在从许多不同的来源阅读了这些算法的实现之后，我仍然看到了关于此的相互矛盾的信息。一些消息来源说（或暗示）你可以获得更高的帧率，因为你可以更少地运行深度学习对象检测器，并连续几帧使用卡尔曼滤波器预测框。另一方面，一些消息来源表明情况并非如此，因为过滤器仅用于根据先前位置预测当前（而非未来）位置，并且需要在每次迭代中使用深度学习检测进行更新。&lt; /p>; &lt;p>;我想知道是否有人对这些算法有经验并且能够提供一个真实而明确的答案。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/_negativeonetwelfth&quot;>; /u/_negativeonetwelfth &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rgy1g </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/"/><updated> 2023-05-25T12:50:26+00:00</updated><published> 2023-05-25T12:50:26+00:00</published><title> [D] 使用卡尔曼滤波器（如 SORT 和 DeepSORT）的跟踪算法是否会增加系统的帧率？</title></entry><entry><author><name> /u/altoidsjedi</name><uri> https://www.reddit.com/user/altoidsjedi </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我有兴趣了解人们为各种实验在 4 位量化中制作的最新模型——例如让它们在MacOS 上的 llama.cpp，例如 Chat-MLC。 &lt;/p>; &lt;p>;有谁知道是否可以将任何流行的 4 位量化 GGML 模型转换回保持 4 位量化的 PyTorch 模型？ &lt;/p>; &lt;p>;或者我是否只需要使用 Google Collab 或 SageMaker 之类的东西自己创建一个非 GGML 量化模型？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;# 32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/altoidsjedi&quot;>; /u/altoidsjedi &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rsijn/can_a_4bit_quantized_ggml_model_be_turned_back/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rsijn/can_a_4bit_quantized_ggml_model_be_turned_back/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rsijn </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rsijn/can_a_4bit_quantized_ggml_model_be_turned_back/"/><updated> 2023-05-25T20:33:24+00:00</updated><published> 2023-05-25T20:33:24+00:00</published><title>能否在保持 4 位量化的同时将 4 位量化 GGML 模型转回 PyTorch .PT 模型？ [讨论]</title></entry><entry><author><name> /u/Tomatomakko</name><uri> https://www.reddit.com/user/Tomatomakko </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;向量神经元 [&lt;a href=&quot;https://arxiv.org/pdf/2104.12229.pdf&quot;>;https://arxiv.org/ pdf/2104.12229.pdf&lt;/a>;] 是一种在 3D 点云处理网络中实现旋转等方差的方法。是否可以将相同的想法转移到 2D CNN？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Tomatomakko&quot;>; /u/Tomatomakko &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rfd15 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/"/><updated> 2023-05-25T11:36:16+00:00</updated><published> 2023-05-25T11:36:16+00:00</published><title> [D] 可以使用向量神经元在 2D CNN 中实现旋转等方差吗？</title></entry><entry><author><name> /u/双倍体</name><uri>https://www.reddit.com/user/Gaploid </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/SESB3xwR0W4B6ja9sC1v5aZbQ7rHU_rvGpMiqXUef5g.jpg&quot; alt=&quot;[P] 使用 GPT-4 自动从数据仪表板中提取见解&quot; title=&quot;[P] 使用 GPT-4 自动从数据仪表板中提取见解&quot; />; &lt;/a>; &lt; /td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;我们刚刚推出了一个新的 GPT-4 驱动的我们的数据分析平台的功能，想征求社区的意见。 &lt;/p>; &lt;p>;&lt;a href=&quot;https://i.redd.it/cb151k919z1b1.gif&quot;>;https://i.redd.it/cb151k919z1b1.gif&lt;/a>;&lt;/p>; &lt;p >;借助新功能，现在用户只需单击一下即可获得图表或仪表板上显示的数据的简单而全面的解释。 ChatGPT 无需任何特殊提示即可根据特定领域的知识生成适用的见解、解释甚至建议。这是可能的，因为我们开发了一种从图表中提取数据并将其以柱状格式传递到引擎盖下的提示的机制。这允许系统理解图表的上下文并使用深入分析所需的原始数据。&lt;/p>; &lt;p>;同时与您分享我们在开发新功能时发现的一些发现可能是对其他人有用：&lt;/p>; &lt;ol>; &lt;li>;提示的措辞至关重要；问题越具体，答案往往越准确。 &lt;strong>;所需语言、答案的最大长度和数据解释&lt;/strong>;等清晰的规范有助于改善结果。角色扮演或模拟专家也可以指导模型在特定知识领域内提供更详细的响应。&lt;/li>; &lt;li>;ChatGPT 擅长解析和&lt;strong>;处理表格数据&lt;/strong>;，包括 CSV。由于其紧凑性、准确性和可读性，我们选择这种格式将原始数据传输到模型。该模型甚至可以概念化来自此类表格的数据可以使用不同图表类型表示的方式，并可以使用这些可视化来解释数据。&lt;/li>; &lt;li>;值得注意的是，ChatGPT 似乎在挣扎具有&lt;strong>;大值和小数&lt;/strong>;的小数点。为了克服这个问题，我们将数字四舍五入到最多 2-3 位小数。这种做法不仅提高了准确性，而且减少了使用的令牌数量。&lt;/li>; &lt;/ol>; &lt;p>;我们想邀请大家试用我们的新功能并与我们分享您的想法。请点击&lt;a href=&quot;https://double.cloud/services/doublecloud-visualization/&quot;>;此链接访问&lt;/a>;我们的免费试用版——无需信用卡或 ChatGPT API 密钥。&lt;/p>; &lt;p >;我们很想听听您对我们开发的产品的看法。我们非常欢迎任何改进建议或潜在用途示例。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Gaploid&quot;>; /u/Gaploid &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13rhgt5 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/SESB3xwR0W4B6ja9sC1v5aZbQ7rHU_rvGpMiqXUef5g.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rhgt5/p_using_gpt4_to_automatically_extract_insights/"/><updated> 2023-05-25T13:12:40+00:00</updated><published> 2023-05-25T13:12:40+00:00</published><title> [P] 使用 GPT-4 自动从数据仪表板中提取见解</title></entry><entry><author><name>/u/威廉弗林奇博</name><uri>https://www.reddit.com/user/WilliamFlinchbaugh </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我正在尝试使用常规 bart-large 预训练模型进行文本摘要。我的代码非常适合 Pegasus，但是当我切换到 BARTForConditionalGeneration 时，它会生成来自其他语言的随机符号和字符。这真的很奇怪，我还没有找到任何修复它的方法。输入数据不会导致这种情况。我真的无法在网上找到任何信息。&lt;/p>; &lt;p>;此外，我对数据进行了一些预处理以确保文本块的长度低于 1024 个标记，所以这不应该导致&lt;/p>; &lt;p>;生成摘要的代码：&lt;/p>; &lt;pre>;&lt;code>;model_name = &quot;facebook/bart-large&quot;; tokenizer = BartTokenizer.from_pretrained(model_name) model = BartForConditionalGeneration.from_pretrained(model_name) chunk = &quot;*在此处输入文本*&quot;; tokenized = tokenizer(chunk, truncation=True, padding=“longest”, return_tensors=“pt”, max_length=tokenizer.max_len_single_sentence)[&#39;input_ids&#39;] generated = model.generate(tokenized, max_length= 256) decoded = tokenizer.decode(generated.squeeze(), skip_special_tokens=True) &lt;/code>;&lt;/pre>; &lt;p>;我的一个输出看起来像这样：&lt;/p>; &lt;pre>;&lt;code>;nihc # 981 -40-48。 ------------------------------------------ ---------- dob �︎︎━━━┻━━━━━━━━╣━━━ﻺ━━⻺╣╣┻────────━━━ ╢ ━━═━━ - ─━━═━━━━△ớ┻╣໛╣⻄⻄╣资源┺╢╣═━╕╣┻━────────╣────━──────━━╗╣─━╔╣㻚──╣մ╣══╣░╛━╚ ╢┻ ┻╕_╟╣↓╛╔┻К &lt;/code>;&lt;/pre>; &lt;p>;如果有人能提供帮助，我将不胜感激！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/WilliamFlinchbaugh&quot;>; /u/WilliamFlinchbaugh &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rqxur/p_bart_giving_random_characters_as_output/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rqxur/p_bart_giving_random_characters_as_output/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rqxur </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rqxur/p_bart_giving_random_characters_as_output/"/><updated> 2023-05-25T19:31:46+00:00</updated><published> 2023-05-25T19:31:46+00:00</published><title> [P] Bart 给出随机字符作为输出</title></entry></feed>