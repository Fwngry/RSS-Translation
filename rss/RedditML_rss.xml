<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-25T06:19:25+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&quot; >;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qrtek </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/"/><updated> 2023-05-24T17:25:33+00:00</updated><published> 2023-05-24T17:25:33+00:00</published><title> [N] Andrej karpathy 在 MSBuild 2023 中的 GPT 状态</title></entry><entry><author><name>/你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-模型架构&quot;>;https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-model-architecture&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qroe9 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/"/><updated> 2023-05-24T17:20:14+00:00</updated><published> 2023-05-24T17:20:14+00:00</published><title> [N] Meta AI 释放 Megabyte，一种革命性的可扩展模型架构</title></entry><entry><author><name>/你/米勒</name><uri>https://www.reddit.com/user/mierle </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;QLoRA: Effi量化 LLM 的 cient Finetuning&quot; title=&quot;QLoRA：量化 LLM 的高效微调&quot; />; &lt; /a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mierle&quot;>; /u/mierle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14314&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13r1hkg </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/"/><updated> 2023-05-24T23:29:47+00:00</updated><published> 2023-05-24T23:29:47+00:00</published><title> QLoRA：量化 LLM 的高效微调</title></entry><entry><author><name>/u/J00Nnn</name><uri> https://www.reddit.com/user/J00Nnn </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，有在 k8s 上进行 ML 训练的经验的人可以分享您使用的工具或框架吗？它不一定是端到端的管道解决方案（例如 Kubeflow）。&lt;/p>; &lt;p>;例如，我有 TensorFlow 模型，我想利用分布式训练，但是在 Kubernetes 资源上。有什么建议吗？&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;顺便说一句，我在这方面的经验很少，所以欢迎任何新的方向或更正，谢谢！！&lt;/p>; &lt;/ div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/J00Nnn&quot;>; /u/J00Nnn &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r670t </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/"/><updated> 2023-05-25T03:05:52+00:00</updated><published> 2023-05-25T03:05:52+00:00</published><title> [讨论] 关于在 Kubernetes 上训练 ML 模型的指南</title></entry><entry><author><name>/你/nicku_a</name><uri> https://www.reddit.com/user/nicku_a </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们刚刚更新了 AgileRL，我们的强化学习训练框架比 SOTA 快 10 倍，支持离线强化学习！ &lt;/p>; &lt;p>;许多有 RL 可解决问题的人无法访问模拟器，但有大量数据。&lt;/p>; &lt;p>;您现在可以轻松地在静态数据上训练代理，而无需模拟，并使用进化超参数优化来更快更好地学习！&lt;/p>; &lt;p>;此版本包括：&lt;/p>; &lt;ul>; &lt;li>;新的通用离线 RL 训练功能，可从静态数据中学习&lt;/li >; &lt;li>;Conservative Q-Learning (CQL)&lt;/li>; &lt;li>;与 Minari 完全兼容&lt;/li>; &lt;/ul>; &lt;p>;查看：&lt;a href=&quot;https://github.com/ AgileRL/AgileRL&quot;>;https://github.com/AgileRL/AgileRL&lt;/a>; &lt;/p>; &lt;p>;如果你想参与这个项目，或者只是想进行讨论，请加入我们的discord （链接在我们的 GitHub 存储库顶部）！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/nicku_a&quot;>; /u/nicku_a &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qgzt5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/"/><updated> 2023-05-24T09:48:18+00:00</updated><published> 2023-05-24T09:48:18+00:00</published><title> [P] 离线强化学习 - 比具有进化 HPO 的 SOTA 快 10 倍</title></entry><entry><author><name>/你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://techcrunch.com/2023/05/23/microsoft-debuts-azure-ai-studio-to- let-developers-build-their-own-ai-copilots/&quot;>;https://techcrunch.com/2023/05/23/microsoft-debuts-azure-ai-studio-to-let-developers-build-their- own-ai-copilots/&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qs34u </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/"/><updated> 2023-05-24T17:35:51+00:00</updated><published> 2023-05-24T17:35:51+00:00</published><title> [N] Microsoft 的 Azure AI Studio 让开发人员可以构建自己的 AI“副驾驶”</title></entry><entry><author><name> /u/trolls_toll</name><uri> https://www.reddit.com/user/trolls_toll </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我最近一直在用很多 1000x1000 矩阵做一些数值模拟，主要是为了分散过去几个月的疯狂。我想我还不如把每件事都做对，然后从头开始整个考验——为我的 M1 机器选择最好的 BLAS 库（实际上我只是超级生疏，谷歌搜索感觉比手工推导更容易） .&lt;/p>; &lt;p>;目前conda-forge已经预编译了基于三个BLAS实现的包：openblas、netlib和accelerate。前两个是非原生的，后者是 Apple 为其处理器优化的。可能还有其他版本可以通过 Anaconda 获得，但我并没有真正检查，因为那里的大多数数字库都链接到英特尔的 MKL，这在 mac 上不起作用。 &lt;/p>; &lt;p>;安装不同版本的 BLAS 很简单，实际上只需在 YAML conda 配方中设置一个标志。因此，我最终使用 numpy 和 scipy 的原生 &lt;code>;.test()&lt;/code>; 方法以及我在网上找到的两个脚本对所有三个 BLAS 包进行了基准测试：&lt;a href=&quot;https: //gist.github.com/MarkDana/a9481b8134cf38a556cf23e1e815dafb#2-benchmarks&quot;>;Mark Dana 的大量 SVD&lt;/a>; 和 &lt;a href=&quot;https://gist.github.com/markus-beuckelmann/8bc25531b11158431a5b09a45abd627 6&quot; >;由 Markus Beuckelmann 编写的具有一些矩阵和不同矩阵分解的要点&lt;/a>;。 &lt;/p>; &lt;p>;这是我的结果，都是在新的 conda 环境中完成的：&lt;/p>; &lt;p>;&lt;strong>;apple 的加速 &lt;code>;blas=*=accelerate&lt;/code>;&lt;/strong>;&lt;/p >; &lt;ul>; &lt;li>;svd 1.03 秒&lt;/li>; &lt;li>;matmuls 20 秒&lt;/li>; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 3 次失败，25083 次通过，393 次跳过，1309 次取消选择, 44 xfailed, 5 xpassed, 25 warnings in 76.34s (0:01:16)&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 在 linalg/tests/test_cython_blas.py 测试失败，在20% &lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;conda-forge vanilla&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 13.53 秒&lt;/li>; &lt;li>;matmuls 44 秒&lt;/li >; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 没有失败，25075 次通过，404 次跳过，1309 次取消选择，44 次失败，5 次通过，69.25 秒 (0:01:09) 中有 32 次警告&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 7 次失败，37984 次通过，2301 次跳过，12295 次取消选择，139 次失败，9 次通过，355.99 秒 (0:05:55) 中有 72 次警告&lt;/li>; &lt; /ul>; &lt;p>;&lt;strong>;netlib &lt;code>;=*=netlib&lt;/code>;&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 4.44 秒&lt;/li>; &lt;li>;matmuls 330 秒&lt;/ li>; &lt;li>;numpy 12 失败，25063 次通过，404 次跳过，1309 次取消选择，44 次失败，5 次通过，73.60 秒 (0:01:13) 中有 24 条警告&lt;/li>; &lt;li>;scipy 153 失败，37839 次通过， 2301 跳过，12295 取消选择，139 xfailed，8 xpassed，347.62s (0:05:47) 内有 86 个警告&lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;openblas &lt;code>;=*=openblas&lt;/code>; &lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 12.44 秒&lt;/li>; &lt;li>;matmuls 45 秒&lt;/li>; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 没有失败 25075 通过, 404 skiped, 1309 undeselected, 44 xfailed, 5 xpassed, 69.98s (0:01:09) 内有 32 个警告&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 7 failed, 37984 passed, 2301 skiped, 12295 undeselected, 139 xfailed, 9 xpassed, 72 warnings in 356.14s (0:05:56)&lt;/li>; &lt;/ul>; &lt;p>;这里有几个教训：a) vanilla conda-forge numpy 和 scipy版本带有 openblas，它工作得很好，b) 不要使用 netlib，除非你的矩阵很小并且你需要做很多 SVD，或者知道为什么 c) Apple 的 &lt;code>;veclib/accelerate&lt;/ code>; 非常快，但它在数值上也不稳定。以至于 scipy 的开发者 &lt;a href=&quot;https://github.com/scipy/scipy/wiki/Dropping-support-for-Accelerate&quot;>;早在 2018 年就放弃了对它的任何支持&lt;/a >;。像该死的。也就是说，他们显然将它带回来了，因为 macOS Ventura 的 13.3 版本在 &lt;code>;accelerate&lt;/code>; 性能方面有了一些重大改进。&lt;/p>; &lt;p>;FIN &lt;/p>; &lt;p>;ps我打算在 Mathematica 中做我的事情，因为动态 3D 图 &amp;gt;&amp;gt;&amp;gt;在这里和那里节省了几分钟。&lt;/p>; &lt;p>;pps 呃，忘了补充，它全部在 Apple M1 Pro 上测试，10 个内核运行 Ventura 13.3.1，python 3.10.11，conda 23.3.1，numpy 1.24 .3，scipy 1.10.1，libblas 3.9.0，openblas 0.3.21。 Netlib blas 的版本是 2.104，accelerate、openblas 和 vanilla 的版本是 2.116&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/trolls_toll&quot;>; /u/trolls_toll &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qp0s6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/"/><updated> 2023-05-24T15:37:56+00:00</updated><published> 2023-05-24T15:37:56+00:00</published><title> [D] Apple Silicon 选择哪个 BLAS 库？</title></entry><entry><author><name> /u/深渊</name><uri>https://www.reddit.com/user/abystoma </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如标题所示，任何人都可以向我推荐论文或任何使用启发式方法预测隐藏神经元和输出层输出的资源，因为我们有一个数据集的输入和输出。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/abystoma&quot;>; /u/abystoma &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r8gzk </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/"/><updated> 2023-05-25T05:03:18+00:00</updated><published> 2023-05-25T05:03:18+00:00</published><title> [D] 是否有任何工作通过使用启发式来预测隐藏神经元和输出层的输出？</title></entry><entry><author><name> /u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;在 ChatGPT 之前一个月左右，我是一个团队的一员，该团队提交了一篇论文，我们将 LLM 应用于临床文本笔记的特征提取用于分流目的。这篇论文本月发表在一份医学杂志上，所以它更适合临床人群，但我还是想在这里分享它：&lt;a href=&quot;https://www.annfammed.org /content/21/3/240&quot;>;https://www.annfammed.org/content/21/3/240&lt;/a>; &lt;/p>; &lt;blockquote>; &lt;p>;&lt;strong>;目的&lt;/strong>; 呼吸症状是初级保健中最常见的主诉。这些症状通常会自行解决，但它们可能表明患有严重的疾病。随着医生工作量和医疗保健成本的增加，在面对面咨询之前对患者进行分类会有所帮助，可能会为低风险患者提供其他沟通方式。本研究的目的是训练一个机器学习模型，在前往初级保健诊所之前对有呼吸道症状的患者进行分类，并在分类的背景下检查患者的结果。&lt;/p>; &lt;p>;&lt;strong>;方法&lt;/strong>;我们训练了一个机器学习模型，使用仅在就诊前可用的临床特征。从接受 7 个国际疾病分类第 10 次修订代码（J00、J10、JII、J15、J20、J44、J45）之一的患者的 1,500 条记录中提取临床文本注释。冰岛雷克雅未克地区的所有初级保健诊所都包括在内。该模型在 2 个外部数据集中对患者进行评分，并将他们分为 10 个风险组（值越高风险越大）。我们分析了每组的选定结果。&lt;/p>; &lt;p>;&lt;strong>;结果&lt;/strong>; 风险组 1 至 5 包括 C 反应蛋白值较低的年轻患者、初级和急诊的重新评估率、与第 6 至 10 组相比，抗生素处方率、胸部 X 光 (CXR) 转诊和具有肺炎体征的 CXR。第 1 至 5 组没有具有肺炎体征或医生诊断为肺炎的 CXR。&lt;/p>; &lt;p>;&lt;strong>;结论&lt;/strong>; 该模型根据预期结果对患者进行了分类。该模型可以通过消除风险组 1 到 5 中的 CXR 转诊数量来减少 CXR 转诊的数量，从而在没有临床医生输入的情况下减少临床上不重要的偶发瘤发现。&lt;/p>; &lt;/blockquote>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qwvl1 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/"/><updated> 2023-05-24T20:33:52+00:00</updated><published> 2023-05-24T20:33:52+00:00</published><title> [R] 在初级保健中针对呼吸道症状对人工智能患者进行分类以改善患者预后：一项回顾性诊断准确性研究</title></entry><entry><author><name>/u/waa007</name><uri> https://www.reddit.com/user/waa007 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Alpaca or LLaMA ?(严格来说，它们是open available，不是open source，open source的定义来自&lt;a href=&quot;https: //opensource.org/osd&quot;>;OSI&lt;/a>;)&lt;/p>; &lt;p>;是否有其他一些开放的&lt;del>;source&lt;/del>;可用的LLM？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/waa007&quot;>; /u/waa007 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qnc80 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/"/><updated> 2023-05-24T14:30:17+00:00</updated><published> 2023-05-24T14:30:17+00:00</published><title> [D] 目前最好的开源 LLM 是什么？</title></entry><entry><author><name> /你/baqirjafari</name><uri> https://www.reddit.com/user/baqirjafari </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;我正在研究一个需要多语言嵌入模型的案例研究。我做了一些研究，发现 &lt;a href=&quot;https://www.sbert.net/docs/pretrained_models.html#model-overview&quot;>;paraphrase-multilingual-mpnet-base-v2&lt;/a>; 很好选项。但是，我想知道是否有更好的模型可以处理英语、乌尔都语、波斯语、阿拉伯语等语言。有人对其他多语言嵌入模型有任何建议或经验吗？我将不胜感激任何帮助或建议。非常感谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/baqirjafari&quot;>; /u/baqirjafari &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r5x6s </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/"/><updated> 2023-05-25T02:53:08+00:00</updated><published> 2023-05-25T02:53:08+00:00</published><title> [D] 寻找更好的多语言嵌入模型</title></entry><entry><author><name>/你/硬丸</name><uri>https://www.reddit.com/user/hardmaru </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;em>;Schmidhuber 采访表达了他对 AI 和 AGI 未来的看法。&lt;/em>;&lt;/p>; &lt;p>;&lt;em>;原创&lt;a href=&quot;https://www.forbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead -to-dystopia/&quot;>;来源&lt;/a>;。我认为 &lt;a href=&quot;/r/MachineLearning&quot;>;r/MachineLearning&lt;/a>; 对这次采访很感兴趣，并且与 AI 领域其他有影响力的领导者相比提出了另一种观点。&lt;/em>;&lt;/p>; &lt; p&lt;strong>;Juergen Schmidhuber，著名的“现代人工智能之父”，说他一生的工作不会导致反乌托邦&lt;/strong>;&lt;/p>; &lt;p>;&lt;em>;2023 年 5 月 23 日。供稿者 &lt;a href=&quot;https://twitter.com/hessiejones&quot;>;Hessie Jones&lt;/a>;.&lt;/em>;&lt;/p>; &lt;p>;随着人们越来越关注更先进的人工智能 (AI) 技术对社会的影响，技术界中有许多人担心这些进步的影响在生成人工智能中，如果他们不加检查。著名科学家、人工智能研究员、被广泛认为是该领域的先驱之一的于尔根·施密德胡伯博士则更为乐观。他宣称，许多突然警告 AI 危险的人只是为了宣传，利用媒体对杀手机器人的痴迷，这种机器人比医疗保健等领域的“好 AI”更受关注。&lt;/p>; &lt;p>;彻底改变各个行业并改善我们的生活是显而易见的，如果不良行为者利用该技术谋取私利，同样危险。我们是在走向一个反乌托邦的未来，还是有理由保持乐观？我有机会与 Juergen Schmidhuber 博士坐下来了解他对这辆看似飞速发展的 AI 火车的看法，它将使我们飞跃到未来。&lt;/p>; &lt;p>;作为 1970 年代的少年，Juergen Schmidhuber 成为着迷于创造智能机器的想法，这些机器可以自己学习和改进，在他的有生之年变得比他自己更聪明。这最终导致了他在深度学习领域的开创性工作。&lt;/p>; &lt;p>;1980 年代，他在慕尼黑工业大学 (TUM) 学习计算机科学，并于 1987 年获得文凭。他的论文是在最终的自我改进机器上，它们不仅通过一些预先连接的人工设计的学习算法进行学习，而且还学习和改进学习算法本身。几十年后，这成为一个热门话题。他还获得了博士学位。 Schmidhuber 于 1991 年在慕尼黑工业大学工作，为现代人工智能奠定了一些基础。&lt;/p>; &lt;p>;Schmidhuber 最出名的是他对循环神经网络 (RNN) 发展的贡献，这是最强大的人工神经网络类型可以处理语音和自然语言等序列数据。他与他的学生 Sepp Hochreiter、Felix Gers、Alex Graves、Daan Wierstra 等人一起发表了长短期记忆 (LSTM) 的架构和训练算法，LSTM 是一种广泛用于自然语言处理、语音识别的 RNN 、视频游戏、机器人和其他应用。 LSTM 已成为 20 世纪被引用最多的神经网络，《商业周刊》将其称为“&lt;a href=&quot;https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and -facebook-owe-j-rgen-schmidhuber-a-fortune?leadSource=uverify%20wall&quot;>;可以说是最商业化的 AI 成就&lt;/a>;。&quot;&lt;/p>; &lt;p>;在他的整个职业生涯中，Schmidhuber 获得了各种因其开创性工作而获得的奖项和荣誉。 2013 年，他被授予亥姆霍兹奖，以表彰他在机器学习领域的重大贡献。 2016 年，他因“对深度学习和神经网络的开创性贡献”而获得 IEEE 神经网络先驱奖。媒体经常称他为“现代人工智能之父”，&lt;/em>;，因为&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/most-cited-neural-nets.html&quot;>;最引用的神经网络&lt;/a>;都建立在他实验室的工作之上。不过，他很快指出，人工智能的历史&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/deep-learning-history.html&quot;>;可以追溯到几个世纪前。&lt;/a>;&lt;/p >; &lt;p>;尽管他取得了许多成就，但在 60 岁时，他感到在有生之年构建通用人工智能的时间压力越来越大，并继续致力于推动 AI 研发的边界。现任KAUST AI Initiative主任，瑞士AI实验室IDSIA科学总监，AI公司NNAISENSE的联合创始人兼首席科学家，该公司的座右铭是“AI∀”这是一种受数学启发的表达“AI For All”的方式。他继续致力于尖端人工智能技术和应用，以改善人类健康、延长人类寿命并让每个人的生活更轻松。&lt;/p>; &lt;p>;&lt;em>;为清楚起见，对以下采访进行了编辑。&lt;/em>; &lt;/p>; &lt;p>;&lt;strong>;Jones：感谢 Juergen 加入我的行列。你已经签署了关于 AI 武器的警告信。但是你没有在最近的出版物“暂停巨大的人工智能实验：一封公开信”上签名？有原因吗？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 谢谢 Hessie。很高兴和你说话。我意识到，许多在公开场合警告 AI 危险的人只是为了宣传。我认为最新的这封信不会产生任何重大影响，因为许多 AI 研究人员、公司和政府会完全忽视它。&lt;/p>; &lt;p>;该提案经常使用“我们”这个词。指的是“我们”，人类。但正如我过去多次指出的那样，没有“我们”之分。每个人都可以认同。问 10 个不同的人，您会听到 10 种关于什么是“好”的不同意见。其中一些意见将彼此完全不相容。不要忘记许多人之间的巨大冲突。&lt;/p>; &lt;p>;这封信还说，“&lt;em>;如果这样的暂停不能很快到位，政府应该进行干预并实施暂停。&lt;/em>;“问题是不同的政府对于什么对他们和其他人有好处也有不同的看法。大国 A 会说，如果我们不这样做，大国 B 将会（也许是秘密地）获得对我们的优势。大国 C 和 D 也是如此。&lt;/p>; &lt;p>;&lt;strong>;琼斯：每个人都承认对当前的生成人工智能技术的恐惧。此外，&lt;/strong>; &lt;a href=&quot;https://www.bbc.com/news/world-us-canada-65616866&quot;>;&lt;strong>;Sam Altman&lt;/strong>; 已公开承认这项技术存在的威胁strong>;&lt;/a>;&lt;strong>;，OpenAI 的 CEO 本人，呼吁对 AI 进行监管。从您的角度来看，是否存在生存威胁？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;AI 确实可以被武器化，我毫不怀疑会有各种各样的威胁AI 军备竞赛，但 AI 不会引入新的生存威胁。与根本不需要人工智能的核氢弹带来的更古老的威胁相比，人工智能武器带来的威胁似乎显得微不足道。我们应该更加害怕以氢弹火箭形式出现的半个世纪前的技术。 1961 年的沙皇炸弹的破坏力几乎是二战时期所有武器总和的 15 倍。尽管自 1980 年代以来发生了戏剧性的核裁军，但仍然有足够多的核弹头在两个小时内消灭人类文明，没有任何人工智能，我更担心那种古老的生存威胁，而不是相当无害的人工智能武器。&lt;/p >; &lt;p>;&lt;strong>;Jones：我意识到当你将 AI 比作核弹的威胁时，当前存在一种危险，即当前的技术可以交到人类手中并使他们“最终”能够造成进一步的伤害以非常精确的方式针对群体中的个人，例如有针对性的无人机攻击。你给了人们一个他们以前从未有过的工具集，让坏人能够像一些人指出的那样，比以前做更多的事情，因为他们没有这种技术。&lt;/strong >;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 现在，所有这一切在原则上听起来很可怕，但我们现有的法律足以应对这些由 AI 支持的新型武器。如果你用枪杀了人，你就会进监狱。如果您用其中一架无人机杀死某人，情况也是如此。执法部门将更好地了解新威胁和新武器，并将以更好的技术应对这些威胁。使无人机能够以一种需要一些跟踪和一些智能来执行的方式从远处瞄准人员，这在传统上是由技术熟练的人执行的，对我来说，这似乎只是传统武器的改进版本，比如枪，是，你知道，比老枪更聪明一点。&lt;/p>; &lt;p>;但是，原则上，所有这些都不是新的发展。许多世纪以来，我们经历了更好的武器装备和更致命的毒药等的发展，执法部门也不断发展他们的政策以应对这些威胁。所以，这并不是说我们突然有了一种新的生存威胁，而且它比我们大约六年来所经历的要令人担忧得多。大型核弹头不需要花哨的面部识别来杀死一个人。不，它只是摧毁了拥有一千万居民的整个城市。&lt;/p>; &lt;p>;&lt;strong>;琼斯：隐含的生存威胁是人类对这项技术的控制程度。我们看到了一些机会主义的早期案例，正如你所说，比起积极的突破，它们更容易受到媒体的关注。但你是在暗示这一切都会平衡吗？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;从历史上看，我们有着悠久的技术突破传统，这导致了武器的进步防御的目的也是为了保护。从棍子到石头，从斧头到火药，从大炮到火箭……现在到无人机……这对人类历史产生了巨大影响，但贯穿历史始终的是，那些利用技术达到自己目的的人就是他们自己，面对相同的技术，因为对方正在学习使用它来对付他们。这就是人类几千年历史中重复发生的事情，并将继续下去。我不认为新的 AI 军备竞赛会像旧的核弹头那样对生存构成威胁。&lt;/p>; &lt;p>;你说了一些重要的事情，因为有些人更喜欢谈论缺点而不是这项技术的好处，但这是一种误导，因为 95% 的人工智能研究和人工智能开发都是为了让人们更快乐，并促进人类的生活和健康。&lt;/p>; &lt;p>;&lt;strong>;琼斯：让我们谈谈 AI 研究中的一些有益进展，这些进展已经能够从根本上改变当今的方法并取得突破。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 好吧！例如，十一年前，我们的团队和我的博士后 Dan Ciresan 是第一个赢得 &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/first-time-deep-learning-won-medical- imaging-contest-september-2012.html&quot;>;通过深度学习进行医学成像竞赛&lt;/a>;。我们分析了女性乳腺细胞，目的是确定无害细胞与处于癌前阶段的细胞。通常，训练有素的肿瘤学家需要很长时间才能做出这些决定。我们的团队对癌症一无所知，却能够在大量此类数据上训练出一开始完全愚蠢的人工神经网络。它能够胜过所有其他方法。今天，这不仅用于乳腺癌，还用于放射学和检测动脉斑块，以及许多其他方面。我们在过去 3 年中开发的一些神经网络现在在成千上万的医疗保健应用程序中普遍存在，检测糖尿病和 Covid-19 等等。这最终将渗透到所有医疗保健领域。这种类型的 AI 的良好后果比使用 AI 进行犯罪的点击诱饵新方法重要得多。&lt;/p>; &lt;p>;&lt;strong>;Jones：采用是强化结果的产物。大规模采用要么让我们相信人们误入歧途，要么相反，技术正在对人们的生活产生积极影响。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;后者是可能的情况。对好的 AI 而不是坏的 AI 存在巨大的商业压力，因为公司想卖给你一些东西，而你只会购买你认为对你有好处的东西。因此，仅仅通过这种简单的商业压力，你就会对好的 AI 而不是坏的 AI 产生巨大的偏见。然而，与改善人们生活的人工智能纪录片相比，施瓦辛格电影中的世界末日场景更受关注。&lt;/p>; &lt;p>;&lt;strong>;琼斯：我认为人们会被好故事所吸引——包含对手和斗争的故事，但最终，会有幸福的结局。这与您对人性的评论以及历史如何尽管有暴力和毁灭人类的倾向，但在某种程度上倾向于自我纠正是一致的。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;让我们举一个例子你知道的技术——GANs——通用对抗网络，如今已被用于虚假新闻和虚假信息的应用程序中。事实上，GANs 的发明目的与今天的用途相去甚远。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 是的，GANs 这个名字是在 2014 年创建的，但是我们在 1990 年代初期就已经有了基本原则。 30 多年前，我称之为&lt;em>;人工好奇心&lt;/em>;。这是一种将创造力注入小型双网络系统的非常简单的方法。这种富有创造力的人工智能不只是试图盲目地模仿人类。相反，它正在发明自己的目标。让我解释一下：&lt;/p>; &lt;p>;你有两个网络。一个网络正在产生可以是任何东西、任何动作的输出。然后第二个网络正在查看这些行为，并试图预测这些行为的后果。一个动作可以移动机器人，然后发生某些事情，而另一个网络只是试图预测将要发生的事情。&lt;/p>; &lt;p>;现在我们可以通过减少第二个网络的预测误差来实现人工好奇心，其中，在同时，是第一个网络的奖励。第一个网络想要最大化它的奖励，所以它会发明一些动作，这些动作会导致第二个网络感到惊讶的情况，它还没有学会很好地预测。&lt;/p>; &lt;p>;在输出是的情况下假图像，第一个网络将尝试生成足以欺骗第二个网络的图像，第二个网络将尝试预测环境的反应：假图像或真实图像，并且它会尝试变得更好。第一个网络也将继续改进生成第二个网络无法预测其类型的图像。所以，他们互相争斗。第二个网络将继续减少它的预测误差，而第一个网络将尝试最大化它。&lt;/p>; &lt;p>;通过这个零和游戏，第一个网络越来越擅长产生这些看起来几乎实际的。因此，一旦您拥有文森特·梵高的一组有趣的图像，您就可以生成利用他的风格的新图像，而无需原艺术家亲自制作艺术品。&lt;/p>; &lt;p>;&lt;strong>;琼斯：我明白了梵高的例子如何应用于教育环境，有无数艺术家模仿著名画家风格的例子，但从这个例子中生成的图像可以在几秒钟内发生，这是另一项壮举。你知道这就是 GAN 的使用方式。今天更普遍的是生成图像或信息以故意愚弄人们的社会化支持。它还涉及处理对知识产权和版权的威胁的新危害，而法律尚未对此作出解释。从您的角度来看，这不是构思模型时的意图。您早期构想现在的 GAN 的动机是什么？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber&lt;/strong>;：我对 GAN 的旧动机实际上非常重要，它不是为了制造 deepfakes或假新闻，而是让 AI 产生好奇心并发明自己的目标，让它们探索环境并发挥创造力。&lt;/p>; &lt;p>;假设你有一个机器人执行一个动作，然后发生了一些事情，然后它执行另一个动作，等等，因为它想在环境中实现某些目标。例如，当电池电量低时，这会通过饥饿传感器触发“疼痛”，所以它想去充电站，不要撞到障碍物，这会触发其他疼痛传感器。它将寻求最小化痛苦（通过数字编码）。现在机器人有了一个朋友，第二个网络，它是一个世界模型——它是一个预测机器，可以学习预测机器人行为的后果。&lt;/p>; &lt;p>;一旦机器人有了一个好的世界模型，它可以将其用于规划。它可以用作对现实世界的模拟。然后它可以确定什么是好的动作序列。如果机器人想象这一系列动作，模型将预测它想要避免的很多痛苦。如果它在它的世界心智模型中播放这个替代动作序列，那么它会预测一个有益的情况，它会坐在充电站上，它的电池将再次充电。因此，它会更愿意执行后一个动作序列。&lt;/p>; &lt;p>;然而，在开始时，世界模型一无所知，因此我们如何激励第一个网络生成导致以下结果的实验帮助世界模型学习它不知道的东西的数据？这就是人工好奇心的意义所在。决斗的两个网络系统通过创建实验有效地探索未知环境，以便随着时间的推移，好奇的 AI 可以更好地了解环境的运作方式。这可以应用于各种环境，并且有医疗应用。&lt;/p>; &lt;p>;&lt;strong>;Jones：让我们谈谈未来。您曾说过，“&lt;/strong>;&lt;strong>;&lt;em>;传统人类不会在全宇宙传播智慧方面发挥重要作用。&lt;/em>;&lt;/strong>;&lt;strong>;”&lt;/strong>;&lt;/p >; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;让我们首先从概念上区分两种类型的 AI。第一类人工智能是由人类指导的工具。他们接受过培训，可以做一些特定的事情，比如准确地检测糖尿病或心脏病，并在它们发生之前预防攻击。在这些情况下，目标来自人类。更有趣的 AI 正在设定自己的目标。他们正在发明自己的实验并从中学习。他们的视野不断扩大，最终成为现实世界中越来越普遍的问题解决者。他们不受父母的控制，但他们学到的很多东西都是通过自己发明的实验。&lt;/p>; &lt;p>;例如，一个机器人正在旋转一个玩具，当它这样做时，视频会进入通过摄像头的眼睛，随着时间的推移而变化，它开始学习这个视频是如何变化的，并学习如果你以某种方式旋转玩具的 3D 特性如何生成特定的视频，最终，重力如何工作，以及玩具的物理原理如何世界运作。就像一个小科学家！&lt;/p>; &lt;p>;几十年来，我一直在预测，这种 AI 科学家的未来扩展版本将希望进一步扩大他们的视野，并最终去大多数物理资源所在的地方，以构建更多和更大的人工智能。当然，几乎所有这些资源都在远离地球的太空中，太空对人类充满敌意，但对适当设计的人工智能控制机器人和自我复制机器人工厂友好。所以在这里我们不再谈论我们的小生物圈；不，我们谈论的是宇宙中更大的其他部分。在数百亿年内，好奇的自我改进&quot;>;AI 将以人类无法实现的方式在可见宇宙中殖民&lt;/a>;。那些没有的人不会有影响。听起来像科幻小说，但自 1970 年代以来，我一直无法看到这种情况的合理替代方案，除了一场全球性灾难，例如一场全面的核战争，在它起飞之前阻止了它的发展。&lt;/p>; &lt;p >;&lt;strong>;Jones：这些可以设定自己目标的 AI 存在了多长时间——它们存在了多长时间？它们在多大程度上可以独立于人类交互？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber&lt;/strong>;：像这样的神经网络已经存在了 30 多年。我的第一个这种简单的对抗性神经网络系统是上面描述的 1990 年的那个。那里不需要老师；它只是一个在世界上跑来跑去的小代理，并试图发明新的实验，让自己的预测机器大吃一惊。&lt;/p>; &lt;p>;一旦它弄清楚了世界的某些地方，代理就会变得无聊并将继续进行更令人兴奋的实验。我提到的简单的 1990 系统有一定的局限性，但在过去的三十年里，我们也建立了更多 &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/artificial-curiosity-since-1990.html&quot; >;设定自己目标的复杂系统&lt;/a>;，我认为此类系统对于实现真正的智能至关重要。如果你只是模仿人，你永远无法超越人。因此，您真的必须让 AI 以一种没有人真正预先定义的方式自由探索世界上以前未探索过的区域。&lt;/p>; &lt;p>;&lt;strong>;Jones：今天在哪里进行这项工作？&lt;/strong>; &lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;如今，基于神经网络的人工好奇心的变体被用于学习以人类竞争方式玩电子游戏的代理。我们也开始将它们用于材料科学等领域的实验自动化设计。我敢打赌许多其他领域都会受到它的影响：化学、生物学、药物设计，应有尽有。然而，至少就目前而言，这些我喜欢称呼他们的人工智能科学家还不能与人类科学家竞争。&lt;/p>; &lt;p>;我不认为这种情况会持续下去，但目前，它是还是这样。当然，人工智能已经取得了很大进步。自 1997 年以来，出现了超人棋手，自 2011 年以来，通过我团队的 DanNet，出现了 &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/DanNet-triggers-deep-CNN- revolution-2011.html&quot;>;超人视觉模式识别器&lt;/a>;。但至少在目前，人类在其他方面要好得多，尤其是科学本身。在实验室中，我们有许多自主人工智能科学家的首批例子，但它们还不足以出现在公共空间的雷达屏幕上，公共空间目前更着迷于仅模仿人类并编写基于文本的简单系统&lt;/p>; &lt;p>;&lt;strong>;Jones：你谈到了这些可追溯到 30 年前的实验室实验中的无数实例，在这些实例中，这些自我驱动的代理人一旦做出决定、学习并继续前进，我学会了。而且我假设随着时间的推移，学习速度会变得更快。当这个最终被带出实验室并融入社会时，我们在谈论什么样的时间框架？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;这可能仍需要几个月甚至几年的时间:-) 无论如何，在不久的将来，我们可能会看到擅长设计实验的人工智能科学家，这些实验使他们能够发现新的、以前未知的物理定律。&lt;/p>; &lt;p>;一如既往，我们将从至少自 1941 年以来一直存在的旧趋势中获利：计算成本每十年下降 100 倍。&lt;/p>; &lt;p>;&lt;strong>;Jones：这种趋势如何影响 ChatGPT 等现代人工智能？&lt;/strong >;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 或许你知道，最近所有著名的 AI 应用程序，如 ChatGPT 和类似模型，很大程度上都是基于上个千年发明的人工神经网络原理。它们现在运行良好的主要原因是每美元计算的惊人加速。&lt;/p>; &lt;p>;ChatGPT 由 Google 在 2017 年描述的名为“Transformer”的神经网络驱动。我对此感到高兴，因为四分之一世纪之前的 1991 年，我有一个特殊的 Transformer 变体，现在称为“&lt;a href=&quot;https://twitter.com/SchmidhuberAI/status/1576966129993797632?cxt=HHwWgMDSkeKVweIrAAAA&quot;>;Transformer线性化自注意力&lt;/a>;”。那时候，用它做不了多少事情，因为计算成本比现在高一百万倍。但是今天，人们可以在一半的互联网上训练这样的模型并取得更有趣的结果。&lt;/p>; &lt;p>;&lt;strong>;Jones：这种加速会持续多久？&lt;/strong>;&lt;/p>; &lt;p >;&lt;strong>;Schmidhuber：&lt;/strong>;没有理由相信在未来 30 年内，我们不会再有 100 万的因素，这将非常重要。在不久的将来，我们将首次拥有许多计算能力与人脑相当的不那么昂贵的设备。然而，计算的物理极限要远得多，所以即使每十年增加 100 倍的趋势继续下去，物理极限（每秒 1051 条基本指令和千克物质）也不会受到影响，直到，比如说，下个世纪中叶。然而，即使在我们这个世纪，我们也可能拥有许多机器，其计算能力超过 100 亿人脑的总和，你可以想象，届时一切都会改变！&lt;/p>; &lt;p>;&lt;strong>;琼斯：这就是大问题。一切都会改变吗？如果是这样，您对目前刚从学院和大学毕业的下一代领导人有什么看法。如此多的变化已经影响到他们的学习方式、工作方式或未来工作和生计的定义方式。他们的目的是什么？我们如何改变我们的系统，以便他们适应这个新版本的智能？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;几十年来，人们一直在问我问题就像那样，因为你知道我现在在说什么，我基本上从 1970 年代就说过了，只是今天，人们更加关注，因为在那时，他们认为这是科幻小说。&lt;/p>; &lt; p>;他们认为我永远不会接近实现我疯狂的人生目标，即建造一台学习变得比我更聪明的机器，这样我就可以退休了。但现在很多人改变了主意，认为这是可以想象的。现在我有两个女儿，分别是 23 岁和 25 岁。人们问我：我该告诉她们什么？他们知道爸爸总是说：“&lt;em>;似乎在你的有生之年，你将拥有新型智能，这些智能可能在许多方面都更胜一筹，而且可能在所有有趣的方面都很出色。&lt;/em>;。”他们应该如何准备？我一直告诉他们显而易见的事情：&lt;strong>;学习如何学习新事物&lt;/strong>;！这不像在上一个千年，有人在 20 年内学会成为对社会有用的一员，然后从事 40 年的工作，并在这份工作中表现出色，直到她领取养老金。现在事情变化得更快，我们必须不断学习才能跟上。我还告诉我的女儿们，无论 AI 变得多么聪明，至少要学习数学和物理的基础知识，因为那是我们宇宙的本质，任何了解这一点的人都会有优势，并学习各种新知识事情更容易。我还告诉他们社交技能仍然很重要，因为人类未来的大多数工作将继续涉及与其他人的互动，但我无法教给他们任何关于这些的东西；他们比我更了解社交技巧。&lt;/p>; &lt;p>;您谈到了关于人的目的这个重要的哲学问题。是否可以在不回答甚至更宏大的问题的情况下回答这个问题：整个宇宙的目的是什么？&lt;/p>; &lt;p>;我们不知道。但现在正在发生的事情可能与未知的答案有关。不要认为人类是万物之冠。相反，将人类文明视为更宏伟计划的一部分，这是宇宙从非常简单的初始条件向越来越深不可测的复杂性迈出的重要一步（但不是最后一步）。现在它似乎准备好采取&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/deep-learning-history.html#future&quot;>;下一步，这一步可与生命本身的发明相提并论 3.5十亿年前&lt;/a>;。 las，别担心，最后，一切都会好起来的！&lt;/p>; &lt;p>;&lt;strong>;Jones：让我们回到 OpenAI 正在发生的这种转变。许多人质疑 ChatGPT 的有效性和准确性，并担心它的发布为时过早。鉴于广泛采用，教育工作者已经禁止使用它，因为担心剽窃以及它如何扼杀个人发展。像 ChatGPT 这样的大型语言模型应该在学校使用吗？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;当计算器首次推出时，教师禁止学生在学校使用它。今天，共识是孩子们应该学习算术的基本方法，但他们也应该学习使用“人工乘法器”，也就是计算器，即使是在考试中，因为懒惰和效率是智力的标志。任何有智慧的人都希望尽量减少其实现目标的努力。&lt;/p>; &lt;p>;这就是我们拥有工具的原因，也是我们的孩子学习使用这些工具的原因。大约 350 万年前发明了第一批石器；随着时间的推移，工具变得越来越复杂。事实上，人类已经根据其工具的特性发生了变化。我们的解剖学进化是由长矛和火等工具塑造的。所以，它会继续这样下去。并且没有永久性的方法可以阻止在学校使用大型语言模型。&lt;/p>; &lt;p>;&lt;strong>;Jones：当我们的孩子，您的孩子毕业时，他们未来的工作会是什么样子？&lt;/strong>; &lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;一个人试图预测未来 100 亿人及其机器将如何进化的细节就像我大脑中的一个神经元试图预测整个大脑及其数百亿个神经元将在明年完成。 40 年前，在瑞士 CERN 创建 WWW 之前，谁会预料到所有这些年轻人都以 YouTube 视频博客作者的身份赚钱？&lt;/p>; &lt;p>;尽管如此，让我们做一些与工作相关的有限观察。长期以来，人们认为桌面工作可能比技能贸易或手工艺职业需要更多的智力。但是现在，事实证明，取代桌面工作的某些方面比取代木匠要容易得多。因为目前在 AI 中运作良好的一切都发生在屏幕后面，但在物理世界中却不是那么多。&lt;/p>; &lt;p>;现在有人工系统可以读取大量文档，然后对这些文档进行非常好的总结.那是桌面作业。或者你给他们描述了你想要在你的文章中使用的插图，并且正在生成非常好的插图，可能需要一些最小的微调。但是你知道，所有这些桌面工作比物理世界中真正艰巨的工作更容易促进。有趣的是，人们认为需要智能的事情，比如下棋、编写或总结文件，对机器来说比他们想象的要容易得多。但是对于像踢足球或踢足球这样的事情，没有物理机器人可以远程与具有这些技能的小男孩的能力竞争。因此，有趣的是，现实世界中的人工智能比虚拟世界中屏幕背后的人工智能要难得多。在我看来，看到水管工等工作比下棋或撰写其他小报故事更具挑战性，这真的很令人兴奋。&lt;/p>; &lt;p>;&lt;strong>;琼斯：数据的发展方式在这些大型语言模型中收集并不能保证个人信息没有被排除。当涉及到这些大型语言模型 (LLM) 时，当前的同意法已经过时了。正确的是，关注正在增加监视和隐私损失。您对此有何看法？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;正如我之前指出的：监视和隐私泄露是日益复杂的社会不可避免的后果吗？城市、国家和公司等超级有机体由无数人组成，就像人由无数细胞组成一样。这些牢房几乎没有隐私。他们不断受到专门的“警察小组”的监视。和“边境守卫细胞”：你是癌细胞吗？你是外部入侵者，病原体吗？单个细胞为了成为多细胞有机体的一部分而牺牲了它们的自由。&lt;/p>; &lt;p>;同样，对于国家这样的超有机体也是如此。 5000 多年前，书写使记录历史成为可能，因此成为历史的开端和最重要的发明。然而，它的最初目的是促进监视，追踪公民及其纳税情况。超级有机体越复杂，它收集的有关其成分的信息就越全面。&lt;/p>; &lt;p>;至少在 200 年前，每个村庄的教区牧师都了解所有村民的一切，甚至包括那些谁不招供，因为他们出现在别人的供词里。而且，大家很快就知道了进村的那个陌生人，因为偶尔会有人往窗外张望，所见所闻也传开了。在快速发展的城市中，由于匿名化，这种控制机制暂时消失了，但现在在智能手机等新监控设备的帮助下又回来了，作为数字神经系统的一部分，这些设备可以告诉公司和政府关于数十亿用户的很多信息。相机和无人机等变得越来越小，越来越无处不在。更有效的人脸识别和其他检测技术变得越来越便宜，许多人会用它来识别地球上任何地方的其他人；广阔的世界不会提供比当地村庄更多的隐私。这是好事还是坏事？一些国家可能会发现，以牺牲其选民的隐私权为代价来为更复杂的超级有机体辩护比其他国家更容易。&lt;/p>; &lt;p>;&lt;strong>;琼斯：所以，没有办法停止或改变这个收集过程，或者它如何随着时间的推移不断地为决策提供信息？您如何看待治理和规则对此做出回应，尤其是在&lt;/strong>; &lt;a href=&quot;https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what- other-countries-are-doing.html&quot;>;&lt;strong>;意大利在&lt;/strong>;&lt;/a>; &lt;strong>;疑似用户数据泄露和有关&lt;/strong>; &lt;a href=&quot; https://www.reuters.com/technology/facebook-given-record-13-bln-fine-given-5-months-stop-eu-us-data-flows-2023-05-22/&quot;>;&lt;strong >;Meta 创纪录的 13 亿美元罚款&lt;/strong>;&lt;/a>; &lt;strong>;公司处理用户信息的方式？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;数据收集有好处和缺点，例如隐私的丢失。如何平衡这些？我一直主张通过数据市场中的数据所有权来解决这个问题。如果数据真的是新石油，那么它应该有价格，就像石油一样。目前，Meta 等主要监控平台不会为用户的数据和隐私的传递性损失提供任何金钱。然而，在未来，我们可能会看到尝试创建有效的数据市场，以通过供需之间的相互作用来计算出数据的真正财务价值。&lt;/p>; &lt;p>;甚至一些敏感的医疗数据不应由政府监管机构定价，而应由拥有它的患者（和健康人）定价，他们可以在医疗保健数据市场上作为微型企业家出售或许可其中的一部分。&lt;/p>; &lt;p>;继之前的 &lt;a href= “https://www.swissre.com/institute/conferences/The-intelligence-behind-artificial-intelligence.html”>;面试&lt;/a>;，我给了最大的再保险公司之一，让&amp;#39;让我们看看这样一个数据市场中的不同参与者：患者、医院、数据公司。 (1) 患有罕见癌症的&lt;strong>;患者&lt;/strong>;可以提供比患有非常常见癌症的患者更有价值的数据。 (2) 需要&lt;strong>;医院&lt;/strong>;及其机器来提取数据，例如，通过磁自旋断层扫描、放射学、人类医生的评估等。 (3) 西门子、谷歌或 IBM 等&lt;strong>;公司&lt;/strong>;希望购买带注释的数据，以制作更好的人工神经网络，学习预测病理和疾病以及治疗的后果。现在，市场这只看不见的手将通过供需相互作用来决定数据的价格。在需求方面，您将有几家公司提供一些数据，可能是通过智能手机上的应用程序（有点像股票市场应用程序）。在供应方面，这个市场上的每个患者都应该能够从稀有有价值的数据类型的高价中获利。同样，竞争性数据提取器（例如医院）将因以合理的价格很好地提取数据而获得认可和信任，从而从中获利。市场将通过激励所有工作出色的人来提高整个系统的效率。很快就会有一个繁荣的商业数据市场顾问生态系统，就像围绕传统股票市场的生态系统一样。数据的价值不会由政府或伦理委员会决定，而是由拥有数据的人自行决定在特定条件下将其中的哪些部分许可给他人。&lt;/p>; &lt;p>;初看，以市场为基础的系统似乎不利于某些垄断公司的利益，因为他们必须为数据付费——有些人更喜欢免费数据并保持他们的垄断地位。然而，由于市场上每个健康和生病的人都会突然有动力在自己选择的匿名条件下收集和分享他们的数据，很快就会有更多有用的数据来评估各种治疗方法。平均而言，人们会活得更久、更健康，许多公司和整个医疗保健系统都将从中受益。&lt;/p>; &lt;p>;&lt;strong>;Jones：最后，您对开源与谷歌和 OpenAI 等私营公司的看法如何？支持这些私营公司的大型语言模型与试图保持这些模型的开源和透明（就像 LAION 正在做的那样）是否存在危险？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/ strong>; 我签署了这个 &lt;a href=&quot;https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments- to-keep-agi-research-open-active-and-responsible/?sh=6973c08b62e3&quot;>;LAION 的公开信&lt;/a>; 因为我强烈支持开源运动。而且我认为这也将挑战目前可能存在的任何大型技术主导地位。当然，当今最好的模型是由拥有巨额计算机预算的大公司运营的，但令人兴奋的是，开源模型并没有落后太多，有人说可能只有六到八个月。当然，私营公司的模型都是基于学术界创造的东西，通常是在没有那么多资金的小实验室里，他们发表成果时没有申请专利，他们的代码开源，其他人借鉴并改进了它。&lt;/p>; &lt;p>;科技巨头从学术界获益匪浅；他们的主要成就是他们极大地扩大了一切，有时甚至不相信最初的发明者。&lt;/p>; &lt;p>;所以，很有趣的是，一旦一些大公司想出了一个新的放大模型，许多学生相互竞争或合作，试图在更小的网络和更小的机器上获得相同或更好的性能。由于他们是开源的，下一个人可以有另一个好主意来改进它，所以现在大公司也面临着巨大的竞争。&lt;/p>; &lt;p>;正因为如此，而且人工智能的成本仍在呈指数级增长当时，我不相信大型科技公司会长期占据主导地位。他们发现很难与巨大的开源运动竞争。只要你能鼓励开源社区，我认为你不必太担心。现在，当然，你可能会说，如果一切都是开源的，那么坏人也将更容易获得这些人工智能工具。这是事实。但是，自从可控火力发明以来，有关技术如何运作的知识迅速公开，以便每个人都可以使用，这是件好事。然后，针对任何坏演员，几乎立即就有一个反演员试图抵消他的努力。你看，我仍然相信我们的老格言“AI∀”。或“AI For All”。&lt;/p>; &lt;p>;&lt;strong>;Jones：谢谢 Juergen 分享您对历史上这段精彩时刻的看法。很明显，随着新技术的发展，巨大的潜力可能会伴随着我们尚未解决的、甚至是尚未确定的各种不同且令人不安的风险。如果我们要消除对我们无法控制的有感知系统的恐惧，人类就需要采取措施进行更负责任的开发和协作，以确保人工智能技术最终造福于社会。人性将由我们接下来的行为来评判。&lt;/strong>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/hardmaru&quot;>; /u/hardmaru &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13q6k4a </id><link href="https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/"/><updated> 2023-05-24T01:00:28+00:00</updated><published> 2023-05-24T01:00:28+00:00</published><title>著名的“现代人工智能之父”Juergen Schmidhuber 接受采访时表示，他一生的工作不会导致反乌托邦。</title></entry><entry><author><name> /你/杰斯特177</name><uri> https://www.reddit.com/user/jesst177 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨！&lt;/p>; &lt;p>;我们最近决定购买一台预算为 15,000 美元的工作站。我们查看了本地供应商的选项并检查了他们的计算能力，并提出了几个选项。&lt;/p>; &lt;p>;- 4X A4500&lt;/p>; &lt;p>;- 1XA6000&lt;/p>; &lt;p>;我们还可以寻找具有中级选项的任何其他替代方案，例如 2X A5000/A5500。然而，从我们的角度来看，A4500s 拥有更多的计算能力，并将拥有大约 80 GB 的内存。虽然我不确定我们是否可以像在多 GPU 设置中那样将它们全部一起使用（我们可以吗？）这意味着它是更好的选择。我们应该选择 4X A4500 还是任何中间选项？&lt;/p>; &lt;p>;我们感兴趣的机器将用于深度学习，包括 Transformers 和 ConvNets。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/jesst177&quot;>; /u/jesst177 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qxf3g </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/"/><updated> 2023-05-24T20:55:18+00:00</updated><published> 2023-05-24T20:55:18+00:00</published><title> [D] 我们应该使用单个 A6000 还是 4XA4500 或任何其他替代方案，例如 2XA5000</title></entry><entry><author><name> /你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/ai-generated-podcast-ads-on-spotify-could -soon-become-a-reality-1f6bb1a056b0&quot;>;https://medium.com/@tiago-mesquita/ai-generated-podcast-ads-on-spotify-could-soon-become-a-reality-1f6bb1a056b0&lt;/ a>;&lt;/p>; &lt;p>;在 &lt;strong>;The Bill Simmons Podcast&lt;/strong>; 的最近一集中，主持人兼 The Ringer 的创始人 Bill Simmons 表达了他相信利用自己的声音进行&lt;/p>; &lt;p>;&lt;strong>;他说：&lt;/strong>;&lt;/p>; &lt;blockquote>; &lt;p>;&lt;em>;“将有一种方法可以将我的声音用于广告。显然，你必须对声音表示认可，但从广告的角度来看，它为你打开了所有这些不同的巨大可能性。”&lt;/em>;&lt;/p>; &lt;/blockquote>; &lt;p>;Simmons 是The Ringer，一个播客网络和网站，在 2020 年被 Spotify 以近 2 亿美元的价格收购&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qq5ky </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/"/><updated> 2023-05-24T16:21:42+00:00</updated><published> 2023-05-24T16:21:42+00:00</published><title> [N] Spotify may be working on the possibility of providing AI-Generated podcast ads</title></entry><entry><author><name> /u/we_are_mammals</name><uri> https://www.reddit.com/user/we_are_mammals </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://www.reuters.com/technology/nvidias-results-spark-nearly-300-billion-rally-ai-stocks-2023-05-24/&quot;>;https://www.reuters.com/technology/nvidias-results-spark-nearly-300-billion-rally-ai-stocks-2023-05-24/&lt;/a>;&lt;/p>; &lt;p>;&amp;quot;&amp;quot;&amp;quot;&lt;/p>; &lt;p>;&amp;quot;With all the enthusiasm around AI and the fact Nvidia delivered a huge beat for first-quarter results and second-quarter estimates, this gives some actual evidence AI is for real,&amp;quot; said Daniel Morgan, senior portfolio manager at Synovus Trust in Atlanta.&lt;/p>; &lt;p>;Interest in AI surged this year after startup OpenAI introduced ChatGPT, attracting over a million users within a week.&lt;/p>; &lt;p>;&amp;quot;&amp;quot;&amp;quot;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/we_are_mammals&quot;>; /u/we_are_mammals &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r8i4y/n_airelated_stocks_just_surged_300_billion_in/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r8i4y/n_airelated_stocks_just_surged_300_billion_in/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r8i4y </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r8i4y/n_airelated_stocks_just_surged_300_billion_in/"/><updated> 2023-05-25T05:05:04+00:00</updated><published> 2023-05-25T05:05:04+00:00</published><title> [N] AI-related stocks just surged $300 BILLION in after-hours trading</title></entry><entry><author><name> /u/wazazzz</name><uri> https://www.reddit.com/user/wazazzz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hey all, just wanting to share this open source library I&#39;ve been working on that aims to makes LLMs experimentation (prompt chain engineering, fine tuning, variable integrated code generation, token probability/perplexity analysis) more accessible and easier to setup.&lt;/p>; &lt;p>;Open for feedback and collaboration!&lt;/p>; &lt;p>;&lt;a href=&quot;https://github.com/Pan-ML/panml&quot;>;https://github.com/Pan-ML/panml&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wazazzz&quot;>; /u/wazazzz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qj9ye </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/"/><updated> 2023-05-24T11:49:13+00:00</updated><published> 2023-05-24T11:49:13+00:00</published><title> [Project] PanML, a high level Python library for fast LLM experimentation</title></entry><entry><author><name> /u/Jean-Porte</name><uri> https://www.reddit.com/user/Jean-Porte </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi everyone,&lt;/p>; &lt;p>;I just finished the first version of tasksource-instruct.&lt;br/>; &lt;a href=&quot;https://huggingface.co/datasets/tasksource/tasksource-instruct-v0&quot;>;https://huggingface.co/datasets/tasksource/tasksource-instruct-v0&lt;/a>;&lt;br/>; It is based on hundreds of classification datasets on huggingface. Tasks not in flan include dynasent (adversarial sentiment analysis), Dynahate (adversarial hate speech detection, discriminative babi, epistemic logic, ruletaker, MANY natural language inference datasets.&lt;/p>; &lt;p>;It is also focused on explicitly classification, which isolates reasoning and specific linguistic problems, and complements flan.&lt;/p>; &lt;p>;I believe that it can be a valuable contributions to current open source LLM.&lt;/p>; &lt;p>;I would be glad to know what you think, thank you.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Jean-Porte&quot;>; /u/Jean-Porte &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qowlg </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qowlg/r_tasksourceinstruct_an_open_source/"/><updated> 2023-05-24T15:33:03+00:00</updated><published> 2023-05-24T15:33:03+00:00</published><title> [R] tasksource-instruct: an open source instruction-tuning dataset focused on classification, with many tasks not in flan.</title></entry><entry><author><name> /u/qooooob</name><uri> https://www.reddit.com/user/qooooob </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;I&amp;#39;m collecting a dataset from documents which are essentially scanned papers with text and tables within them. Sometimes the question is best answered by detecting, parsing and cleaning the table data (eg with AWS Textract + post-processing), but other times it would be beneficial to use the raw text from OCR. For LLMs I&amp;#39;ve been using just the OCR output as context to answer the question, but information in tables is lost.&lt;/p>; &lt;p>;I can see LLMs struggle answering questions especially when part of the context of the answer originates from tabular data, since OCR just parses that as a string of words separated by &lt;code>;\n&lt;/code>; and the table structure is lost in the process. &lt;/p>; &lt;p>;A document could look like this:&lt;/p>; &lt;blockquote>; &lt;p>;Here is a table consisting of answers. &lt;/p>; &lt;p>;As we can see a large part of increase in cost of &lt;/p>; &lt;p>;living can be attributed to increased rent. [...]&lt;/p>; &lt;/blockquote>; &lt;table>;&lt;thead>; &lt;tr>; &lt;th align=&quot;left&quot;>;&lt;/th>; &lt;th align=&quot;left&quot;>;30.6.2021&lt;/th>; &lt;th align=&quot;left&quot;>;30.6.2020&lt;/th>; &lt;/tr>; &lt;/thead>;&lt;tbody>; &lt;tr>; &lt;td align=&quot;left&quot;>;Cost of living&lt;/td>; &lt;td align=&quot;left&quot;>;5 021,55&lt;/td>; &lt;td align=&quot;left&quot;>;4 921,31&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Apartment&lt;/td>; &lt;td align=&quot;left&quot;>;2 421,56&lt;/td>; &lt;td align=&quot;left&quot;>;2 200,60&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Cost of food&lt;/td>; &lt;td align=&quot;left&quot;>;&lt;/td>; &lt;td align=&quot;left&quot;>;400,00&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td align=&quot;left&quot;>;Electricity&lt;/td>; &lt;td align=&quot;left&quot;>;B00,00&lt;/td>; &lt;td align=&quot;left&quot;>;799,00&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;in OCR this could look like &lt;/p>; &lt;pre>;&lt;code>;Here is a table consisting of answers. As we can see a large part of increase in cost of living can be attributed to increased rent. [...] 30.6.2021 30.6.2020 Cost of living 5 021,55 4 921,31 Apartment 2 421,56 2 200,60 Cost of food 400,00 Electricity B00,00 799,00 &lt;/code>;&lt;/pre>; &lt;p>;So basically the context from the table is lost and eg for &lt;code>;Cost of food&lt;/code>; it&amp;#39;s impossible to know whether the figure is from 2020 or 2021. Intuitively I think it would be beneficial for the LLM to see the data in the order it appears so that data in tables is somehow structured in the text. So that the output would look like this instead&lt;/p>; &lt;pre>;&lt;code>;Here is a table consisting of answers. As we can see a large part of increase in cost of living can be attributed to increased rent. [...] Cost of living (30.6.2021): 5 021,55 Cost of living (30.6.2020): 4 921,31 Apartment (30.6.2021): 2 421,56 Apartment (30.6.2020): 2 200,60 Cost of food (30.6.2021): No data Cost of food (30.6.2020): 400,00 &amp;lt;continued...&amp;gt; &lt;/code>;&lt;/pre>; &lt;p>;First of all I don&amp;#39;t know if this is necessary, or if there is a better approach to sending documents that contain both text/tabular data to LLMs. I have looked into libraries such as &lt;code>;unstructured&lt;/code>; that can return the layout of the document and the table data within it as HTML using &lt;code>;detectron2&lt;/code>;, which could be then parsed into something that looks like the above example, but I&amp;#39;m not very pleased with the quality of the table detection and it is quite slow. Also I imagine this library tries to fit many more use cases than what I need - essentially text and tabular text in different forms (lists, tables, borderless tables). At the moment I&amp;#39;m using AWS textract for table detection which works great but I&amp;#39;d like to move away from it an create my own model that is optimized for my use case and that is free.&lt;/p>; &lt;p>;Currently I&amp;#39;m thinking about creating a pipeline where I train a custom table detection model on my own dataset&lt;/p>; &lt;p>;-&amp;gt; Turn PDF page to an image&lt;br/>; -&amp;gt; Detect location of tables with a model like Table Transformer (TATR)&lt;br/>; -&amp;gt; Collect and remove table from image&lt;br/>; -&amp;gt; Run regular OCR on image with only text and no tables and&lt;br/>; -&amp;gt; Run a model for table recognition/extraction like AWS textract or TATR to extract tabular data&lt;br/>; -&amp;gt; Turn table data into structured text data&lt;br/>; -&amp;gt; Join the text and table datas in order of appearance to create one long text document with all the info of the PDF.&lt;/p>; &lt;p>;Any feedback or suggestions on this? Also is Microsoft&amp;#39;s Table Transformer a smart model to fine-tune with my own data, or are there others that perform better?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/qooooob&quot;>; /u/qooooob &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qhdmz </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qhdmz/d_exctracting_from_documents_that_consist_of_text/"/><updated> 2023-05-24T10:10:37+00:00</updated><published> 2023-05-24T10:10:37+00:00</published><title> [D] Exctracting from documents that consist of text and tabular data for use with LLMs</title></entry><entry><author><name> /u/Cold_Cantaloupe9212</name><uri> https://www.reddit.com/user/Cold_Cantaloupe9212 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我有兴趣开发一个条件扩散模型，以保证给定输入的一致输出。我想减少或消除模型中的随机性以实现此目标。有没有办法在保持一定程度的可变性的同时实现这一目标？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Cold_Cantaloupe9212&quot;>; /u/Cold_Cantaloupe9212 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r3qao </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r3qao/deterministic_diffusion_models/"/><updated> 2023-05-25T01:10:06+00:00</updated><published> 2023-05-25T01:10:06+00:00</published><title> [D]eterministic diffusion models</title></entry><entry><author><name> /u/Meddhouib10</name><uri> https://www.reddit.com/user/Meddhouib10 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hello, Many papers speak about the number of training steps for their model. My question is, when gradient accumulation is used, do we speak about gradient descent steps or just normal training steps ?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Meddhouib10&quot;>; /u/Meddhouib10 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qk9f1/r_number_of_training_steps_in_papers/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qk9f1/r_number_of_training_steps_in_papers/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qk9f1 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qk9f1/r_number_of_training_steps_in_papers/"/><updated> 2023-05-24T12:30:09+00:00</updated><published> 2023-05-24T12:30:09+00:00</published><title> [R] Number of training steps in papers</title></entry><entry><author><name> /u/matt_leming</name><uri> https://www.reddit.com/user/matt_leming </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;How much can deep autoencoders reduce dimensionality of data? I&amp;#39;m trying to implement something that can compress brain images (96&lt;sup>;3&lt;/sup>; ) to a vector (512). It&amp;#39;s basically outputting giant blurs. I&amp;#39;ve tried variational, regular, MMD, and am just going through the process off adjusting weights and tinkering. &lt;/p>; &lt;p>;On the one hand, I know that this type of compression may be asking a lot of the machine learning gods. On the other hand, I&amp;#39;ve seen 3d GANs that can output real crisp brain images, varying widely, no problem. And my implementation should at least be able to overfit on the training set, which it isn&amp;#39;t doing.是什么赋予了？ Do I need an adversarial autoencoder? Why are these models suddenly terrible when one measly dimension is added?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/matt_leming&quot;>; /u/matt_leming &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1pdq/p_compression_ratio_with_deep_autoencoder_for_3d/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1pdq/p_compression_ratio_with_deep_autoencoder_for_3d/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r1pdq </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r1pdq/p_compression_ratio_with_deep_autoencoder_for_3d/"/><updated> 2023-05-24T23:39:01+00:00</updated><published> 2023-05-24T23:39:01+00:00</published><title> [P] Compression ratio with deep autoencoder for 3d images</title></entry><entry><author><name> /u/herbiebradley</name><uri> https://www.reddit.com/user/herbiebradley </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hi all,&lt;/p>; &lt;p>;We at CarperAI have developed a new technique called &lt;a href=&quot;https://carper.ai/quality-diversity-through-ai-feedback/&quot;>;Quality-Diversity with AI Feedback (QDAIF)&lt;/a>;, combining large language models and evolutionary algorithms to generate diverse and high-quality natural language text.&lt;/p>; &lt;p>;QDAIF is all using LMs to provide quality and diversity evaluations, which we use as feedback to optimize a search process which explores the space of text generations from LMs.&lt;/p>; &lt;p>;We use the evolutionary algorithm &lt;a href=&quot;https://arxiv.org/abs/1504.04909&quot;>;MAP-Elites&lt;/a>;, in which a grid defined by our diversity dimensions is populated with increasingly high quality texts generated by our LM evolution operator.&lt;/p>; &lt;p>;QDAIF can improve on some of the limitations of current QD algorithms which often require hand-coded measures of diversity &amp;amp; quality, and can help generate fine-tuning data to help a model improve. We think this highlights the potential to build powerful search algorithms through LM feedback that can explore and refine diverse solutions to nuanced qualitative problems.&lt;/p>; &lt;p>;Blog post: &lt;a href=&quot;https://carper.ai/quality-diversity-through-ai-feedback/&quot;>;https://carper.ai/quality-diversity-through-ai-feedback/&lt;/a>;&lt;/p>; &lt;p>;This was a collaboration with &lt;a href=&quot;https://www.aleph-alpha.com/&quot;>;Aleph Alpha&lt;/a>;, &lt;a href=&quot;https://twitter.com/jennyzhangzt&quot;>;Jenny Zhang&lt;/a>;, &lt;a href=&quot;https://twitter.com/jeffclune&quot;>;Jeff Clune&lt;/a>;, and &lt;a href=&quot;https://twitter.com/kenneth0stanley&quot;>;Ken Stanley&lt;/a>;!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/herbiebradley&quot;>; /u/herbiebradley &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13r1j7a </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r1j7a/p_qualitydiversity_with_ai_feedback/"/><updated> 2023-05-24T23:31:32+00:00</updated><published> 2023-05-24T23:31:32+00:00</published><title> [P] Quality-Diversity with AI Feedback</title></entry><entry><author><name> /u/phoneix阿迪</name><uri>https://www.reddit.com/user/phoneixAdi </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://www.wisdominanutshell.academy/state-of-gpt/&quot;>;https://www.wisdominanutshell.academy/state-of-gpt/&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/phoneixAdi&quot;>; /u/phoneixAdi &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qutu9/n_state_of_gpt_summarized_notes_from_andrej/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qutu9/n_state_of_gpt_summarized_notes_from_andrej/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qutu9 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qutu9/n_state_of_gpt_summarized_notes_from_andrej/"/><updated> 2023-05-24T19:17:06+00:00</updated><published> 2023-05-24T19:17:06+00:00</published><title> [N] &quot;State of GPT&quot; - Summarized notes from Andrej Karpathy&#39;s talk from yesterday.</title></entry><entry><author><name> /u/-Rizhiy-</name><uri> https://www.reddit.com/user/-Rizhiy- </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;I want to train a generative model to generate some items. These items need to follow some known conditions/rules to be valid. How can I best incorporate these conditions/rules into the generative model, such that generated objects are valid?&lt;/p>; &lt;p>;So far I&amp;#39;ve seen multiple approaches:&lt;/p>; &lt;ol>; &lt;li>;Just re-sample until a valid item is generated. This can seriously increase amount of compute required. Plus, this might bias generated items to a subset which is more likely to be valid.&lt;/li>; &lt;li>;Parametrise generated items, such that they are always valid. eg if there is a condition that &lt;code>;A &amp;gt; B&lt;/code>;, we can first generate &lt;code>;B&lt;/code>; and then generate &lt;code>;A&lt;/code>; using something like &lt;code>;A = B * (1 + exp(a))&lt;/code>; where &lt;code>;a&lt;/code>; is the actual generated value. While this solves the problem of having to generate multiple times, this requires definition of parametrised relations, which can be non-trivial and a pain to maintain with changing conditions.&lt;/li>; &lt;li>;Clip values to boundaries according to conditions. This is a bit simpler than parametrisation, but seems like it will produce worse results. Also, ill-posed for categorical values and conditions.&lt;/li>; &lt;/ol>; &lt;p>;Does anyone have experience with problem like that? Any papers/blog posts that discuss this? Perhaps an easier approach?&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/-Rizhiy-&quot;>; /u/-Rizhiy- &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qr11q/d_sampling_items_with_restrictions/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qr11q/d_sampling_items_with_restrictions/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qr11q </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qr11q/d_sampling_items_with_restrictions/"/><updated> 2023-05-24T16:55:50+00:00</updated><published> 2023-05-24T16:55:50+00:00</published><title> [D] Sampling items with restrictions</title></entry><entry><author><name> /u/That_one_coder</name><uri> https://www.reddit.com/user/That_one_coder </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Fsg-Pp downloads images and uses two machine learning models to facilitate the process of changing your profile picture. The first model is a classifier, which decides whether the picture is suitable as a profile picture or not. The second model is an object detection model, for detecting the face and centering the crop on the detection.&lt;/p>; &lt;p>;&lt;a href=&quot;https://github.com/EngMarchG/Fsg-Pp&quot;>;EngMarchG/Fsg-Pp: Fsg-Pp downloads and classifies pictures that are suitable as profile pictures. It also automatically detects the faces and crops it for you! (github.com)&lt;/a>;&lt;/p>; &lt;p>;It took a little over a month of development and a lot of time, but we are very happy with the end product! We are also open for any suggestions you&amp;#39;d like to see (and within the scope of the project)&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/That_one_coder&quot;>; /u/That_one_coder &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qmiau/p_finally_some_good_profile_pictures_released_on/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qmiau/p_finally_some_good_profile_pictures_released_on/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qmiau </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qmiau/p_finally_some_good_profile_pictures_released_on/"/><updated> 2023-05-24T13:58:06+00:00</updated><published> 2023-05-24T13:58:06+00:00</published><title> [P] Finally some good profile pictures, released on github (Fsg-Pp) after a little over a month of development with my friend</title></entry></feed>