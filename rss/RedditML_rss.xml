<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-25T14:13:40+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /u/我是布兰妮</name><uri>https://www.reddit.com/user/ISpearedBritney </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如果你的很多工作都涉及 AI 或 ML（无论标题如何），你能分享一下你的典型工作日是怎样的吗？您将时间花在什么上，最终经常使用哪些工具或资源？其中有多少是数据争论，你使用了多少数学？谢谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ISpearedBritney&quot;>; /u/ISpearedBritney &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rct07 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rct07/d_for_those_of_you_who_work_in_mlai_what_are_your/"/><updated> 2023-05-25T09:21:06+00:00</updated><published> 2023-05-25T09:21:06+00:00</published><title> [D] 对于那些从事 ML/AI 工作的人，您的工作和工作日是什么样的？</title></entry><entry><author><name> /u/Ok_Bank_2217</name><uri> https://www.reddit.com/user/Ok_Bank_2217 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们需要为我们的平台获取大量 YouTube 数据并训练自定义 ML 模型，但除了 YouTube 之外找不到任何有用的东西8M Dataset，相当过时，信息非常有限。官方的 YouTube 数据 API 也被限制在大约 10.000 个积分，这远远不能满足我们需要的数量。&lt;/p>; &lt;p>;这就是为什么我们说去他妈的，并决定自己构建一个巨大的 YouTube 数据集。在为超过 1 亿个视频编制索引并构建自定义 API 来访问它之后，我们决定公开 API 并允许人们购买访问权限！&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.blizzy -data.com/&quot;>;链接到网站&lt;/a>;&lt;/p>; &lt;p>;我们很乐意听到我们的 ML 工程师和数据科学家的反馈，并希望解决您和我们遇到的问题!&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Ok_Bank_2217&quot;>; /u/Ok_Bank_2217 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rh9yj </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rh9yj/p_we_created_a_large_youtube_video_dataset_to/"/><updated> 2023-05-25T13:04:43+00:00</updated><published> 2023-05-25T13:04:43+00:00</published><title> [P] 我们创建了一个大型 YouTube 视频数据集来替换 YouTube 数据 API</title></entry><entry><author><name> /你/米勒</name><uri>https://www.reddit.com/user/mierle </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;QLoRA: Effi量化 LLM 的 cient Finetuning&quot; title=&quot;QLoRA：量化 LLM 的高效微调&quot; />; &lt; /a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mierle&quot;>; /u/mierle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14314&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13r1hkg </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13r1hkg/qlora_efficient_finetuning_of_quantized_llms/"/><updated> 2023-05-24T23:29:47+00:00</updated><published> 2023-05-24T23:29:47+00:00</published><title> QLoRA：量化 LLM 的高效微调</title></entry><entry><author><name>/你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&quot; >;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qrtek </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qrtek/n_state_of_gpt_by_andrej_karpathy_in_msbuild_2023/"/><updated> 2023-05-24T17:25:33+00:00</updated><published> 2023-05-24T17:25:33+00:00</published><title> [N] Andrej karpathy 在 MSBuild 2023 中的 GPT 状态</title></entry><entry><author><name>/u/_negativeonetwelfth</name><uri> https://www.reddit.com/user/_negativeonetwelfth </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;在从许多不同的来源阅读了这些算法的实现之后，我仍然看到了关于此的相互矛盾的信息。一些消息来源说（或暗示）你可以获得更高的帧率，因为你可以更少地运行深度学习对象检测器，并连续几帧使用卡尔曼滤波器预测框。另一方面，一些消息来源表明情况并非如此，因为过滤器仅用于根据先前位置预测当前（而非未来）位置，并且需要在每次迭代中使用深度学习检测进行更新。&lt; /p>; &lt;p>;我想知道是否有人对这些算法有经验并且能够提供一个真实而明确的答案。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/_negativeonetwelfth&quot;>; /u/_negativeonetwelfth &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rgy1g </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rgy1g/d_do_tracking_algorithms_that_use_a_kalman_filter/"/><updated> 2023-05-25T12:50:26+00:00</updated><published> 2023-05-25T12:50:26+00:00</published><title> [D] 使用卡尔曼滤波器（如 SORT 和 DeepSORT）的跟踪算法是否会增加系统的帧率？</title></entry><entry><author><name> /你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-模型架构&quot;>;https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-model-architecture&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qroe9 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qroe9/n_meta_ai_unleashes_megabyte_a_revolutionary/"/><updated> 2023-05-24T17:20:14+00:00</updated><published> 2023-05-24T17:20:14+00:00</published><title> [N] Meta AI 释放 Megabyte，一种革命性的可扩展模型架构</title></entry><entry><author><name>/u/太极官方</name><uri>https://www.reddit.com/user/TaichiOfficial </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/yR2V61hG-MR6-c-B7hm1M_8QJ7LEXpdvxPUZ7Gq9xFI.jpg&quot; alt=&quot;[P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP&quot; title=&quot;[P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP&quot; / >; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Taichi NeRF 使用神经辐射场实现高效的 3D 场景重建和新视点合成，同时提供基于 Python 的工作流程，用于即时 NGP 开发和移动设备上的轻松部署。&lt;/p>; &lt;p>;查看博客：&lt;a href=&quot;https://docs.taichi-lang.org/blog/taichi-instant- ngp&quot;>;https://docs.taichi-lang.org/blog/taichi-instant-ngp&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https ://i.redd.it/gymp61re7z1b1.gif&quot;>;https://i.redd.it/gymp61re7z1b1.gif&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/TaichiOfficial&quot;>; /u/TaichiOfficial &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>;&lt; /表>;</content><id> t3_13rbij5 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/yR2V61hG-MR6-c-B7hm1M_8QJ7LEXpdvxPUZ7Gq9xFI.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13rbij5/p_taichi_nerf_develop_and_deploy_instant_ngp/"/><updated> 2023-05-25T08:03:13+00:00</updated><published> 2023-05-25T08:03:13+00:00</published><title> [P] Taichi NeRF：无需编写 CUDA 即可开发和部署即时 NGP</title></entry><entry><author><name> /u/Tomatomakko</name><uri> https://www.reddit.com/user/Tomatomakko </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;向量神经元 [&lt;a href=&quot;https://arxiv.org/pdf/2104.12229.pdf&quot;>;https://arxiv.org/ pdf/2104.12229.pdf&lt;/a>;] 是一种在 3D 点云处理网络中实现旋转等方差的方法。是否可以将相同的想法转移到 2D CNN？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Tomatomakko&quot;>; /u/Tomatomakko &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rfd15 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rfd15/d_can_vector_neurons_be_used_to_achive_rotational/"/><updated> 2023-05-25T11:36:16+00:00</updated><published> 2023-05-25T11:36:16+00:00</published><title> [D] 可以使用向量神经元在 2D CNN 中实现旋转等方差吗？</title></entry><entry><author><name> /u/周杰伦</name><uri>https://www.reddit.com/user/JayCTee </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嘿伙计们，我正在做一个（假设的）项目，我想在大量非结构化客户数据上训练预训练的 LLM，以获得大公司。这个想法是它可以作为来自所有数据源的客户的知识库，可用于超个性化（例如产品匹配、内容生成等）。&lt;/p>; &lt;p>;我对训练部分——如果我想让它“学习”客户数据，我是使用微调、提示调优还是 RAG（我将微调理解为调整现有参数的权重，提示调优为添加更多参数，并且很难理解 RAG 是什么）。我看到一些消息来源说微调不能用于学习新数据？&lt;/p>; &lt;p>;任何人都可以就此过程或需要考虑的事项（例如数据敏感性）提供任何指示。我的在线搜索并不是那么富有成果&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/JayCTee&quot;>; /u/JayCTee &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rgmyt/p_uptraining_a_pretrained_model_using_company_data/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rgmyt/p_uptraining_a_pretrained_model_using_company_data/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rgmyt </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rgmyt/p_uptraining_a_pretrained_model_using_company_data/"/><updated> 2023-05-25T12:36:29+00:00</updated><published> 2023-05-25T12:36:29+00:00</published><title> [P] 使用公司数据训练预训练模型？</title></entry><entry><author><name> /u/I_will_delete_myself</name><uri> https://www.reddit.com/user/I_will_delete_myself </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我犹豫了一会儿，但听到这个消息后虚伪让我发疯。&lt;/p>; &lt;p>;SMH 这家公司就像白衣骑士一样，他们认为他们凌驾于所有人之上。他们想要监管，但他们希望不受该监管的影响。只想伤害其他人，但不想伤害“全能的”Sam 和朋友。&lt;/p>; &lt;p>;向国会撒谎说建议在欧盟采取类似的做法，但现在开始抱怨他们。在任何政治领域都不应该认真对待这个家伙。&lt;/p>; &lt;p>;我的观点是，这家公司通过锁定与其品牌名称相悖的东西来反对 AI 进步。如果他们甚至不能忠于这样简单的事情，我们怎么能指望他们忠于更难的 AI 安全？&lt;/p>; &lt;p>;我很高兴他们现在改变了立场，但我很高兴他们如何他们认为他们有权为了自己的利益而腐败。 SMH!!!!!!!!&lt;/p>; &lt;p>;你有什么想法？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/I_will_delete_myself&quot;>; /u/I_will_delete_myself &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rie0e </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rie0e/openai_is_now_complaining_about_regulation_of_ai_d/"/><updated> 2023-05-25T13:51:58+00:00</updated><published> 2023-05-25T13:51:58+00:00</published><title> OpenAI 现在抱怨人工智能的监管 [D]</title></entry><entry><author><name> /u/深渊</name><uri>https://www.reddit.com/user/abystoma </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;如标题所示，任何人都可以向我推荐论文或任何使用启发式方法预测隐藏神经元和输出层输出的资源，因为我们有一个数据集的输入和输出。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/abystoma&quot;>; /u/abystoma &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r8gzk </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r8gzk/d_has_there_been_any_work_done_to_predict_the/"/><updated> 2023-05-25T05:03:18+00:00</updated><published> 2023-05-25T05:03:18+00:00</published><title> [D] 是否有任何工作通过使用启发式来预测隐藏神经元和输出层的输出？</title></entry><entry><author><name> /u/panthsdger</name><uri> https://www.reddit.com/user/panthsdger </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;全文：&lt;a href=&quot;https://arxiv.org/abs/2305.11252v1&quot;>;https://arxiv.org/abs /2305.11252v1&lt;/a>;&lt;/p>; &lt;p>;人工神经网络 (ANN) 已成为机器学习的重要工具，在图像和语音生成、游戏和机器人技术等多个领域取得了显著成功。然而，人工神经网络之间存在根本差异。操作机制和生物大脑的机制，特别是关于学习过程。本文全面回顾了人工神经网络中当前受大脑启发的学习表征。我们研究整合更多生物学上合理的机制，例如突触可塑性，以增强这些网络的功能。能力。此外，我们深入研究了伴随这种方法的潜在优势和挑战。最终，我们为这个快速发展的领域的未来研究指出了有前途的途径，这可以使我们更接近理解智能的本质。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/panthsdger&quot;>; /u/panthsdger &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rinw8 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rinw8/r_braininspired_learning_in_artificial_neural/"/><updated> 2023-05-25T14:02:59+00:00</updated><published> 2023-05-25T14:02:59+00:00</published><title> [r] 人工神经网络中的类脑学习：综述</title></entry><entry><author><name>/u/联邦任务</name><uri>https://www.reddit.com/user/fedetask </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我对变形金刚没有太多经验，但我的理解是让它们如此强大的主要特征是它们不在输入之间有一个&lt;em>;连续&lt;/em>;隐藏状态来维护，以及它们对离散标记进行操作的事实。&lt;/p>; &lt;p>;在 RNN 中，在每个新输入之后，模型产生的连续隐藏状态甚至可能有很小的“错误” （由于精度、模型权重的不完善等）并且没有机制强制此输出“退回”到它的“正确”价值。这个输出然后用于 RNN 的下一步，但是没有硬性保证 RNN 能够正确地解释它并且不会开始偏离正确的轨迹。当然，这就是训练的目的，但由于神经网络总是有点嘈杂，问题仍然存在。&lt;/p>; &lt;p>;另一方面，变形金刚没有连续的隐藏状态在每一步更新，并产生离散的令牌。如果模型为当前标记生成不完美的 logits，则相应的离散输出不太可能发生变化。这种机制使得任何足够小的错误都可以被“重新吸收”。由模型。出于同样的原因，我们可以通过为前面的步骤提供正确的值来安全地进行教师强制并为序列的每一步训练模型——不需要在训练期间进行自动回归——。 &lt;/p>; &lt;p>;例如，如果对连续值进行操作的转换器执行类似，我会感到惊讶。我希望当使用自动回归时，输出中的任何小错误都会在该输出用作下一个输入时使模型漂移，因为模型仅使用完美输入进行训练。 &lt;/p>; &lt;p>;Transformers 还有其他不错的特性，Attention 可能是最重要的，但我认为这才是真正让 Transformers 在 NLP 任务上表现出色的原因。&lt;/p>; &lt;p>;你同意吗？是否有任何工作与上述推理相矛盾？还是我错过了一些重要的东西？如果我上面说的是正确的，是否有任何工作将注意力集中在其他可能的“错误吸收”方法上？机制还是架构？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/fedetask&quot;>; /u/fedetask &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13ri6tc/d_transformers_are_so_effective_because_they_are/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ri6tc/d_transformers_are_so_effective_because_they_are/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13ri6tc </id><link href="https://www.reddit.com/r/MachineLearning/comments/13ri6tc/d_transformers_are_so_effective_because_they_are/"/><updated> 2023-05-25T13:43:42+00:00</updated><published> 2023-05-25T13:43:42+00:00</published><title> [D] 变形金刚之所以如此有效，是因为它们是离散的</title></entry><entry><author><name>/u/Usual-Shopping-9638</name><uri> https://www.reddit.com/user/Usual-Shopping-9638 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;strong>;论文&lt;/strong>;&lt;br/>; &lt;a href=&quot;https://arxiv.org/abs/2210.05409&quot;>;https ://arxiv.org/abs/2210.05409&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;strong>;代码&lt;/strong>;&lt;/p>; &lt;p>;&lt;a href =&quot;https://github.com/kakaobrain/leco&quot;>;https://github.com/kakaobrain/leco&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;strong >;Abstract&lt;/strong>;&lt;/p>; &lt;p>;Episodic count 已被广泛用于设计一种简单而有效的内在动机，用于具有稀疏奖励的强化学习。然而，在高维状态空间以及长时间内使用情节计数需要彻底的状态压缩和快速散列，这阻碍了在如此困难和复杂的探索环境中对其进行严格的利用。此外，情节计数中与任务无关的观察的干扰可能会导致其内在动机忽略与任务相关的重要状态变化，而情节方式的新颖性会导致反复重访熟悉的状态。为了解决这些问题，在本文中，我们提出了一种可学习的基于哈希的情节计数，我们将其命名为 LECO，它可以在困难的探索问题中作为特定于任务的内在奖励有效地执行。特别是，所提出的内在奖励包括情节新颖性和特定于任务的调制，其中前者使用矢量量化变分自编码器自动获取离散状态代码以进行快速计数，而后者通过学习调制器来优化情节新颖性任务特定的外在奖励。拟议的 LECO 特别允许在强化学习期间从探索到利用的自动过渡。我们通过实验表明，与以前的探索方法相比，LECO 成功解决了困难的探索问题，并且还通过 MiniGrid 和 DMLab 环境中最困难的任务扩展到大状态空间。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -- >; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Usual-Shopping-9638&quot;>; /u/Usual-Shopping-9638 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13rbch6/r_leco_learnable_episodic_count_for_taskspecific/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13rbch6/r_leco_learnable_episodic_count_for_taskspecific/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rbch6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13rbch6/r_leco_learnable_episodic_count_for_taskspecific/"/><updated> 2023-05-25T07:53:14+00:00</updated><published> 2023-05-25T07:53:14+00:00</published><title> [R] LECO：针对特定任务的内在奖励的可学习情节计数</title></entry><entry><author><name>/u/J00Nnn</name><uri> https://www.reddit.com/user/J00Nnn </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，有在 k8s 上进行 ML 训练的经验的人可以分享您使用的工具或框架吗？它不一定是端到端的管道解决方案（例如 Kubeflow）。&lt;/p>; &lt;p>;例如，我有 TensorFlow 模型，我想利用分布式训练，但是在 Kubernetes 资源上。有什么建议吗？&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;顺便说一句，我在这方面的经验很少，所以欢迎任何新的方向或更正，谢谢！！&lt;/p>; &lt;/ div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/J00Nnn&quot;>; /u/J00Nnn &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r670t </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r670t/discussion_guidance_on_training_ml_models_on/"/><updated> 2023-05-25T03:05:52+00:00</updated><published> 2023-05-25T03:05:52+00:00</published><title> [讨论] 关于在 Kubernetes 上训练 ML 模型的指南</title></entry><entry><author><name>/你/sann540</name><uri> https://www.reddit.com/user/sann540 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://techcrunch.com/2023/05/23/microsoft-debuts-azure-ai-studio-to- let-developers-build-their-own-ai-copilots/&quot;>;https://techcrunch.com/2023/05/23/microsoft-debuts-azure-ai-studio-to-let-developers-build-their- own-ai-copilots/&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sann540&quot;>; /u/sann540 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qs34u </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qs34u/n_microsofts_azure_ai_studio_lets_developers/"/><updated> 2023-05-24T17:35:51+00:00</updated><published> 2023-05-24T17:35:51+00:00</published><title> [N] Microsoft 的 Azure AI Studio 让开发人员可以构建自己的 AI“副驾驶”</title></entry><entry><author><name> /你/nicku_a</name><uri> https://www.reddit.com/user/nicku_a </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我们刚刚更新了 AgileRL，我们的强化学习训练框架比 SOTA 快 10 倍，支持离线强化学习！ &lt;/p>; &lt;p>;许多有 RL 可解决问题的人无法访问模拟器，但有大量数据。&lt;/p>; &lt;p>;您现在可以轻松地在静态数据上训练代理，而无需模拟，并使用进化超参数优化来更快更好地学习！&lt;/p>; &lt;p>;此版本包括：&lt;/p>; &lt;ul>; &lt;li>;新的通用离线 RL 训练功能，可从静态数据中学习&lt;/li >; &lt;li>;Conservative Q-Learning (CQL)&lt;/li>; &lt;li>;与 Minari 完全兼容&lt;/li>; &lt;/ul>; &lt;p>;查看：&lt;a href=&quot;https://github.com/ AgileRL/AgileRL&quot;>;https://github.com/AgileRL/AgileRL&lt;/a>; &lt;/p>; &lt;p>;如果你想参与这个项目，或者只是想进行讨论，请加入我们的discord （链接在我们的 GitHub 存储库顶部）！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/nicku_a&quot;>; /u/nicku_a &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qgzt5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qgzt5/p_offline_reinforcement_learning_10x_faster_than/"/><updated> 2023-05-24T09:48:18+00:00</updated><published> 2023-05-24T09:48:18+00:00</published><title> [P] 离线强化学习 - 比具有进化 HPO 的 SOTA 快 10 倍</title></entry><entry><author><name>/u/trolls_toll</name><uri> https://www.reddit.com/user/trolls_toll </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我最近一直在用很多 1000x1000 矩阵做一些数值模拟，主要是为了分散过去几个月的疯狂。我想我还不如把每件事都做对，然后从头开始整个考验——为我的 M1 机器选择最好的 BLAS 库（实际上我只是超级生疏，谷歌搜索感觉比手工推导更容易） .&lt;/p>; &lt;p>;目前conda-forge已经预编译了基于三个BLAS实现的包：openblas、netlib和accelerate。前两个是非原生的，后者是 Apple 为其处理器优化的。可能还有其他版本可以通过 Anaconda 获得，但我并没有真正检查，因为那里的大多数数字库都链接到英特尔的 MKL，这在 mac 上不起作用。 &lt;/p>; &lt;p>;安装不同版本的 BLAS 很简单，实际上只需在 YAML conda 配方中设置一个标志。因此，我最终使用 numpy 和 scipy 的原生 &lt;code>;.test()&lt;/code>; 方法以及我在网上找到的两个脚本对所有三个 BLAS 包进行了基准测试：&lt;a href=&quot;https: //gist.github.com/MarkDana/a9481b8134cf38a556cf23e1e815dafb#2-benchmarks&quot;>;Mark Dana 的大量 SVD&lt;/a>; 和 &lt;a href=&quot;https://gist.github.com/markus-beuckelmann/8bc25531b11158431a5b09a45abd627 6&quot; >;由 Markus Beuckelmann 编写的具有一些矩阵和不同矩阵分解的要点&lt;/a>;。 &lt;/p>; &lt;p>;这是我的结果，都是在新的 conda 环境中完成的：&lt;/p>; &lt;p>;&lt;strong>;apple 的加速 &lt;code>;blas=*=accelerate&lt;/code>;&lt;/strong>;&lt;/p >; &lt;ul>; &lt;li>;svd 1.03 秒&lt;/li>; &lt;li>;matmuls 20 秒&lt;/li>; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 3 次失败，25083 次通过，393 次跳过，1309 次取消选择, 44 xfailed, 5 xpassed, 25 warnings in 76.34s (0:01:16)&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 在 linalg/tests/test_cython_blas.py 测试失败，在20% &lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;conda-forge vanilla&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 13.53 秒&lt;/li>; &lt;li>;matmuls 44 秒&lt;/li >; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 没有失败，25075 次通过，404 次跳过，1309 次取消选择，44 次失败，5 次通过，69.25 秒 (0:01:09) 中有 32 次警告&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 7 次失败，37984 次通过，2301 次跳过，12295 次取消选择，139 次失败，9 次通过，355.99 秒 (0:05:55) 中有 72 次警告&lt;/li>; &lt; /ul>; &lt;p>;&lt;strong>;netlib &lt;code>;=*=netlib&lt;/code>;&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 4.44 秒&lt;/li>; &lt;li>;matmuls 330 秒&lt;/ li>; &lt;li>;numpy 12 失败，25063 次通过，404 次跳过，1309 次取消选择，44 次失败，5 次通过，73.60 秒 (0:01:13) 中有 24 条警告&lt;/li>; &lt;li>;scipy 153 失败，37839 次通过， 2301 跳过，12295 取消选择，139 xfailed，8 xpassed，347.62s (0:05:47) 内有 86 个警告&lt;/li>; &lt;/ul>; &lt;p>;&lt;strong>;openblas &lt;code>;=*=openblas&lt;/code>; &lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;svd 12.44 秒&lt;/li>; &lt;li>;matmuls 45 秒&lt;/li>; &lt;li>;&lt;code>;numpy.test()&lt;/code>; 没有失败 25075 通过, 404 skiped, 1309 undeselected, 44 xfailed, 5 xpassed, 69.98s (0:01:09) 内有 32 个警告&lt;/li>; &lt;li>;&lt;code>;scipy.test()&lt;/code>; 7 failed, 37984 passed, 2301 skiped, 12295 undeselected, 139 xfailed, 9 xpassed, 72 warnings in 356.14s (0:05:56)&lt;/li>; &lt;/ul>; &lt;p>;这里有几个教训：a) vanilla conda-forge numpy 和 scipy版本带有 openblas，它工作得很好，b) 不要使用 netlib，除非你的矩阵很小并且你需要做很多 SVD，或者知道为什么 c) Apple 的 &lt;code>;veclib/accelerate&lt;/ code>; 非常快，但它在数值上也不稳定。以至于 scipy 的开发者 &lt;a href=&quot;https://github.com/scipy/scipy/wiki/Dropping-support-for-Accelerate&quot;>;早在 2018 年就放弃了对它的任何支持&lt;/a >;。像该死的。也就是说，他们显然将它带回来了，因为 macOS Ventura 的 13.3 版本在 &lt;code>;accelerate&lt;/code>; 性能方面有了一些重大改进。&lt;/p>; &lt;p>;FIN &lt;/p>; &lt;p>;ps我打算在 Mathematica 中做我的事情，因为动态 3D 图 &amp;gt;&amp;gt;&amp;gt;在这里和那里节省了几分钟。&lt;/p>; &lt;p>;pps 呃，忘了补充，它全部在 Apple M1 Pro 上测试，10 个内核运行 Ventura 13.3.1，python 3.10.11，conda 23.3.1，numpy 1.24 .3，scipy 1.10.1，libblas 3.9.0，openblas 0.3.21。 Netlib blas 的版本是 2.104，accelerate、openblas 和 vanilla 的版本是 2.116&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/trolls_toll&quot;>; /u/trolls_toll &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qp0s6 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qp0s6/d_which_blas_library_to_choose_for_apple_silicon/"/><updated> 2023-05-24T15:37:56+00:00</updated><published> 2023-05-24T15:37:56+00:00</published><title> [D] Apple Silicon 选择哪个 BLAS 库？</title></entry><entry><author><name> /u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;在 ChatGPT 之前一个月左右，我是一个团队的一员，该团队提交了一篇论文，我们将 LLM 应用于临床文本笔记的特征提取用于分流目的。这篇论文本月发表在一份医学杂志上，所以它更适合临床人群，但我还是想在这里分享它：&lt;a href=&quot;https://www.annfammed.org /content/21/3/240&quot;>;https://www.annfammed.org/content/21/3/240&lt;/a>; &lt;/p>; &lt;blockquote>; &lt;p>;&lt;strong>;目的&lt;/strong>; 呼吸症状是初级保健中最常见的主诉。这些症状通常会自行解决，但它们可能表明患有严重的疾病。随着医生工作量和医疗保健成本的增加，在面对面咨询之前对患者进行分类会有所帮助，可能会为低风险患者提供其他沟通方式。本研究的目的是训练一个机器学习模型，在前往初级保健诊所之前对有呼吸道症状的患者进行分类，并在分类的背景下检查患者的结果。&lt;/p>; &lt;p>;&lt;strong>;方法&lt;/strong>;我们训练了一个机器学习模型，使用仅在就诊前可用的临床特征。从接受 7 个国际疾病分类第 10 次修订代码（J00、J10、JII、J15、J20、J44、J45）之一的患者的 1,500 条记录中提取临床文本注释。冰岛雷克雅未克地区的所有初级保健诊所都包括在内。该模型在 2 个外部数据集中对患者进行评分，并将他们分为 10 个风险组（值越高风险越大）。我们分析了每组的选定结果。&lt;/p>; &lt;p>;&lt;strong>;结果&lt;/strong>; 风险组 1 到 5 包括 C 反应蛋白值较低的年轻患者、初级和急诊的重新评估率、与第 6 至 10 组相比，抗生素处方率、胸部 X 光 (CXR) 转诊和具有肺炎体征的 CXR。第 1 至 5 组没有具有肺炎体征或医生诊断为肺炎的 CXR。&lt;/p>; &lt;p>;&lt;strong>;结论&lt;/strong>; 该模型根据预期结果对患者进行了分类。该模型可以通过消除风险组 1 到 5 中的 CXR 转诊数量来减少 CXR 转诊的数量，从而在没有临床医生输入的情况下减少临床上无关紧要的偶发瘤发现。&lt;/p>; &lt;/blockquote>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qwvl1 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qwvl1/r_triaging_patients_with_artificial_intelligence/"/><updated> 2023-05-24T20:33:52+00:00</updated><published> 2023-05-24T20:33:52+00:00</published><title> [R] 在初级保健中针对呼吸道症状对人工智能患者进行分类以改善患者预后：一项回顾性诊断准确性研究</title></entry><entry><author><name>/u/waa007</name><uri> https://www.reddit.com/user/waa007 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Alpaca or LLaMA ?(严格来说，它们是open available，不是open source，open source的定义来自&lt;a href=&quot;https: //opensource.org/osd&quot;>;OSI&lt;/a>;)&lt;/p>; &lt;p>;是否有其他一些开放的&lt;del>;source&lt;/del>;可用的LLM？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/waa007&quot;>; /u/waa007 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qnc80 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qnc80/d_what_is_the_best_open_source_llm_so_far/"/><updated> 2023-05-24T14:30:17+00:00</updated><published> 2023-05-24T14:30:17+00:00</published><title> [D] 目前最好的开源 LLM 是什么？</title></entry><entry><author><name> /你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/ai-generated-podcast-ads-on-spotify-could -soon-become-a-reality-1f6bb1a056b0&quot;>;https://medium.com/@tiago-mesquita/ai-generated-podcast-ads-on-spotify-could-soon-become-a-reality-1f6bb1a056b0&lt;/ a>;&lt;/p>; &lt;p>;在 &lt;strong>;The Bill Simmons Podcast&lt;/strong>; 的最近一集中，主持人兼 The Ringer 的创始人 Bill Simmons 表达了他相信利用自己的声音进行&lt;/p>; &lt;p>;&lt;strong>;他说：&lt;/strong>;&lt;/p>; &lt;blockquote>; &lt;p>;&lt;em>;“将有一种方法可以将我的声音用于广告。显然，你必须对声音表示认可，但从广告的角度来看，它为你打开了所有这些不同的巨大可能性。”&lt;/em>;&lt;/p>; &lt;/blockquote>; &lt;p>;Simmons 是The Ringer，一个播客网络和网站，在 2020 年被 Spotify 以近 2 亿美元的价格收购&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qq5ky </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qq5ky/n_spotify_may_be_working_on_the_possibility_of/"/><updated> 2023-05-24T16:21:42+00:00</updated><published> 2023-05-24T16:21:42+00:00</published><title> [N] Spotify 可能正在研究提供 AI 生成的播客广告的可能性</title></entry><entry><author><name>/你/杰斯特177</name><uri> https://www.reddit.com/user/jesst177 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨！&lt;/p>; &lt;p>;我们最近决定购买一台预算为 15,000 美元的工作站。我们查看了本地供应商的选项并检查了他们的计算能力，并提出了几个选项。&lt;/p>; &lt;p>;- 4X A4500&lt;/p>; &lt;p>;- 1XA6000&lt;/p>; &lt;p>;我们还可以寻找具有中级选项的任何其他替代方案，例如 2X A5000/A5500。然而，从我们的角度来看，A4500s 拥有更多的计算能力，并将拥有大约 80 GB 的内存。虽然我不确定我们是否可以像在多 GPU 设置中那样将它们全部一起使用（我们可以吗？）这意味着它是更好的选择。我们应该选择 4X A4500 还是任何中间选项？&lt;/p>; &lt;p>;我们感兴趣的机器将用于深度学习，包括 Transformers 和 ConvNets。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/jesst177&quot;>; /u/jesst177 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qxf3g </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qxf3g/d_should_we_go_with_a_single_a6000_or_4xa4500_or/"/><updated> 2023-05-24T20:55:18+00:00</updated><published> 2023-05-24T20:55:18+00:00</published><title> [D] 我们应该使用单个 A6000 还是 4XA4500 或任何其他替代方案，例如 2XA5000</title></entry><entry><author><name> /你/baqirjafari</name><uri> https://www.reddit.com/user/baqirjafari </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，&lt;/p>; &lt;p>;我正在研究一个需要多语言嵌入模型的案例研究。我做了一些研究，发现 &lt;a href=&quot;https://www.sbert.net/docs/pretrained_models.html#model-overview&quot;>;paraphrase-multilingual-mpnet-base-v2&lt;/a>; 很好选项。但是，我想知道是否有更好的模型可以处理英语、乌尔都语、波斯语、阿拉伯语等语言。有人对其他多语言嵌入模型有任何建议或经验吗？我将不胜感激任何帮助或建议。非常感谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/baqirjafari&quot;>; /u/baqirjafari &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r5x6s </id><link href="https://www.reddit.com/r/MachineLearning/comments/13r5x6s/d_looking_for_a_better_multilingual_embedding/"/><updated> 2023-05-25T02:53:08+00:00</updated><published> 2023-05-25T02:53:08+00:00</published><title> [D] 寻找更好的多语言嵌入模型</title></entry><entry><author><name>/你/硬丸</name><uri>https://www.reddit.com/user/hardmaru </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;em>;Schmidhuber 采访表达了他对 AI 和 AGI 未来的看法。&lt;/em>;&lt;/p>; &lt;p>;&lt;em>;原创&lt;a href=&quot;https://www.forbes.com/sites/hessiejones/2023/05/23/juergen-schmidhuber-renowned-father-of-modern-ai-says-his-lifes-work-wont-lead -to-dystopia/&quot;>;来源&lt;/a>;。我认为 &lt;a href=&quot;/r/MachineLearning&quot;>;r/MachineLearning&lt;/a>; 对这次采访很感兴趣，并且与 AI 领域其他有影响力的领导者相比提出了另一种观点。&lt;/em>;&lt;/p>; &lt; p&lt;strong>;Juergen Schmidhuber，著名的“现代人工智能之父”，说他一生的工作不会导致反乌托邦&lt;/strong>;&lt;/p>; &lt;p>;&lt;em>;2023 年 5 月 23 日。供稿者 &lt;a href=&quot;https://twitter.com/hessiejones&quot;>;Hessie Jones&lt;/a>;.&lt;/em>;&lt;/p>; &lt;p>;随着人们越来越关注更先进的人工智能 (AI) 技术对社会的影响，技术界中有许多人担心这些进步的影响在生成人工智能中，如果他们不加检查。著名科学家、人工智能研究员、被广泛认为是该领域的先驱之一的于尔根·施密德胡伯博士则更为乐观。他宣称，许多突然警告 AI 危险的人只是为了宣传，利用媒体对杀手机器人的痴迷，这种机器人比医疗保健等领域的“好 AI”更受关注。&lt;/p>; &lt;p>;彻底改变各个行业并改善我们的生活是显而易见的，如果不良行为者利用该技术谋取私利，同样危险。我们是在走向一个反乌托邦的未来，还是有理由保持乐观？我有机会与 Juergen Schmidhuber 博士坐下来了解他对这辆看似飞速发展的 AI 火车的看法，它将带领我们迈向未来。&lt;/p>; &lt;p>;作为 1970 年代的少年，Juergen Schmidhuber 成为着迷于创造智能机器的想法，这些机器可以自己学习和改进，在他的有生之年变得比他自己更聪明。这最终导致了他在深度学习领域的开创性工作。&lt;/p>; &lt;p>;1980 年代，他在慕尼黑工业大学 (TUM) 学习计算机科学，并于 1987 年获得文凭。他的论文是在最终的自我改进机器上，它们不仅通过一些预先连接的人工设计的学习算法进行学习，而且还学习和改进学习算法本身。几十年后，这成为一个热门话题。他还获得了博士学位。 Schmidhuber 于 1991 年在慕尼黑工业大学工作，为现代人工智能奠定了一些基础。&lt;/p>; &lt;p>;Schmidhuber 最出名的是他对循环神经网络 (RNN) 发展的贡献，这是最强大的人工神经网络类型可以处理语音和自然语言等序列数据。他与他的学生 Sepp Hochreiter、Felix Gers、Alex Graves、Daan Wierstra 等人一起发表了长短期记忆 (LSTM) 的架构和训练算法，LSTM 是一种广泛用于自然语言处理、语音识别的 RNN 、视频游戏、机器人和其他应用。 LSTM 已成为 20 世纪被引用最多的神经网络，《商业周刊》将其称为“&lt;a href=&quot;https://www.bloomberg.com/news/features/2018-05-15/google-amazon-and -facebook-owe-j-rgen-schmidhuber-a-fortune?leadSource=uverify%20wall&quot;>;可以说是最商业化的 AI 成就&lt;/a>;。&quot;&lt;/p>; &lt;p>;在他的整个职业生涯中，Schmidhuber 获得了各种因其开创性工作而获得的奖项和荣誉。 2013 年，他被授予亥姆霍兹奖，以表彰他在机器学习领域的重大贡献。 2016 年，他因“对深度学习和神经网络的开创性贡献”而获得 IEEE 神经网络先驱奖。媒体经常称他为“现代人工智能之父”，&lt;/em>;，因为&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/most-cited-neural-nets.html&quot;>;最引用的神经网络&lt;/a>;都建立在他实验室的工作之上。不过，他很快指出，人工智能的历史&lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/deep-learning-history.html&quot;>;可以追溯到几个世纪前。&lt;/a>;&lt;/p >; &lt;p>;尽管他取得了许多成就，但在 60 岁时，他感到在有生之年构建通用人工智能的时间压力越来越大，并继续致力于推动 AI 研发的边界。现任KAUST AI Initiative主任，瑞士人工智能实验室IDSIA科学总监，人工智能公司NNAISENSE的联合创始人兼首席科学家，该公司的座右铭是“AI∀”这是一种受数学启发的表达“AI For All”的方式。他继续致力于尖端人工智能技术和应用，以改善人类健康、延长人类寿命并让每个人的生活更轻松。&lt;/p>; &lt;p>;&lt;em>;为清楚起见，对以下采访进行了编辑。&lt;/em>; &lt;/p>; &lt;p>;&lt;strong>;Jones：感谢 Juergen 加入我的行列。你已经签署了关于 AI 武器的警告信。但是你没有在最近的出版物“暂停巨大的人工智能实验：一封公开信”上签名？有原因吗？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 谢谢 Hessie。很高兴和你说话。我意识到，许多在公开场合警告 AI 危险的人只是为了宣传。我认为最新的这封信不会产生任何重大影响，因为许多 AI 研究人员、公司和政府会完全忽视它。&lt;/p>; &lt;p>;该提案经常使用“我们”这个词。指的是“我们”，人类。但正如我过去多次指出的那样，没有“我们”之分。每个人都可以认同。问 10 个不同的人，您会听到 10 种关于什么是“好”的不同意见。其中一些意见将彼此完全不相容。不要忘记许多人之间的巨大冲突。&lt;/p>; &lt;p>;这封信还说，“&lt;em>;如果这样的暂停不能很快到位，政府应该进行干预并实施暂停。&lt;/em>;“问题是不同的政府对于什么对他们和其他人有好处也有不同的看法。大国 A 会说，如果我们不这样做，大国 B 将会（也许是秘密地）获得对我们的优势。大国 C 和 D 也是如此。&lt;/p>; &lt;p>;&lt;strong>;琼斯：每个人都承认对当前的生成人工智能技术的恐惧。此外，&lt;/strong>; &lt;a href=&quot;https://www.bbc.com/news/world-us-canada-65616866&quot;>;&lt;strong>;Sam Altman&lt;/strong>; 已公开承认这项技术存在的威胁strong>;&lt;/a>;&lt;strong>;，OpenAI 的 CEO 本人，呼吁对 AI 进行监管。从您的角度来看，是否存在生存威胁？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;AI 确实可以被武器化，我毫不怀疑会有各种各样的威胁AI 军备竞赛，但 AI 不会引入新的生存威胁。与根本不需要人工智能的核氢弹带来的更古老的威胁相比，人工智能武器带来的威胁似乎显得微不足道。我们应该更加害怕以氢弹火箭形式出现的半个世纪前的技术。 1961 年的沙皇炸弹的破坏力几乎是二战时期所有武器总和的 15 倍。尽管自 1980 年代以来发生了戏剧性的核裁军，但仍然有足够多的核弹头在两个小时内消灭人类文明，没有任何人工智能，我更担心那种古老的生存威胁，而不是相当无害的人工智能武器。&lt;/p >; &lt;p>;&lt;strong>;Jones：我意识到当你将 AI 比作核弹的威胁时，当前存在一种危险，即当前的技术可以交到人类手中并使他们“最终”能够造成进一步的伤害以非常精确的方式针对群体中的个人，例如有针对性的无人机攻击。你给了人们一个他们以前从未有过的工具集，让坏人能够像一些人指出的那样，比以前做更多的事情，因为他们没有这种技术。&lt;/strong >;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 现在，所有这一切在原则上听起来很可怕，但我们现有的法律足以应对这些由 AI 支持的新型武器。如果你用枪杀了人，你就会进监狱。如果您用其中一架无人机杀死某人，情况也是如此。执法部门将更好地了解新威胁和新武器，并将以更好的技术应对这些威胁。使无人机能够以一种需要一些跟踪和一些智能来执行的方式从远处瞄准人员，这在传统上是由技术熟练的人执行的，对我来说，这似乎只是传统武器的改进版本，比如枪，是，你知道，比老枪更聪明一点。&lt;/p>; &lt;p>;但是，原则上，所有这些都不是新的发展。许多世纪以来，我们经历了更好的武器装备和更致命的毒药等的发展，执法部门也不断发展他们的政策以应对这些威胁。所以，这并不是说我们突然有了一种新的生存威胁，而且它比我们大约六年来所经历的要令人担忧得多。大型核弹头不需要花哨的面部识别来杀死一个人。不，它只是摧毁了拥有一千万居民的整个城市。&lt;/p>; &lt;p>;&lt;strong>;琼斯：隐含的生存威胁是人类对这项技术的控制程度。我们看到了一些机会主义的早期案例，正如你所说，比起积极的突破，它们更容易受到媒体的关注。但你是在暗示这一切都会平衡吗？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;从历史上看，我们有着悠久的技术突破传统，这导致了武器的进步防御的目的也是为了保护。从棍子到石头，从斧头到火药，从大炮到火箭……现在到无人机……这对人类历史产生了巨大影响，但贯穿历史始终的是，那些利用技术达到自己目的的人就是他们自己，面对相同的技术，因为对方正在学习使用它来对付他们。这就是人类几千年历史中重复发生的事情，并将继续下去。我不认为新的 AI 军备竞赛会像旧的核弹头那样对生存构成威胁。&lt;/p>; &lt;p>;你说了一些重要的事情，因为有些人更喜欢谈论缺点而不是这项技术的好处，但这是一种误导，因为 95% 的人工智能研究和人工智能开发都是为了让人们更快乐，并促进人类的生活和健康。&lt;/p>; &lt;p>;&lt;strong>;琼斯：让我们谈谈 AI 研究中的一些有益进展，这些进展已经能够从根本上改变当今的方法并取得突破。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 好吧！例如，十一年前，我们的团队和我的博士后 Dan Ciresan 是第一个赢得 &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/first-time-deep-learning-won-medical- imaging-contest-september-2012.html&quot;>;通过深度学习进行医学成像竞赛&lt;/a>;。我们分析了女性乳腺细胞，目的是确定无害细胞与处于癌前阶段的细胞。通常，训练有素的肿瘤学家需要很长时间才能做出这些决定。我们的团队对癌症一无所知，却能够在大量此类数据上训练出一开始完全愚蠢的人工神经网络。它能够胜过所有其他方法。今天，这不仅用于乳腺癌，还用于放射学和检测动脉斑块，以及许多其他方面。我们在过去 3 年中开发的一些神经网络现在在成千上万的医疗保健应用程序中普遍存在，检测糖尿病和 Covid-19 等等。这最终将渗透到所有医疗保健领域。这种类型的 AI 的良好后果比使用 AI 进行犯罪的点击诱饵新方法重要得多。&lt;/p>; &lt;p>;&lt;strong>;Jones：采用是强化结果的产物。大规模采用要么让我们相信人们误入歧途，要么相反，技术正在对人们的生活产生积极影响。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;后者是可能的情况。对好的 AI 而不是坏的 AI 存在巨大的商业压力，因为公司想卖给你一些东西，而你只会购买你认为对你有好处的东西。因此，仅仅通过这种简单的商业压力，你就会对好的 AI 而不是坏的 AI 产生巨大的偏见。然而，与改善人们生活的人工智能纪录片相比，施瓦辛格电影中的世界末日场景更受关注。&lt;/p>; &lt;p>;&lt;strong>;琼斯：我认为人们会被好故事所吸引——包含对手和斗争的故事，但最终，会有幸福的结局。这与您对人性的评论以及历史如何尽管有暴力和毁灭人类的倾向，但在某种程度上倾向于自我纠正是一致的。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;让我们举一个例子你知道的技术——GANs——通用对抗网络，如今已被用于虚假新闻和虚假信息的应用程序中。事实上，GANs 的发明目的与今天的用途相去甚远。&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 是的，GANs 这个名字是在 2014 年创建的，但是我们在 1990 年代初期就已经有了基本原则。 30 多年前，我称之为&lt;em>;人工好奇心&lt;/em>;。这是一种将创造力注入小型双网络系统的非常简单的方法。这种富有创造力的人工智能不只是试图盲目地模仿人类。相反，它正在发明自己的目标。让我解释一下：&lt;/p>; &lt;p>;你有两个网络。一个网络正在产生可以是任何东西、任何动作的输出。然后第二个网络正在查看这些行为，并试图预测这些行为的后果。一个动作可以移动机器人，然后发生某些事情，而另一个网络只是试图预测将要发生的事情。&lt;/p>; &lt;p>;现在我们可以通过减少第二个网络的预测误差来实现人工好奇心，其中，在同时，是第一个网络的奖励。第一个网络想要最大化它的奖励，所以它会发明一些动作，这些动作会导致第二个网络感到惊讶的情况，它还没有学会很好地预测。&lt;/p>; &lt;p>;在输出是的情况下假图像，第一个网络将尝试生成足以欺骗第二个网络的图像，第二个网络将尝试预测环境的反应：假图像或真实图像，并且它会尝试变得更好。第一个网络也将继续改进生成第二个网络无法预测其类型的图像。所以，他们互相争斗。第二个网络将继续减少它的预测误差，而第一个网络将尝试最大化它。&lt;/p>; &lt;p>;通过这个零和游戏，第一个网络越来越擅长产生这些看起来几乎实际的。因此，一旦您拥有文森特·梵高的一组有趣的图像，您就可以生成利用他的风格的新图像，而无需原艺术家亲自制作艺术品。&lt;/p>; &lt;p>;&lt;strong>;琼斯：我明白了梵高的例子如何应用于教育环境，有无数艺术家模仿著名画家风格的例子，但从这个例子中生成的图像可以在几秒钟内发生，这是另一项壮举。你知道这就是 GAN 的使用方式。今天更普遍的是生成图像或信息以故意愚弄人们的社会化支持。它还涉及处理对知识产权和版权的威胁的新危害，而法律尚未对此作出解释。从您的角度来看，这不是构思模型时的意图。您早期构想现在的 GAN 的动机是什么？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber&lt;/strong>;：我对 GAN 的旧动机实际上非常重要，它不是为了制造 deepfakes或假新闻，而是让 AI 产生好奇心并发明自己的目标，让它们探索环境并发挥创造力。&lt;/p>; &lt;p>;假设你有一个机器人执行一个动作，然后发生了一些事情，然后它执行另一个动作，等等，因为它想在环境中实现某些目标。例如，当电池电量低时，这会通过饥饿传感器触发“疼痛”，所以它想去充电站，不要撞到障碍物，这会触发其他疼痛传感器。它将寻求最小化痛苦（通过数字编码）。现在机器人有了一个朋友，第二个网络，它是一个世界模型——它是一个预测机器，可以学习预测机器人行为的后果。&lt;/p>; &lt;p>;一旦机器人有了一个好的世界模型，它可以将其用于规划。它可以用作对现实世界的模拟。然后它可以确定什么是好的动作序列。如果机器人想象这一系列动作，模型将预测它想要避免的很多痛苦。如果它在它的世界心智模型中播放这个替代动作序列，那么它会预测一个有益的情况，它会坐在充电站上，它的电池将再次充电。因此，它会更愿意执行后一个动作序列。&lt;/p>; &lt;p>;然而，在开始时，世界模型一无所知，因此我们如何激励第一个网络生成导致以下结果的实验帮助世界模型学习它不知道的东西的数据？这就是人工好奇心的意义所在。决斗的两个网络系统通过创建实验有效地探索未知环境，以便随着时间的推移，好奇的 AI 可以更好地了解环境的运作方式。这可以应用于各种环境，并且有医疗应用。&lt;/p>; &lt;p>;&lt;strong>;Jones：让我们谈谈未来。您曾说过，“&lt;/strong>;&lt;strong>;&lt;em>;传统人类不会在全宇宙传播智慧方面发挥重要作用。&lt;/em>;&lt;/strong>;&lt;strong>;”&lt;/strong>;&lt;/p >; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;让我们首先从概念上区分两种类型的 AI。第一类人工智能是由人类指导的工具。他们接受过培训，可以做一些特定的事情，比如准确地检测糖尿病或心脏病，并在它们发生之前预防攻击。在这些情况下，目标来自人类。更有趣的 AI 正在设定自己的目标。他们正在发明自己的实验并从中学习。他们的视野不断扩大，最终成为现实世界中越来越普遍的问题解决者。他们不受父母的控制，但他们学到的很多东西都是通过自己发明的实验。&lt;/p>; &lt;p>;例如，一个机器人正在旋转一个玩具，当它这样做时，视频会进入通过摄像头的眼睛，随着时间的推移而变化，它开始学习这个视频是如何变化的，并学习如果你以某种方式旋转玩具的 3D 特性如何生成特定的视频，最终，重力如何工作，以及玩具的物理原理如何世界运作。就像一个小科学家！&lt;/p>; &lt;p>;几十年来，我一直在预测，这种 AI 科学家的未来扩展版本将希望进一步扩大他们的视野，并最终去大多数物理资源所在的地方，以构建更多和更大的人工智能。当然，几乎所有这些资源都在远离地球的太空中，太空对人类充满敌意，但对适当设计的人工智能控制机器人和自我复制机器人工厂友好。所以在这里我们不再谈论我们的小生物圈；不，我们谈论的是宇宙中更大的其他部分。在数百亿年内，好奇的自我改进&quot;>;AI 将以人类无法实现的方式在可见宇宙中殖民&lt;/a>;。那些没有的人不会有影响。听起来像科幻小说，但自 1970 年代以来，我一直无法看到这种情况的合理替代方案，除了一场全球性灾难，例如一场全面的核战争，在它起飞之前阻止了它的发展。&lt;/p>; &lt;p >;&lt;strong>;Jones：这些可以设定自己目标的 AI 存在了多长时间——它们存在了多长时间？它们在多大程度上可以独立于人类交互？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber&lt;/strong>;：像这样的神经网络已经存在了 30 多年。我的第一个这种简单的对抗性神经网络系统是上面描述的 1990 年的那个。那里不需要老师；它只是一个在世界上跑来跑去的小代理，并试图发明新的实验，让自己的预测机器大吃一惊。&lt;/p>; &lt;p>;一旦它弄清楚了世界的某些地方，代理就会变得无聊并将继续进行更令人兴奋的实验。我提到的简单的 1990 系统有一定的局限性，但在过去的三十年里，我们也建立了更多 &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/artificial-curiosity-since-1990.html&quot; >;设定自己目标的复杂系统&lt;/a>;，我认为此类系统对于实现真正的智能至关重要。如果你只是模仿人，你永远无法超越人。因此，您真的必须让 AI 以一种没有人真正预先定义的方式自由探索世界上以前未探索过的区域。&lt;/p>; &lt;p>;&lt;strong>;Jones：今天在哪里进行这项工作？&lt;/strong>; &lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;如今，基于神经网络的人工好奇心的变体被用于学习以人类竞争方式玩电子游戏的代理。我们也开始将它们用于材料科学等领域的实验自动化设计。我敢打赌许多其他领域都会受到它的影响：化学、生物学、药物设计，应有尽有。然而，至少就目前而言，这些我喜欢称呼他们的人工智能科学家还不能与人类科学家竞争。&lt;/p>; &lt;p>;我不认为这种情况会持续下去，但目前，它是还是这样。当然，人工智能已经取得了很大进步。自 1997 年以来，出现了超人棋手，自 2011 年以来，通过我团队的 DanNet，出现了 &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/DanNet-triggers-deep-CNN- revolution-2011.html&quot;>;超人视觉模式识别器&lt;/a>;。但至少在目前，人类在其他方面要好得多，尤其是科学本身。在实验室中，我们有许多自主人工智能科学家的首批例子，但它们还不足以出现在公共空间的雷达屏幕上，公共空间目前更着迷于仅模仿人类并编写基于文本的简单系统&lt;/p>; &lt;p>;&lt;strong>;Jones：你谈到了这些可追溯到 30 年前的实验室实验中的无数实例，在这些实例中，这些自我驱动的代理人一旦做出决定、学习并继续前进，我学会了。而且我假设随着时间的推移，学习速度会变得更快。当这个最终被带出实验室并融入社会时，我们在谈论什么样的时间框架？&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>;这可能仍需要几个月甚至几年的时间:-) 无论如何，在不久的将来，我们可能会看到擅长设计实验的人工智能科学家能够发现新的、以前未知的物理定律。&lt;/p>; &lt;p>;一如既往，我们将从至少自 1941 年以来一直存在的旧趋势中获利：计算成本每十年下降 100 倍。&lt;/p>; &lt;p>;&lt;strong>;Jones：这种趋势如何影响 ChatGPT 等现代人工智能？&lt;/strong >;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber：&lt;/strong>; 或许你知道，最近所有著名的 AI 应用程序，如 ChatGPT 和类似模型，很大程度上都是基于上个千年发明的人工神经网络原理。它们现在运行良好的主要原因是每美元计算的惊人加速。&lt;/p>; &lt;p>;ChatGPT 由 Google 在 2017 年描述的名为“Transformer”的神经网络驱动。我对此感到高兴，因为四分之一世纪之前的 1991 年，我有一个特殊的 Transformer 变体，现在称为“&lt;a href=&quot;https://twitter.com/SchmidhuberAI/status/1576966129993797632?cxt=HHwWgMDSkeKVweIrAAAA&quot;>;Transformer线性化自注意力&lt;/a>;”。那时候，用它做不了多少事情，因为计算成本比现在高一百万倍。但是今天，人们可以在一半的互联网上训练这样的模型并取得更有趣的结果。&lt;/p>; &lt;p>;&lt;strong>;Jones：这种加速会持续多久？&lt;/strong>;&lt;/p>; &lt;p >;&lt;strong>;Schmidhuber：&lt;/strong>;没有理由相信在接下来的 30 年里，我们不会再有 100 万，这将非常重要。在不久的将来，我们将首次拥有许多计算能力与人脑相当的不那么昂贵的设备。 The physical limits of computation, however, are much further out so even if the trend of a factor of 100 every decade continues, the physical limits (of 1051 elementary instructions per second and kilogram of matter) won&#39;t be hit until, say, the mid-next century. Even in our current century, however, we&#39;ll probably have many machines that compute more than all 10 billion human brains collectively and you can imagine, everything will change then!&lt;/p>; &lt;p>;&lt;strong>;Jones: That is the big question. Is everything going to change? If so, what do you say to the next generation of leaders, currently coming out of college and university. So much of this change is already impacting how they study, how they will work, or how the future of work and livelihood is defined. What is their purpose and how do we change our systems so they will adapt to this new version of intelligence?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; For decades, people have asked me questions like that, because you know what I&amp;#39;m saying now, I have basically said since the 1970s, it&#39;s just that today, people are paying more attention because, back then, they thought this was science fiction.&lt;/p>; &lt;p>;They didn&amp;#39;t think that I would ever come close to achieving my crazy life goal of building a machine that learns to become smarter than myself such that I can retire. But now many have changed their minds and think it&amp;#39;s conceivable. And now I have two daughters, 23 and 25. People ask me: what do I tell them? They know that Daddy always said, “&lt;em>;It seems likely that within your lifetimes, you will have new types of intelligence that are probably going to be superior in many ways, and probably all kinds of interesting ways.&lt;/em>;” How should they prepare for that? And I kept telling them the obvious: &lt;strong>;Learn how to learn new things&lt;/strong>;! It&amp;#39;s not like in the previous millennium where within 20 years someone learned to be a useful member of society, and then took a job for 40 years and performed in this job until she received her pension. Now things are changing much faster and we must learn continuously just to keep up. I also told my girls that no matter how smart AIs are going to get, learn at least the basics of math and physics, because that&#39;s the essence of our universe, and anybody who understands this will have an advantage, and learn all kinds of new things more easily. I also told them that social skills will remain important, because most future jobs for humans will continue to involve interactions with other humans, but I couldn&#39;t teach them anything about that; they know much more about social skills than I do.&lt;/p>; &lt;p>;You touched on the big philosophical question about people&#39;s purpose. Can this be answered without answering the even grander question: What&#39;s the purpose of the entire universe?&lt;/p>; &lt;p>;We don&#39;t know. But what&#39;s happening right now might be connected to the unknown answer. Don&#39;t think of humans as the crown of creation. Instead view human civilization as part of a much grander scheme, an important step (but not the last one) on the path of the universe from very simple initial conditions towards more and more unfathomable complexity. Now it seems ready to take its &lt;a href=&quot;https://people.idsia.ch/%7Ejuergen/deep-learning-history.html#future&quot;>;next step, a step comparable to the invention of life itself over 3.5 billion years ago&lt;/a>;. Alas, don&#39;t worry, in the end, all will be good!&lt;/p>; &lt;p>;&lt;strong>;Jones: Let&#39;s get back to this transformation happening right now with OpenAI. There are many questioning the efficacy and accuracy of ChatGPT, and are concerned its release has been premature. In light of the rampant adoption, educators have banned its use over concerns of plagiarism and how it stifles individual development. Should large language models like ChatGPT be used in school?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; When the calculator was first introduced, instructors forbade students from using it in school. Today, the consensus is that kids should learn the basic methods of arithmetic, but they should also learn to use the “artificial multipliers” aka calculators, even in exams, because laziness and efficiency is a hallmark of intelligence. Any intelligent being wants to minimize its efforts to achieve things.&lt;/p>; &lt;p>;And that&amp;#39;s the reason why we have tools, and why our kids are learning to use these tools. The first stone tools were invented maybe 3.5 million years ago; tools just have become more sophisticated over time. In fact, humans have changed in response to the properties of their tools. Our anatomical evolution was shaped by tools such as spears and fire. So, it&amp;#39;s going to continue this way. And there is no permanent way of preventing large language models from being used in school.&lt;/p>; &lt;p>;&lt;strong>;Jones: And when our children, your children graduate, what does their future work look like?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; A single human trying to predict details of how 10 billion people and their machines will evolve in the future is like a single neuron in my brain trying to predict what the entire brain and its tens of billions of neurons will do next year. 40 years ago, before the WWW was created at CERN in Switzerland, who would have predicted all those young people making money as YouTube video bloggers?&lt;/p>; &lt;p>;Nevertheless, let&#39;s make a few limited job-related observations. For a long time, people have thought that desktop jobs may require more intelligence than skills trade or handicraft professions. But now, it turns out that it&amp;#39;s much easier to replace certain aspects of desktop jobs than replacing a carpenter, for example. Because everything that works well in AI is happening behind the screen currently, but not so much in the physical world.&lt;/p>; &lt;p>;There are now artificial systems that can read lots of documents and then make really nice summaries of these documents. That is a desktop job. Or you give them a description of an illustration that you want to have for your article and pretty good illustrations are being generated that may need some minimal fine-tuning. But you know, all these desktop jobs are much easier to facilitate than the real tough jobs in the physical world. And it&amp;#39;s interesting that the things people thought required intelligence, like playing chess, or writing or summarizing documents, are much easier for machines than they thought. But for things like playing football or soccer, there is no physical robot that can remotely compete with the abilities of a little boy with these skills. So, AI in the physical world, interestingly, is much harder than AI behind the screen in virtual worlds. And it&amp;#39;s really exciting, in my opinion, to see that jobs such as plumbers are much more challenging than playing chess or writing another tabloid story.&lt;/p>; &lt;p>;&lt;strong>;Jones: The way data has been collected in these large language models does not guarantee personal information has not been excluded. Current consent laws already are outdated when it comes to these large language models (LLM). The concern, rightly so, is increasing surveillance and loss of privacy. What is your view on this?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; As I have indicated earlier: are surveillance and loss of privacy inevitable consequences of increasingly complex societies? Super-organisms such as cities and states and companies consist of numerous people, just like people consist of numerous cells. These cells enjoy little privacy. They are constantly monitored by specialized &amp;quot;police cells&amp;quot; and &amp;quot;border guard cells&amp;quot;: Are you a cancer cell? Are you an external intruder, a pathogen? Individual cells sacrifice their freedom for the benefits of being part of a multicellular organism.&lt;/p>; &lt;p>;Similarly, for super-organisms such as nations. Over 5000 years ago, writing enabled recorded history and thus became its inaugural and most important invention. Its initial purpose, however, was to facilitate surveillance, to track citizens and their tax payments. The more complex a super-organism, the more comprehensive its collection of information about its constituents.&lt;/p>; &lt;p>;200 years ago, at least, the parish priest in each village knew everything about all the village people, even about those who did not confess, because they appeared in the confessions of others. Also, everyone soon knew about the stranger who had entered the village, because some occasionally peered out of the window, and what they saw got around. Such control mechanisms were temporarily lost through anonymization in rapidly growing cities but are now returning with the help of new surveillance devices such as smartphones as part of digital nervous systems that tell companies and governments a lot about billions of users. Cameras and drones etc. are becoming increasingly tinier and more ubiquitous. More effective recognition of faces and other detection technology are becoming cheaper and cheaper, and many will use it to identify others anywhere on earth; the big wide world will not offer any more privacy than the local village. Is this good or bad? Some nations may find it easier than others to justify more complex kinds of super-organisms at the expense of the privacy rights of their constituents.&lt;/p>; &lt;p>;&lt;strong>;Jones: So, there is no way to stop or change this process of collection, or how it continuously informs decisions over time? How do you see governance and rules responding to this, especially amid&lt;/strong>; &lt;a href=&quot;https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html&quot;>;&lt;strong>;Italy&#39;s ban on ChatGPT following&lt;/strong>;&lt;/a>; &lt;strong>;suspected user data breach and the more recent news about the&lt;/strong>; &lt;a href=&quot;https://www.reuters.com/technology/facebook-given-record-13-bln-fine-given-5-months-stop-eu-us-data-flows-2023-05-22/&quot;>;&lt;strong>;Meta&#39;s record $1.3billion fine&lt;/strong>;&lt;/a>; &lt;strong>;in the company&#39;s handling of user information?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; Data collection has benefits and drawbacks, such as the loss of privacy. How to balance those? I have argued for addressing this through data ownership in data markets. If it is true that data is the new oil, then it should have a price, just like oil. At the moment, the major surveillance platforms such as Meta do not offer users any money for their data and the transitive loss of privacy. In the future, however, we will likely see attempts at creating efficient data markets to figure out the data&amp;#39;s true financial value through the interplay between supply and demand.&lt;/p>; &lt;p>;Even some of the sensitive medical data should not be priced by governmental regulators but by patients (and healthy persons) who own it and who may sell or license parts thereof as micro-entrepreneurs in a healthcare data market.&lt;/p>; &lt;p>;Following a previous &lt;a href=&quot;https://www.swissre.com/institute/conferences/The-intelligence-behind-artificial-intelligence.html&quot;>;interview&lt;/a>;, I gave for one of the largest re-insurance companies , let&amp;#39;s look at the different participants in such a data market: patients, hospitals, data companies. (1) &lt;strong>;Patients&lt;/strong>; with a rare form of cancer can offer more valuable data than patients with a very common form of cancer. (2) &lt;strong>;Hospitals&lt;/strong>; and their machines are needed to extract the data, eg, through magnet spin tomography, radiology, evaluations through human doctors, and so on. (3) &lt;strong>;Companies&lt;/strong>; such as Siemens, Google or IBM would like to buy annotated data to make better artificial neural networks that learn to predict pathologies and diseases and the consequences of therapies. Now the market&#39;s invisible hand will decide about the data&#39;s price through the interplay between demand and supply. On the demand side, you will have several companies offering something for the data, maybe through an app on the smartphone (a bit like a stock market app). On the supply side, each patient in this market should be able to profit from high prices for rare valuable types of data. Likewise, competing data extractors such as hospitals will profit from gaining recognition and trust for extracting data well at a reasonable price. The market will make the whole system efficient through incentives for all who are doing a good job. Soon there will be a flourishing ecosystem of commercial data market advisors and what not, just like the ecosystem surrounding the traditional stock market. The value of the data won&#39;t be determined by governments or ethics committees, but by those who own the data and decide by themselves which parts thereof they want to license to others under certain conditions.&lt;/p>; &lt;p>;At first glance, a market-based system seems to be detrimental to the interest of certain monopolistic companies, as they would have to pay for the data - some would prefer free data and keep their monopoly. However, since every healthy and sick person in the market would suddenly have an incentive to collect and share their data under self-chosen anonymity conditions, there will soon be many more useful data to evaluate all kinds of treatments. On average, people will live longer and healthier, and many companies and the entire healthcare system will benefit.&lt;/p>; &lt;p>;&lt;strong>;Jones: Finally, what is your view on open source versus the private companies like Google and OpenAI? Is there a danger to supporting these private companies&#39; large language models versus trying to keep these models open source and transparent, very much like what LAION is doing?&lt;/strong>;&lt;/p>; &lt;p>;&lt;strong>;Schmidhuber:&lt;/strong>; I signed this &lt;a href=&quot;https://www.forbes.com/sites/hessiejones/2023/04/19/amid-growing-call-to-pause-ai-research-laion-petitions-governments-to-keep-agi-research-open-active-and-responsible/?sh=6973c08b62e3&quot;>;open letter by LAION&lt;/a>; because I strongly favor the open-source movement. And I think it&amp;#39;s also something that is going to challenge whatever big tech dominance there might be at the moment. Sure, the best models today are run by big companies with huge budgets for computers, but the exciting fact is that open-source models are not so far behind, some people say maybe six to eight months only. Of course, the private company models are all based on stuff that was created in academia, often in little labs without so much funding, which publish without patenting their results and open source their code and others take it and improved it.&lt;/p>; &lt;p>;Big tech has profited tremendously from academia; their main achievement being that they have scaled up everything greatly, sometimes even failing to credit the original inventors.&lt;/p>; &lt;p>;So, it&amp;#39;s very interesting to see that as soon as some big company comes up with a new scaled-up model, lots of students out there are competing, or collaborating, with each other, trying to come up with equal or better performance on smaller networks and smaller machines. And since they are open sourcing, the next guy can have another great idea to improve it, so now there&#39;s tremendous competition also for the big companies.&lt;/p>; &lt;p>;Because of that, and since AI is still getting exponentially cheaper all the time, I don&amp;#39;t believe that big tech companies will dominate in the long run. They find it very hard to compete with the enormous open-source movement. As long as you can encourage the open-source community, I think you shouldn&amp;#39;t worry too much. Now, of course, you might say if everything is open source, then the bad actors also will more easily have access to these AI tools. And there&amp;#39;s truth to that. But as always since the invention of controlled fire, it was good that knowledge about how technology works quickly became public such that everybody could use it. And then, against any bad actor, there&amp;#39;s almost immediately a counter actor trying to nullify his efforts. You see, I still believe in our old motto &amp;quot;AI∀&amp;quot; or &amp;quot;AI For All.&amp;quot;&lt;/p>; &lt;p>;&lt;strong>;Jones: Thank you, Juergen for sharing your perspective on this amazing time in history. It&#39;s clear that with new technology, the enormous potential can be matched by disparate and troubling risks which we&#39;ve yet to solve, and even those we have yet to identify. If we are to dispel the fear of a sentient system for which we have no control, humans, alone need to take steps for more responsible development and collaboration to ensure AI technology is used to ultimately benefit society. Humanity will be judged by what we do next.&lt;/strong>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/hardmaru&quot;>; /u/hardmaru &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13q6k4a </id><link href="https://www.reddit.com/r/MachineLearning/comments/13q6k4a/interview_with_juergen_schmidhuber_renowned/"/><updated> 2023-05-24T01:00:28+00:00</updated><published> 2023-05-24T01:00:28+00:00</published><title> Interview with Juergen Schmidhuber, renowned &#39;Father Of Modern AI&#39;, says his life&#39;s work won&#39;t lead to dystopia.</title></entry><entry><author><name> /u/wazazzz</name><uri> https://www.reddit.com/user/wazazzz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Hey all, just wanting to share this open source library I&#39;ve been working on that aims to makes LLMs experimentation (prompt chain engineering, fine tuning, variable integrated code generation, token probability/perplexity analysis) more accessible and easier to setup.&lt;/p>; &lt;p>;Open for feedback and collaboration!&lt;/p>; &lt;p>;&lt;a href=&quot;https://github.com/Pan-ML/panml&quot;>;https://github.com/Pan-ML/panml&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/wazazzz&quot;>; /u/wazazzz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/&quot;>;[link]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/&quot;>;[comments]&lt;/a>;&lt;/span>;</content><id> t3_13qj9ye </id><link href="https://www.reddit.com/r/MachineLearning/comments/13qj9ye/project_panml_a_high_level_python_library_for/"/><updated> 2023-05-24T11:49:13+00:00</updated><published> 2023-05-24T11:49:13+00:00</published><title> [Project] PanML, a high level Python library for fast LLM experimentation</title></entry></feed>