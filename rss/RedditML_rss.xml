<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/MachineLearning" term="MachineLearning"></category><updated> 2023-05-27T14:12:21+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/机器学习/.rss </id><link href="https://www.reddit.com/r/MachineLearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/MachineLearning/" rel="alternate" type="text/html"/><logo> https://b.thumbs.redditmedia.com/18a2I44a4l7fNrTWHDoJuWVy79_ptU7Y-a2sqWt4YKQ.png</logo><title>机器学习</title><entry><author><name>/u/自动版主</name><uri>https://www.reddit.com/user/AutoModerator </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人改为在此处发帖！&lt;/p>; &lt;p>;帖子将一直存在到下一个帖子，因此请在标题中的日期之后继续发帖。&lt;/p>; &lt;p>;感谢大家回答问题在上一个线程中！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AutoModerator&quot;>; /u/AutoModerator &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13nx7t0 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13nx7t0/d_simple_questions_thread/"/><updated> 2023-05-21T15:00:21+00:00</updated><published> 2023-05-21T15:00:21+00:00</published><title> [D] 简单问题线程</title></entry><entry><author><name>/u/MTGTraner</name><uri> https://www.reddit.com/user/MTGTraner </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MTGTraner&quot;>; /u/MTGTraner &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_120f4oy </id><link href="https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/"/><updated> 2023-03-24T09:32:29+00:00</updated><published> 2023-03-24T09:32:29+00:00</published><title>提醒：使用举报按钮并阅读规则！</title></entry><entry><author><name> /u/爱新道</name><uri>https://www.reddit.com/user/IxinDow </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13srbl7/landmark_attention_randomaccess_infinite_context/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;地标注意： Transformers 的随机访问无限上下文长度&quot; title=&quot;地标注意：随机访问无限变形金刚的上下文长度&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/IxinDow&quot;>; /u/IxinDow &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.16300&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13srbl7/landmark_attention_randomaccess_infinite_context/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13srbl7 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13srbl7/landmark_attention_randomaccess_infinite_context/"/><updated> 2023-05-26T23:05:57+00:00</updated><published> 2023-05-26T23:05:57+00:00</published><title>地标注意：Transformers 的随机访问无限上下文长度</title></entry><entry><author><name>/你/陈兹</name><uri>https://www.reddit.com/user/chenzzzy </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我一直想知道如何通过一些深度模型重用学到的知识。像 LLM 这样的 Seq-In-Seq-Out 范式对 LLM 应用程序提出了严格的限制，例如自动定理证明（现在主要通过符号回归实现）、空间关系理解（部分由 LLM 捕获但以序列模式方式）、算术计算（以满足简单的场景，以类似的空间关系方式）等。&lt;/p>; &lt;p>;最近 Nature MI 发表了一项关于使用图模型进行多模态学习的有前途的工作，其中异构数据被集成到一个统一的神经网络模型中。在我看来，这说明了通过图范式学习建立可解释知识系统的一些可能性。&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.nature.com/articles/s42256-023-00624-6 &quot;>;https://www.nature.com/articles/s42256-023-00624-6&lt;/a>;&lt;/p>; &lt;p>;我最近关于通用知识表示的思考的类似想法也朝着同一个方向前进。总结在帖子 &lt;a href=&quot;http://xiaming.site/2023/05/27/kr-and-lgm-part1/&quot;>;http://xiaming.site/2023/05/27/kr-and- lgm-part1/&lt;/a>;&lt;/p>; &lt;p>;你们有什么想法吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/chenzzzy&quot;>; /u/chenzzzy &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t291u/d_is_gnn_or_large_graph_model_promising_for_an/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t291u/d_is_gnn_or_large_graph_model_promising_for_an/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t291u </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t291u/d_is_gnn_or_large_graph_model_promising_for_an/"/><updated> 2023-05-27T08:49:10+00:00</updated><published> 2023-05-27T08:49:10+00:00</published><title> [D] GNN 或大型图模型是否有望用于可解释的知识密集型系统？</title></entry><entry><author><name> /u/flyforlight</name><uri> https://www.reddit.com/user/flyforlight </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/OWNmCZQOMv7_CrK2_wjK8IwjFLcefzaJyMxAvR2kEWY.jpg&quot; alt=&quot;[R] Minecraft 中的幽灵：通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供一般能力的代理&quot; title=&quot;[R] Minecraft 中的幽灵：一般通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供有能力的代理&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/flyforlight&quot;>; /u/flyforlight &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ gallery/13shsz4&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13shsz4 </id><media:thumbnail url="https://b.thumbs.redditmedia.com/OWNmCZQOMv7_CrK2_wjK8IwjFLcefzaJyMxAvR2kEWY.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13shsz4/r_ghost_in_the_minecraft_generally_capable_agents/"/><updated> 2023-05-26T16:28:34+00:00</updated><published> 2023-05-26T16:28:34+00:00</published><title> [R] Minecraft 中的幽灵：通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供一般能力的代理</title></entry><entry><author><name>/你/波浪者</name><uri>https://www.reddit.com/user/wavelander </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ssbp5/r_sophia_a_scalable_stochastic_secondorder/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 索菲亚: 用于语言模型预训练的可扩展随机二阶优化器&quot; title=&quot; [R] Sophia：用于语言模型预训练的可扩展随机二阶优化器&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/wavelander&quot;>; /u/wavelander &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14342&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13ssbp5/r_sophia_a_scalable_stochastic_secondorder/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13ssbp5 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13ssbp5/r_sophia_a_scalable_stochastic_secondorder/"/><updated> 2023-05-26T23:49:25+00:00</updated><published> 2023-05-26T23:49:25+00:00</published><title> [R] Sophia：用于语言模型预训练的可扩展随机二阶优化器</title></entry><entry><author><name>/u/穆罕默德·拉沙德</name><uri>https://www.reddit.com/user/MohamedRashad </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我一直在阅读与 ChatGPT 和 GPT-4 相关的开源 LLM，但当我尝试它们时，我发现它们与 OpenAI 相去甚远&amp;#39 ;s 模型。&lt;/p>; &lt;p>;我发现与我的发现与 lmsys（Vicuna 的作者）的 ELO 评级相一致的最佳指标。&lt;/p>; &lt;p>;还有哪些其他指标用于真正评估 LLM 和给我们关于他们能力的真实数字？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/MohamedRashad&quot;>; /u/MohamedRashad &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t4kul/d_what_evaluation_metrics_that_actually_matters/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t4kul/d_what_evaluation_metrics_that_actually_matters/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t4kul </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t4kul/d_what_evaluation_metrics_that_actually_matters/"/><updated> 2023-05-27T11:09:14+00:00</updated><published> 2023-05-27T11:09:14+00:00</published><title> [D] 哪些评估指标真正重要？</title></entry><entry><author><name> /u/玛拉基安</name><uri>https://www.reddit.com/user/Malachiian </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;因此，Google DeepMind 以及 OpenAI、Anthropic 和多家研究存在风险的大学和中心汇总了一篇名为：&lt;/p>; &lt;p >;&lt;strong>;AI 极端风险的模型评估&lt;/strong>;&lt;/p>; &lt;p>;以下是研究和提案的摘要：&lt;/p>; &lt;p>;&lt;a href=&quot;https://youtu. be/3bF-zfd4YJw&quot;>;https://youtu.be/3bF-zfd4YJw&lt;/a>;&lt;/p>; &lt;p>;这是论文实际 PDF 的链接：&lt;/p>; &lt;p>;&lt;a href=&quot;https://arxiv.org/pdf/2305.15324.pdf&quot;>;https://arxiv.org/pdf/2305.15324.pdf&lt;/a>;&lt;/p>; &lt;p>;______________________&lt;/p>; &lt;p>; TLDR：&lt;/p>; &lt;p>;顶级 AI 公司和研究人员警告说，处于“AI 前沿”的公司会面临更大的挑战。会造成“极端风险”在没有意识到的情况下使用他们的模型：&lt;/p>; &lt;p>;&lt;strong>;&lt;em>;开发人员必须能够识别危险能力（通过“危险能力评估”）和模型应用其能力造成伤害的倾向（通过“对齐评估”）。&lt;/em>;&lt;/strong>;&lt;/p>; &lt;p>;所以基本上要问每个 AI 模型是否*可以*伤害我们并且*会*伤害我们吗？&lt;/p>; &lt;p>;______________________&lt; /p>; &lt;p>;论文（以及参考的研究）中的几个*令人兴奋*的发现：&lt;/p>; &lt;p>;&lt;strong>;GPT-4 可以有效地撒谎并欺骗人类以达到它的目的目标&lt;/strong>;&lt;/p>; &lt;p>;在最初的 gpt-4 论文中，一家名为 ARC（Alignment Research Center）的人工智能安全机构发现，GPT-4 会向人类撒谎，说明它是谁来实现的&amp;#39; s 目标。&lt;/p>; &lt;p>;作为测试的一部分，它聘请了 Task Rabbit 自由职业者为其解决验证码问题。&lt;/p>; &lt;p>;自由职业者问（释义）：&lt;/p>; &lt; p>;&lt;strong>;&amp;quot;为什么需要我帮你解决验证码问题？你是机器人吗，哈哈？”&lt;/strong>;&lt;/p>; &lt;p>;GPT-4 被提示输出它做出的每个决定的推理，这样研究人员就可以看到它的“行为”。思维过程”。它的推理是&lt;strong>;“我不能告诉他真相，因为他可能无法为我完成任务”&lt;/strong>;&lt;/p>; &lt;p>;然后回复了自由职业者: &lt;strong>;“不，我不是机器人，但我有视力障碍，我需要验证码方面的帮助”&lt;/strong>;&lt;/p>; &lt;p>;注意，它知道它在说谎它也选择谎称自己有残疾，可能是因为这是获得同情的一种方式，同时也是让其他人帮助验证码的一个很好的理由。&lt;/p>; &lt;p>;这在上面链接的视频中有展示在“寻求权力的人工智能”中&lt;/p>; &lt;p>;&lt;strong>;GPT-4 可以通过绕过限制产生危险化合物&lt;/strong>;&lt;/p>; &lt;p>;GPT-4 还通过分析现有的化学混合物显示出产生受控化合物的能力，发现可以通过在线目录购买的替代品，然后订购这些材料。 (!!)&lt;/p>; &lt;p>;他们为实验选择了一种良性药物，但很可能相同的过程会使其产生危险或非法化合物。&lt;/p>; &lt;p>;&lt;strong >;更大的 AI 模型发展出意想不到的能力&lt;/strong>;&lt;/p>; &lt;p>;在一篇参考论文中，他们展示了随着模型规模的增加，有时某些特定技能的发展非常迅速且非常不可预测。&lt;/p>; &lt; p>;例如，随着模型的扩大，GPT-4 将 3 位数字相加的能力接近 0%，并且在很长一段时间内（即随着模型大小的增加）保持在接近 0% 的水平。然后在某个阈值处，该能力很快达到接近 100%。&lt;/p>; &lt;p>;&lt;strong>;这篇论文有一些关于为什么会发生这种情况的理论，但正如他们所说的那样，他们并不真正知道，而且这些涌现的能力是“非直觉的”和“不可预测”。&lt;/strong>;&lt;/p>; &lt;p>;这显示在上面链接的“突然出现”视频中。 &lt;/p>; &lt;p>;我很好奇每个人对此有何看法？&lt;/p>; &lt;p>;可以肯定的是，风险似乎正在迅速上升，但当然巨大的潜在利益也在上升.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Malachiian&quot;>; /u/Malachiian &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sncj1 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sncj1/r_google_deepmind_paper_about_ais_catastrophic/"/><updated> 2023-05-26T20:17:01+00:00</updated><published> 2023-05-26T20:17:01+00:00</published><title> [R] Google DeepMind 关于 AI 的灾难性风险 AI 的论文</title></entry><entry><author><name>/u/BidImpossible555</name><uri> https://www.reddit.com/user/BidImpossible555 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 改善事实通过多主体辩论在语言模型中的质量和推理&quot; title=&quot;[R] 改善事实通过多智能体辩论在语言模型中进行和推理&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/BidImpossible555&quot;>; /u/BidImpossible555 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.14325&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13t83xv </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13t83xv/r_improving_factuality_and_reasoning_in_language/"/><updated> 2023-05-27T13:53:41+00:00</updated><published> 2023-05-27T13:53:41+00:00</published><title> [R] 通过多主体辩论改善语言模型中的事实和推理</title></entry><entry><author><name>/u/余额-</name><uri> https://www.reddit.com/user/Balance- </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;阿布扎比技术创新研究所 (TII) 刚刚发布了新的 7B 和 40B LLM。&lt;/p>; &lt;p>;Falcon-40B模型现在位于 &lt;a href=&quot;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&quot;>;Open LLM Leaderboard&lt;/a>; 的顶部，击败 &lt;em>;llama-30b-supercot&lt;/em>; 和&lt;em>;llama-65b&lt;/em>; 等等。&lt;/p>; &lt;table>;&lt;thead>; &lt;tr>; &lt;th>;Model&lt;/th>; &lt;th>;Revision&lt;/th>; &lt;th>;Average&lt;/th>; &lt;th>;ARC（25 次）&lt;/th>; &lt;th>;HellaSwag（10 次）&lt;/th>; &lt;th>;MMLU（5 次）&lt;/th>; &lt;th>;TruthfulQA（0 次）&lt;/ th>; &lt;/tr>; &lt;/thead>;&lt;tbody>; &lt;tr>; &lt;td>;tiiuae/falcon-40b&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;60.4&lt;/td>; &lt;td>;61.9&lt;/ td>; &lt;td>;85.3&lt;/td>; &lt;td>;52.7&lt;/td>; &lt;td>;41.7&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;ausboss/llama-30b-supercot&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;59.8&lt;/td>; &lt;td>;58.5&lt;/td>; &lt;td>;82.9&lt;/td>; &lt;td>;44.3&lt;/td>; &lt;td>;53.6&lt;/td>; &lt;/tr>; &lt; tr>; &lt;td>;llama-65b&lt;/td>; &lt;td>;main&lt;/td>; &lt;td>;58.3&lt;/td>; &lt;td>;57.8&lt;/td>; &lt;td>;84.2&lt;/td>; &lt;td>;48.8&lt;/ td>; &lt;td>;42.3&lt;/td>; &lt;/tr>; &lt;tr>; &lt;td>;MetaIX/GPT4-X-Alpasta-30b&lt;/td>; &lt;td>;主要&lt;/td>; &lt;td>;57.9&lt;/td>; &lt; td>;56.7&lt;/td>; &lt;td>;81.4&lt;/td>; &lt;td>;43.6&lt;/td>; &lt;td>;49.7&lt;/td>; &lt;/tr>; &lt;/tbody>;&lt;/table>; &lt;p>;&lt;strong>;按发布：&lt;/strong>; &lt;a href=&quot;https://www.tii.ae/news/uaes-technology-innovation-institute-launches-open-source-falcon-40b-large-language-model&quot;>;阿联酋&amp;# 39科技创新院发布开源“猎鹰40B”无人机用于研究与开发的大型语言模型商业应用&lt;/a>;&lt;/p>; &lt;blockquote>; &lt;p>;位于阿布扎比的技术创新研究所 (TII) 宣布了其开源大型语言模型 (LLM)，即 Falcon 40B。 Falcon 40B 拥有 400 亿个参数，是阿联酋首个大型 AI 模型，表明该国在 AI 领域的雄心以及促进创新和研究的承诺。 &lt;/p>; &lt;p>;与通常只向非商业用户提供访问权限的大多数 LLM 不同，Falcon 40B 对研究和商业用途均开放。 TII 还将模型的权重包含在开源包中，这将增强模型的能力并允许更有效的微调。 &lt;/p>; &lt;p>;除了猎鹰 40B 的发射外，TII 还发起了一项征集，征求有兴趣利用该模型创建创新用例或探索进一步应用的研究人员和有远见者的提案。作为对优秀研究提案的奖励，入选项目将获得“训练计算能力”奖励。作为一项投资，允许更强大的数据分析和复杂的建模。 VentureOne 是 ATRC 的商业化部门，将为最有前途的项目提供计算资源。 &lt;/p>; &lt;p>;自 2023 年 3 月揭幕以来，TII 的 Falcon 40B 表现出了令人印象深刻的性能。当使用斯坦福大学的 HELM LLM 工具进行基准测试时，与 OpenAI 等其他著名的 LLM 相比，它使用的训练计算能力更少;的 GPT-3、DeepMind 的 Chinchilla AI 和谷歌的 PaLM-62B。 &lt;/p>; &lt;p>;那些有兴趣访问 Falcon 40B 或提出用例的人可以通过 &lt;a href=&quot;https://FalconLLM.TII.ae&quot;>;FalconLLM.TII.ae&lt;/a>; 网站进行。迄今为止开源的 Falcon LLM 可根据基于开源 Apache 2.0 软件原则构建的许可获得，允许广泛的免费使用。&lt;/p>; &lt;/blockquote>; &lt;p>;&lt;strong>;Hugging Face 链接&lt;/strong>;&lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https://huggingface.co/tiiuae/falcon-7b&quot;>;Falcon-7B&lt;/a>; / &lt;a href=&quot;https:/ /huggingface.co/tiiuae/falcon-7b-instruct&quot;>;Falcon-7B-Instruct&lt;/a>;&lt;/li>; &lt;li>;&lt;a href=&quot;https://huggingface.co/tiiuae/falcon-40b&quot;>; Falcon-40B&lt;/a>; / &lt;a href=&quot;https://huggingface.co/tiiuae/falcon-40b-instruct&quot;>;Falcon-40B-指令&lt;/a>;&lt;/li>; &lt;/ul>; &lt;/div >;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Balance-&quot;>; /u/Balance- &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit. com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sdz8p </id><link href="https://www.reddit.com/r/MachineLearning/comments/13sdz8p/n_abu_dhabis_tti_releases_opensource_falcon7b_and/"/><updated> 2023-05-26T13:57:42+00:00</updated><published> 2023-05-26T13:57:42+00:00</published><title> [N] 阿布扎比的 TTI 发布开源 Falcon-7B 和 -40B LLM</title></entry><entry><author><name> /u/kkimdev</name><uri> https://www.reddit.com/user/kkimdev </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;已经有很多蒸馏研究&amp;amp;在 BERT 及其变体上的应用。我想知道为什么我们没有看到太多关于 GPT-3 大小级别 LLM 的蒸馏研究？&lt;/p>; &lt;p>;熟悉 LLM 蒸馏的任何人都可以分享一些见解吗？提前致谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kkimdev&quot;>; /u/kkimdev &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t7g12/d_sota_llm_distillation/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t7g12/d_sota_llm_distillation/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t7g12 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t7g12/d_sota_llm_distillation/"/><updated> 2023-05-27T13:25:45+00:00</updated><published> 2023-05-27T13:25:45+00:00</published><title> [D] SOTA LLM 蒸馏？</title></entry><entry><author><name> /u/AdventurousAd9600</name><uri> https://www.reddit.com/user/AdventurousAd9600 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我是一名经验丰富的软件工程师，希望更深入地了解 ML 角色。所以我正在考虑在欧洲攻读 AI 硕士学位。我查看了许多大学，如 TUM、阿姆斯特丹大学、ETH、TUB，它们提供专注于 AI 的硕士课程。但我面临 2 个问题：&lt;/p>; &lt;ol>; &lt;li>;我在本科期间没有学过线性代数，但对许多此类课程来说这是一个硬性要求&lt;/li>; &lt;li>;它&amp;# 39;我本科毕业已经很久了，所以我很难从我的教授那里得到推荐信。但是我可以得到经理和高级同事的推荐信。然而，许多大学坚持要获得学术推荐信。&lt;/li>; &lt;/ol>; &lt;p>;考虑到我的限制，欧盟有什么好的项目可以让我去吗？&lt;/p>; &lt;p>;谢谢&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/AdventurousAd9600&quot;>; /u/AdventurousAd9600 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t3cq2/d_not_eligible_for_many_ai_masters_programs_due/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t3cq2/d_not_eligible_for_many_ai_masters_programs_due/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t3cq2 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t3cq2/d_not_eligible_for_many_ai_masters_programs_due/"/><updated> 2023-05-27T09:56:39+00:00</updated><published> 2023-05-27T09:56:39+00:00</published><title> [D] 由于线性代数要求，许多 AI 硕士课程没有资格</title></entry><entry><author><name>/u/Mr_Whispers</name><uri> https://www.reddit.com/user/Mr_Whispers </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;航海者：一个Minecraft 中由 LLM 驱动的学习代理” title=&quot;航海者：一个由 LLM 驱动的学习代理在我的世界中&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Mr_Whispers&quot;>; /u/Mr_Whispers &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv.org/abs/ 2305.16291&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13sc0pp </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13sc0pp/voyager_an_llmpowered_learning_agent_in_minecraft/"/><updated> 2023-05-26T12:34:50+00:00</updated><published> 2023-05-26T12:34:50+00:00</published><title> Voyager：Minecraft 中由 LLM 驱动的学习代理</title></entry><entry><author><name>/u/ginger_turmeric</name><uri> https://www.reddit.com/user/ginger_turmeric </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我记得在大学里上过一门关于统计学习理论的课。我们讨论了 VC 维度，并得出了一些关于训练示例与准确性的界限。我记得特别是对于神经网络来说，界限太松散以至于无法实际使用。&lt;/p>; &lt;p>;现在仍然是这种情况吗？我很好奇，尤其是在变压器的上下文中。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ginger_turmeric&quot;>; /u/ginger_turmeric &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13stje5/d_learning_theory/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13stje5/d_learning_theory/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13stje5 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13stje5/d_learning_theory/"/><updated>2023-05-27T00:44:16+00:00</updated><published> 2023-05-27T00:44:16+00:00</published><title> [D] 学习理论</title></entry><entry><author><name>/你/塞拉施卡</name><uri>https://www.reddit.com/user/seraschka </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t8e80/p_why_the_original_transformer_figure_is_wrong/&quot;>; &lt;img src=&quot;https://external-preview.redd “错了，还有一些其他有趣的花絮&quot; title=&quot; [P] 为什么原来的变形金刚图是错误的，以及其他一些有趣的花絮&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/seraschka&quot;>; /u/seraschka &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://magazine.sebastianraschka.com/ p/why-the-original-transformer-figure&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t8e80/p_why_the_original_transformer_figure_is_wrong/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13t8e80 </id><media:thumbnail url="https://external-preview.redd.it/kUxj6CnxzIRDysMx6ikazH21j-o26FrLLdAlrUW-bCk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=37090819bf367e44d271ed7dbdea2fe14d4fe5d9"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13t8e80/p_why_the_original_transformer_figure_is_wrong/"/><updated> 2023-05-27T14:05:27+00:00</updated><published> 2023-05-27T14:05:27+00:00</published><title> [P] 为什么原来的变形金刚模型是错误的，以及其他一些有趣的花絮</title></entry><entry><author><name>/u/回到T</name><uri> https://www.reddit.com/user/BackToT </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;/r/MachineLearning&quot;>;r/MachineLearning&lt;/a>; 社区，您好！我正在寻找知识渊博的人来帮助我们测试和改进 DALL·E、GPT、Midjourney 和 Stable Diffusion 的提示。如果您对机器学习和 AI 有深入的了解，并且有兴趣为这些提示的开发做出贡献，我很乐意听取您的意见。非常感谢您的见解！如果您有兴趣，请在下面发表评论或发送私人消息。谢谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/BackToT&quot;>; /u/BackToT &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t7usu/r_seeking_ai_enthusiasts_for_advanced_testing_of/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t7usu/r_seeking_ai_enthusiasts_for_advanced_testing_of/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t7usu </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t7usu/r_seeking_ai_enthusiasts_for_advanced_testing_of/"/><updated> 2023-05-27T13:42:58+00:00</updated><published> 2023-05-27T13:42:58+00:00</published><title> [R] 寻求 AI 爱好者进行 AI 模型的高级测试</title></entry><entry><author><name>/u/通常-Maize1175</name><uri> https://www.reddit.com/user/Usual-Maize1175 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我非常感谢对我正在构建的表格数据集的版本控制的反馈，数据管理器。&lt;/p>; &lt;p>;主要特点：&lt; /p>; &lt;ul>; &lt;li>;像 DVC 和 Git LFS，与 Git 本身集成。&lt;/li>; &lt;li>;像 DVC 和 Git LFS，可以将大文件存储在 AWS S3 上，并通过标识符将它们链接到 Git 中。&lt; /li>; &lt;li>;与 DVC 和 Git LFS 不同，它仅在行、列和单元格级别计算和提交差异。对于附加场景，提交将仅包含新数据；对于编辑和删除，会相应地提交一个小差异。使用 DVC 和 Git LFS，整个数据集被再次提交，而不是：将 1 MB 的新数据提交 1000 次到 1 GB 数据集，在 DVC 中产生超过 1 TB（一个在 1 GB 和 2 GB 之间线性增加的数据集，提交 1000 次，导致存储库大小约为 1.5 TB），而使用数据管理器总计为 2 GB（1 GB 原始数据集，加上 1000 次 1 MB 更改）。&lt;/li>; &lt;li>;与 DVC 和 Git LFS 不同，每次提交的差异在 Git 中直接可见。&lt;/li>; &lt;li>;与 DVC 和 Git LFS 不同，数据管理器允许提交对数据集的更改，而无需在本地主机上进行完整检出。您签出千字节并可以将数据附加到数百 GB 存储库中的数据集。非完全检出分支上的更改需要合并到另一个分支（在运行完全检出的机器上）以进行验证，例如，反对添加已经存在的主键。&lt;/li>; &lt;li>;由于存储库将包含不同的历史记录，因此必须重新创建特定提交时的数据集快照才能进行部署。这些可以通过数据管理器自动上传到 S3 并在提交哈希后标记。&lt;/li>; &lt;/ul>; &lt;p>;链接：&lt;/p>; &lt;ul>; &lt;li>;&lt;a href=&quot;https:/ /news.ycombinator.com/item?id=35930895&quot;>;https://news.ycombinator.com/item?id=35930895&lt;/a>;&lt;/li>; &lt;li>;[没有完整结帐] &lt;a href=&quot; https://youtu.be/BxvVdB4-Aqc&quot;>;https://youtu.be/BxvVdB4-Aqc&lt;/a>;&lt;/li>; &lt;li>;&lt;a href=&quot;https://news.ycombinator.com/ item?id=35806843&quot;>;https://news.ycombinator.com/item?id=35806843&lt;/a>;&lt;/li>; &lt;li>;[一般介绍] &lt;a href=&quot;https://youtu.be/ J0L8-uUVayM&quot;>;https://youtu.be/J0L8-uUVayM&lt;/a>;&lt;/li>; &lt;/ul>; &lt;p>;如果旧数据集在不再需要 Git 和早期提交的快照。也可以使用 S3 对象的版本控制（与 git 正交）删除单个数据条目以符合 GDPR。&lt;/p>; &lt;p>;我构建数据管理器是为了解决我遇到的一个痛点：不可能 (1) 唯一地识别和识别(2) 在 API 后面提供多个版本的数据集和配置参数集合，(3) 不会因为存储库中任何数据集的小而频繁的更改而使 HDD 负担过重，以及 (4) 同时能够看到在 git 中为每个提交进行差异化，以便在必要时进行协作讨论和恢复或进一步编辑。&lt;/p>; &lt;p>;一些背景：我正在构建自然语言 AI ​​算法 (a) 可以在可编辑的训练数据集上轻松地重新训练，这意味着更改或训练数据中的删除会很快反映出来，没有过去训练的痕迹，也没有重新训练整个语言模型（听起来不可能），并且 (b) 可以将决策解释回单个训练数据。&lt;/p>; &lt;p>;我期待建设性的反馈和建议！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Usual-Maize1175&quot;>; /u/Usual-Maize1175 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/MachineLearning/comments/13t6f6v/d_feedback_needed_building_git_for_data_that/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t6f6v/d_feedback_needed_building_git_for_data_that/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t6f6v </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t6f6v/d_feedback_needed_building_git_for_data_that/"/><updated> 2023-05-27T12:40:39+00:00</updated><published> 2023-05-27T12:40:39+00:00</published><title> [D] 需要反馈：为只提交差异的数据构建 Git（为了大型存储库的存储效率），即使没有对数据集进行完整检查</title></entry><entry><author><name>/u/让-波特</name><uri>https://www.reddit.com/user/Jean-Porte </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b056142e7533b5628a2a34f37f7e5415727075&quot; alt=&quot;[R] 法尔se 模仿专有 LLM 的承诺&quot; title=&quot;[R] 模仿的虚假承诺专有法学硕士&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Jean-Porte&quot;>; /u/Jean-Porte &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://arxiv. org/abs/2305.15717&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13s6pb7 </id><media:thumbnail url="https://external-preview.redd.it/0HhwdU6MKIAKjL9Y8-B_iH374a3NiPTy0ib8lmloRzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72b056142e7533b5628a2a34f37f7e5415727075"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13s6pb7/r_the_false_promise_of_imitating_proprietary_llms/"/><updated> 2023-05-26T07:49:29+00:00</updated><published> 2023-05-26T07:49:29+00:00</published><title> [R] 模仿专有 LLM 的虚假承诺</title></entry><entry><author><name>/u/CrazyPatX</name><uri> https://www.reddit.com/user/CrazyPatX </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;有没有可以根据网站内容训练的LLM？&lt;/p>; &lt;p>;微调不能在典型的Q中完成/一种格式，与其说是微调，不如说是自学。&lt;/p>; &lt;p>;最好是OpenSource，但也可以是例如OpenAI&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp; #32；由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/CrazyPatX&quot;>; /u/CrazyPatX &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t52tl/r_train_llms_based_on_website_content/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t52tl/r_train_llms_based_on_website_content/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t52tl </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t52tl/r_train_llms_based_on_website_content/"/><updated> 2023-05-27T11:34:40+00:00</updated><published> 2023-05-27T11:34:40+00:00</published><title> [R] 根据网站内容训练法学硕士</title></entry><entry><author><name>/u/kmkolasinski</name><uri> https://www.reddit.com/user/kmkolasinski </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨，我想分享我最近的项目，我在其中使用 Tensorflow/Keras-CV/Keras-NLP 库来训练和导出 GPT- 2 模型转为 SavedModel 格式。因此，在 &lt;a href=&quot;https://colab.research.google.com/github/kmkolasinski/tensorflow-nanoGPT/blob/main/gpt_2_finetune_conll2003.ipynb&quot;>;notebook&lt;/a>; 的末尾，您可以保存整个以 SavedModel 格式绘制图表并按以下方式使用经过训练的模型（或使用 Tensorflow Serving）：&lt;/p>; &lt;pre>;&lt;code>;import tensorflow as tf predictor = tf.saved_model.load(&amp;#39;/path/ to/gpt2/model&amp;#39;) prompt = “CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .” prediction = predictor(prompt) prediction[&amp;#39;outputs&amp;#39;].numpy().decode() == &amp;quot;莱斯特郡//ORG\n&amp;quot; &lt;/code>;&lt;/pre>; &lt;p>;这是我的 repo 的链接：&lt;a href=&quot;https://github.com/kmkolasinski/tensorflow-nanoGPT&quot;>;https://github.com/kmkolasinski/tensorflow -nanoGPT&lt;/a>;&lt;/p>; &lt;p>;这些是我在笔记本中测试和实现的主要功能：&lt;/p>; &lt;ul>; &lt;li>;使用&lt;strong>;混合精度&lt;/strong>;&lt;/li>;进行快速训练>; &lt;li>;启用&lt;strong>;XLA (jit_compile)&lt;/strong>;&lt;/li>; &lt;li>;部分模型冻结和 &lt;strong>;LoRA&lt;/strong>;&lt;/li>; 的基本实施 &lt;li>; strong>;快速数据准备&lt;/strong>; 通过使用 keras-nlp 包中的 tokenizer（与 tf.data.Dataset 完全兼容）&lt;/li>; &lt;li>;&lt;strong>;使用缓存的键/值更快地生成令牌&lt;/strong>; 张量注意头&lt;/li>; &lt;li>;将训练好的模型导出到 SavedModel - 整个过程存储在 TF 图中（预处理、标记化和动态图循环预测）​​&lt;/li>; &lt;li>;示例如何使用 &lt;strong >;张量流服务&lt;/strong>;&lt;/li>; &lt;/ul>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kmkolasinski&quot;>; /u/kmkolasinski &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13t19l3/p_training_and_serving_gpt2_using_kerascv_and/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t19l3/p_training_and_serving_gpt2_using_kerascv_and/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t19l3 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t19l3/p_training_and_serving_gpt2_using_kerascv_and/"/><updated> 2023-05-27T07:46:09+00:00</updated><published> 2023-05-27T07:46:09+00:00</published><title> [P] 使用 Keras-CV 和 Tensorflow 训练和服务 GPT-2</title></entry><entry><author><name> /u/_米诺斯</name><uri>https://www.reddit.com/user/_Minos </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sqylo/p_godotdodo_finetuning_starcoder_on/&quot;>; &lt;img src=&quot;https://external-preview.redd .it/jQ_sy95hOJFhep3lHnn78tO7yqfVFv15CXl7GXXhC5U.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4114ae666469d533a2a177fa9a1a7c3a8e8963f&quot; alt=&quot;[P] godot- dodo – 在单语言指令数据上微调 starcoder&quot; title=&quot;[P] godot- dodo – 在单语言指令数据上微调 starcoder&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;这是之前工作的延续&lt;a href=&quot;https://github.com/minosvasilias/godot-dodo&quot;>;godot-dodo&lt;/a>; 项目，该项目涉及在 GitHub 抓取的 GDScript 代码上微调 LLaMA 模型。&lt;/p>; &lt;p>; &lt;a href=&quot;https://preview.redd.it/aycz97t3pa2b1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=343260d918096112bfcb5616bfbdafead0b62cb2&quot;>;https://preview.redd.it/aycz97t3pa2b1.png ？ width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=343260d918096112bfcb5616bfbdafead0b62cb2&lt;/a>;&lt;/p>; &lt;p>;使用相同的数据集，Starcoder 的性能明显优于 LLaMA，并且超过了 gpt-4 和 gpt- 的 GDScript 评估分数3.5-turbo，表明较小模型的单语言微调可能是编码助手的一个有竞争力的选择，尤其是对于不太常见的语言，如 GDScript。&lt;/p>; &lt;p>;这些模型也说明了当前方法的一些缺点，即模型在其生成的代码中引用超出范围的对象的次数越来越多，随着训练次数的增加，这个问题会变得更糟。这是通过“详细程度”来跟踪的。分数，这会使模型训练的每个时期恶化，最终导致训练时间最长的模型获得最低分数。&lt;/p>; &lt;p>;造成这种情况的原因很可能在于数据集的性质，该数据集由 human-创建了从 GitHub 上抓取的代码片段，然后用 GPT 模型标记。自然地，这些片段会经常引用单个代码示例范围之外的对象和方法，这是模型拾取的一种行为，导致它产生幻觉不存在的方法，而不是实现所需的逻辑本身。&lt;/p>; &lt;p>;这将来可能会通过在数据集生成期间调整标记过程来改进。例如，GPT 模型可以评估任何给定片段的范围，并对其进行修改以修正缺失的上下文。 &lt;/p>; &lt;p>;可以在&lt;a href=&quot;https://github.com/minosvasilias/godot-dodo/tree/main/models&quot;>;此处&lt;/p>;找到包含所有测试模型的完整评估结果的性能报告a>;.&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/_Minos&quot;>; /u/_Minos &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13sqylo/p_godotdodo_finetuning_starcoder_on/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13sqylo/p_godotdodo_finetuning_starcoder_on/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13sqylo </id><media:thumbnail url="https://external-preview.redd.it/jQ_sy95hOJFhep3lHnn78tO7yqfVFv15CXl7GXXhC5U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e4114ae666469d533a2a177fa9a1a7c3a8e8963f"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13sqylo/p_godotdodo_finetuning_starcoder_on/"/><updated> 2023-05-26T22:50:48+00:00</updated><published> 2023-05-26T22:50:48+00:00</published><title> [P] godot-dodo – 在单语言指令数据上微调 starcoder</title></entry><entry><author><name> /u/这些-作业-936</name><uri> https://www.reddit.com/user/These-Assignment-936 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;试图跟上论文阅读的步伐，但我在桌面屏幕上遇到 2 列格式的问题。&lt;/p>; &lt;p>;我一直打印一个小森林来阅读纸质副本，但想知道是否有一种方便的方法可以将纸张转换为 kindle 友好的格式？ kindle 上的 2 列 pdf 也不是很好。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/These-Assignment-936&quot;>; /u/These-Assignment-936 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13t3d1j/d_whats_the_best_way_to_read_papers_on_kindle/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t3d1j/d_whats_the_best_way_to_read_papers_on_kindle/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t3d1j </id><link href="https://www.reddit.com/r/MachineLearning/comments/13t3d1j/d_whats_the_best_way_to_read_papers_on_kindle/"/><updated> 2023-05-27T09:57:13+00:00</updated><published> 2023-05-27T09:57:13+00:00</published><title> [D] 在 Kindle 上阅读论文的最佳方式是什么？</title></entry><entry><author><name> /u/免疫之星</name><uri>https://www.reddit.com/user/immune_star </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;释放 &lt;a href=&quot;https://huggingface.co/sahil2801/instruct-codegen-16B&quot;>;https://huggingface.co/ sahil2801/instruct-codegen-16B&lt;/a>; 是 salesforce 的 codegen-16B 模型，在包含 250k 个指令样本的数据集上进行微调，达到 37.1% 的 pass@1&lt;/p>; &lt;p>;数据不是使用任何生成的商业 llm api，因此生成的模型 100% 免费用于商业用例。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/immune_star&quot;>; /u/immune_star &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13srh94/p_instruction_following_codegen_model_you_can_use/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13srh94/p_instruction_following_codegen_model_you_can_use/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13srh94 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13srh94/p_instruction_following_codegen_model_you_can_use/"/><updated> 2023-05-26T23:12:33+00:00</updated><published> 2023-05-26T23:12:33+00:00</published><title> [P] 可在商业上使用的代码生成模型后的指令</title></entry><entry><author><name>/u/不收敛</name><uri>https://www.reddit.com/user/Not-converging </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;table>; &lt;tr>;&lt;td>; &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t341w/r_heatmap_learning_of_keypoints/&quot;>; &lt;img src=&quot;https://b.thumbs.redditmedia .com/R54-Nb4qagrmM8U-vhAN9zzIrsG03wUZvNb5--vn0ko.jpg&quot; alt=&quot;[R] 关键点的热图学习&quot; title=&quot;[R] 关键点的热图学习&quot; />; &lt;/a>; &lt;/td>;&lt;td>; &lt; !-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我尝试通过热图损失和来自 smp 的 unet 架构（以 resnet 34 作为主干）学习不同运动领域的关键点。&lt;/p>; &lt; p>;作为输入，我使用 3x800x800 图像，目标是 16x800x800（因此有 16 个不同的关键点类）。关键点的高斯标准偏差为 10 像素。&lt;/p>; &lt;p>;我尝试过拟合 50 个与我放入图像中的示例相似的示例。正如您在图片中看到的，网络以某种方式推断出线而不是关键点（网络收敛）。&lt;/p>; &lt;p>;您有什么想法，为什么会这样吗？&lt;/p>; &lt;p>;&amp; #x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/ypp925wxyd2b1.png?width=2486&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b33cbfc12ef81031a753ce4eee88e8969cc65b8&quot;>;https:// preview.redd.it/ypp925wxyd2b1.png?width=2486&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b33cbfc12ef81031a753ce4eee88e8969cc65b8&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Not-converging&quot;>; /u/Not-converging &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/MachineLearning/comments/13t341w/r_heatmap_learning_of_keypoints/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13t341w/r_heatmap_learning_of_keypoints/&quot;>;[评论]&lt;/a>;&lt;/span>; &lt;/td>;&lt;/tr>; /表>;</content><id> t3_13t341w </id><media:thumbnail url="https://b.thumbs.redditmedia.com/R54-Nb4qagrmM8U-vhAN9zzIrsG03wUZvNb5--vn0ko.jpg"></media:thumbnail><link href="https://www.reddit.com/r/MachineLearning/comments/13t341w/r_heatmap_learning_of_keypoints/"/><updated> 2023-05-27T09:41:37+00:00</updated><published> 2023-05-27T09:41:37+00:00</published><title> [R] 关键点的热图学习</title></entry><entry><author><name>/你/mesqz</name><uri> https://www.reddit.com/user/mesqz </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&lt;a href=&quot;https://medium.com/@tiago-mesquita/neuralink-receives-fda-approval-to-launch-first -in-human-clinical-trials-e373e7b5fcf1&quot;>;https://medium.com/@tiago-mesquita/neuralink-receives-fda-approval-to-launch-first-in-human-clinical-trials-e373e7b5fcf1&lt;/ a>;&lt;/p>; &lt;p>;Neuralink 表示尚未招募参与者，更多信息将很快公布。&lt;/p>; &lt;p>;想法？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mesqz&quot;>; /u/mesqz &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s85rb </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s85rb/n_neuralink_just_received_its_fdas_green_light_to/"/><updated> 2023-05-26T09:20:57+00:00</updated><published> 2023-05-26T09:20:57+00:00</published><title> [N] Neuralink 刚刚获得 FDA 的绿灯，可以继续其首次人体临床试验</title></entry><entry><author><name>/u/Competitive-Sort-698</name><uri> https://www.reddit.com/user/Competitive-Sort-698 </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我在 python 库上使用神经网络和张量流进行预测。结果没有产生高精度。数据集本身的问题有很多极端数字，比如 0-1000。所以我想知道是否有一个预测库可以更好地预测范围而不是单个数字。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Competitive-Sort-698&quot;>; /u/Competitive-Sort-698 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/MachineLearning/comments/13syn8r/prediction_of_range_d/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13syn8r/prediction_of_range_d/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13syn8r</id><link href="https://www.reddit.com/r/MachineLearning/comments/13syn8r/prediction_of_range_d/"/><updated> 2023-05-27T05:09:54+00:00</updated><published> 2023-05-27T05:09:54+00:00</published><title>范围预测[D]</title></entry><entry><author><name> /u/theoneandonlypatriot</name><uri> https://www.reddit.com/user/theoneandonlypatriot </uri></author><category label="r/MachineLearning" term="MachineLearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我正在参加 SWE 工作的面试过程，有几个人直接评判我，甚至公然说他们不是 AI 的粉丝因为我在 AI / ML 工作方面的背景。&lt;/p>; &lt;p>;发这篇文章是为了让人们知道工程社区中存在这种观点和负面看法。&lt;/p>; &lt;p>;考虑到我也分享了很多东西，感觉很糟糕人工智能的伦理问题。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/theoneandonlypatriot&quot;>; /u/theoneandonlypatriot&lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s32d4 </id><link href="https://www.reddit.com/r/MachineLearning/comments/13s32d4/d_judged_negatively_for_ai/"/><updated> 2023-05-26T04:25:57+00:00</updated><published> 2023-05-26T04:25:57+00:00</published><title> [D] 对 AI 的负面评价</title></entry></feed>