<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/deeplearning" term="deeplearning"></category><updated> 2023-05-26T22:13:39+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/深度学习/.rss </id><link href="https://www.reddit.com/r/deeplearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/deeplearning/" rel="alternate" type="text/html"/><title>深度学习</title><entry><author><name>/u/公平竞争</name><uri>https://www.reddit.com/user/Play-Equal</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Play-Equal&quot;>; /u/Play-Equal &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://v. redd.it/fvf4ka4qs62b1&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s8e6h/building_a_3_layered_neural_network_from_scratch/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s8e6h </id><link href="https://www.reddit.com/r/deeplearning/comments/13s8e6h/building_a_3_layered_neural_network_from_scratch/"/><updated> 2023-05-26T09:35:36+00:00</updated><published> 2023-05-26T09:35:36+00:00</published><title>仅使用 NumPy 从头开始​​构建 3 层神经网络 https://youtu.be/w7Hn3jmbj1A 如需完整视频，请参阅评论部分</title></entry><entry><author><name>/u/笑佛_</name><uri> https://www.reddit.com/user/SmilingBuddha_</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，我对深度学习中使用的不同优化器有所了解，但我想深入了解著名优化器的细节，包括证明、直觉等. 你能推荐任何相同的教科书/在线资源吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/SmilingBuddha_&quot;>; /u/SmilingBuddha_ &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13sjddd/some_help_to_find_resources_for_optimisation/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13sjddd/some_help_to_find_resources_for_optimisation/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sjddd </id><link href="https://www.reddit.com/r/deeplearning/comments/13sjddd/some_help_to_find_resources_for_optimisation/"/><updated> 2023-05-26T17:30:53+00:00</updated><published> 2023-05-26T17:30:53+00:00</published><title>一些帮助找到优化方法的资源。</title></entry><entry><author><name> /u/Kian5658</name><uri> https://www.reddit.com/user/Kian5658</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，需要帮助！！&lt;/p>; &lt;p>;我一直在尝试训练我的药物审查数据集（一个非常大的 csv 文件）在 BERT 上有一段时间了，在阅读了大量文档之后，我能够训练模型并想出这样的东西......&lt;/p>; &lt;p>;我已经一直在使用 Kaggle 来运行它，但是上面的代码只使用了 5GB 的 GPU 内存，并且在第一个纪元（1+ 小时）中途吃掉了整个 CPU 内存（14.8GB），你能给我一些改进代码的建议吗？告诉我如何减少 CPU 内存消耗？&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;import torch&lt;/p>; &lt;p>;from &lt;a href=&quot;https://torch .utils.data&quot;>;torch.utils.data&lt;/a>; import Dataset, DataLoader&lt;/p>; &lt;p>;from transformers import BertTokenizer, BertForSequenceClassification, AdamW&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt; p>;# 定义自定义数据集&lt;/p>; &lt;p>;class ReviewDataset(Dataset):&lt;/p>; &lt;p>;def __init__(self, reviews, tokenizer, max_length):&lt;/p>; &lt;p>;&lt;a href= &quot;https://self.reviews&quot;>;self.reviews&lt;/a>; = 评论&lt;/p>; &lt;p>;self.tokenizer = tokenizer&lt;/p>; &lt;p>;self.max_length = max_length&lt;/p>; &lt;p>; def __len__(self):&lt;/p>; &lt;p>;return len(&lt;a href=&quot;https://self.reviews&quot;>;self.reviews&lt;/a>;)&lt;/p>; &lt;p>;def __getitem__(self, idx):&lt;/p>; &lt;p>;review = str(&lt;a href=&quot;https://self.reviews&quot;>;self.reviews&lt;/a>;[&quot;review&quot;][idx])&lt;/p>; &lt; p>;rating = &lt;a href=&quot;https://self.reviews&quot;>;self.reviews&lt;/a>;[&quot;rating&quot;][idx]&lt;/p>; &lt;p>;encoding = self.tokenizer.encode_plus(&lt; /p>; &lt;p>;review,&lt;/p>; &lt;p>;add_special_tokens=True,&lt;/p>; &lt;p>;max_length=self.max_length,&lt;/p>; &lt;p>;padding=&amp;#39;max_length&amp;#39;, &lt;/p>; &lt;p>;truncation=True,&lt;/p>; &lt;p>;return_tensors=&amp;#39;pt&amp;#39;&lt;/p>; &lt;p>;)&lt;/p>; &lt;p>;input_ids = encoding[&amp;#39 ;input_ids&amp;#39;].squeeze().to(device)&lt;/p>; &lt;p>;attention_mask = encoding[&amp;#39;attention_mask&amp;#39;].squeeze().to(device)&lt;/p>; &lt;p >;return {&lt;/p>; &lt;p>;&amp;#39;input_ids&amp;#39;: input_ids,&lt;/p>; &lt;p>;&amp;#39;attention_mask&amp;#39;: attention_mask,&lt;/p>; &lt;p>;&amp;#39; rating&amp;#39;: torch.tensor((rating - 1), dtype=torch.float)&lt;/p>; &lt;p>;}&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;#设置设备&lt;/p>; &lt;p>;device = torch.device(&#39;cuda&#39;;如果 torch.cuda.is_available() else &amp;#39;cpu&amp;#39;)&lt;/p>; &lt;p>;tokenizer = BertTokenizer.from_pretrained(&amp;#39;bert-base-uncased&amp;#39;)&lt;/p>; &lt;p >;model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;).to(device)&lt;/p>; &lt;p>;分类器 = torch.nn.Linear(768, 10)&lt;/p>; &lt;p >;&lt;a href=&quot;https://classifier.to&quot;>;classifier.to&lt;/a>;(device)&lt;/p>; &lt;p>;#准备数据集和数据加载器&lt;/p>; &lt;p>;&amp;#x200B;&lt; /p>; &lt;p>;dataset = ReviewDataset(train_imp, tokenizer, 512)&lt;/p>; &lt;p>;batch_size = 4&lt;/p>; &lt;p>;dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)&lt;/p >; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;# 设置优化器和学习率&lt;/p>; &lt;p>;optimizer = AdamW(model.parameters(), lr=1e-4)&lt;/p>; &lt; p>;&amp;#x200B;&lt;/p>; &lt;p>;#训练循环&lt;/p>; &lt;p>;num_epochs = 5&lt;/p>; &lt;p>;loss = torch.nn.CrossEntropyLoss()&lt;/p>; &lt;p>;for范围内的纪元(num_epochs):&lt;/p>; &lt;p>;model.train()&lt;/p>; &lt;p>;total_loss = 0&lt;/p>; &lt;p>;对于 n ，在 tqdm(enumerate(dataloader)) 中批处理：&lt; /p>; &lt;p>;print(n) if n%100 == 0 else None&lt;/p>; &lt;p>;input_ids = batch[&amp;#39;input_ids&amp;#39;]&lt;/p>; &lt;p>;# input_ids = input_ids .type(torch.LongTensor)&lt;/p>; &lt;p>;attention_mask = batch[&amp;#39;attention_mask&amp;#39;]&lt;/p>; &lt;p>;labels = batch[&amp;#39;rating&amp;#39;]&lt;/p >; &lt;p>;labels = labels.type(torch.cuda.LongTensor)&lt;/p>; &lt;p>;&lt;a href=&quot;https://labels.to&quot;>;labels.to&lt;/a>;（设备）&lt;/p >; &lt;p>;optimizer.zero_grad()&lt;/p>; &lt;p>;outputs = model(&lt;/p>; &lt;p>;input_ids=input_ids.to(device),&lt;/p>; &lt;p>;attention_mask=attention_mask.to(device ),&lt;/p>; &lt;p>;)&lt;/p>; &lt;p>;logits = classifier(outputs.pooler_output)&lt;/p>; &lt;p>;losses = loss(logits, labels)&lt;/p>; &lt;p>;total_loss +=损失&lt;/p>; &lt;p>;losses.backward()&lt;/p>; &lt;p>;optimizer.step()&lt;/p>; &lt;p>;average_loss = total_loss / len(dataloader)&lt;/p>; &lt;p>;print(f&amp; #39;epoch {epoch + 1}/{num_epochs}，损失：{average_loss}&amp;#39;)&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -- >; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Kian5658&quot;>; /u/Kian5658 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13scsgb/code_optimization_for_better_gpu_usage/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13scsgb/code_optimization_for_better_gpu_usage/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13scsgb </id><link href="https://www.reddit.com/r/deeplearning/comments/13scsgb/code_optimization_for_better_gpu_usage/"/><updated> 2023-05-26T13:07:20+00:00</updated><published> 2023-05-26T13:07:20+00:00</published><title>代码优化以更好地使用 GPU</title></entry><entry><author><name> /u/kateklink</name><uri> https://www.reddit.com/user/kateklink</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kateklink&quot;>; /u/kateklink &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://refact.ai/blog/ 2023/applying-recent-innovations-to-train-model/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13sd795/applying_all_recent_innovations_to_train_a_code/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sd795 </id><link href="https://www.reddit.com/r/deeplearning/comments/13sd795/applying_all_recent_innovations_to_train_a_code/"/><updated> 2023-05-26T13:24:52+00:00</updated><published> 2023-05-26T13:24:52+00:00</published><title>应用所有最新创新来训练代码模型</title></entry><entry><author><name>/u/爱范探</name><uri>https://www.reddit.com/user/ADfantan</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;有没有你希望以前知道的 PyTorch 技巧？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ADfantan&quot;>; /u/ADfantan &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13skqhq/torch_tips/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13skqhq/torch_tips/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13skqhq</id><link href="https://www.reddit.com/r/deeplearning/comments/13skqhq/torch_tips/"/><updated> 2023-05-26T18:27:44+00:00</updated><published> 2023-05-26T18:27:44+00:00</published><title>手电筒提示</title></entry><entry><author><name>/u/钶钽铁矿3</name><uri> https://www.reddit.com/user/coltan3</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/coltan3&quot;>; /u/coltan3 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://help.medium.com/ hc/zh-cn/articles/360018677974&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13sfljp/i_just_wrote_a_comprehensive_breakdown_of_mass/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sfljp </id><link href="https://www.reddit.com/r/deeplearning/comments/13sfljp/i_just_wrote_a_comprehensive_breakdown_of_mass/"/><updated> 2023-05-26T15:03:06+00:00</updated><published> 2023-05-26T15:03:06+00:00</published><title>我刚刚写了一份关于大规模流离失所的全面分类，你可能不知道生成 AI 已经发生了</title></entry><entry><author><name>/你/unrahul</name><uri> https://www.reddit.com/user/unrahul</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我无法在网上找到如何在 Intel dGPU 上微调 LLM，所以我做了一个简单的版本。这个特别的可以用来根据你最喜欢的书（例如）生成文本。如果您拥有 Intel 独立 GPU，我希望它对您有用：&lt;a href=&quot;https://github.com/rahulunair/tiny_llm_finetuning&quot;>;https://github.com/rahulunair/tiny_llm_finetuning&lt;/a>;/ p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/unrahul&quot;>; /u/unrahul &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13s0xqu/tiny_llm_finetuning_a_finetuner_for_openllama_llm/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s0xqu/tiny_llm_finetuning_a_finetuner_for_openllama_llm/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s0xqu </id><link href="https://www.reddit.com/r/deeplearning/comments/13s0xqu/tiny_llm_finetuning_a_finetuner_for_openllama_llm/"/><updated> 2023-05-26T02:36:01+00:00</updated><published> 2023-05-26T02:36:01+00:00</published><title> tiny_llm_finetuning - 英特尔离散 GPU 上 openLLaMA LLM 模型的微调器</title></entry><entry><author><name>/你/米尔赫</name><uri>https://www.reddit.com/user/mirhec</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mirhec&quot;>; /u/mirhec &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://wordsabout.dev/posts/如何提高迁移学习中的表现/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s4ogw/how_to_improve_performance_in_transfer_learning/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s4ogw </id><link href="https://www.reddit.com/r/deeplearning/comments/13s4ogw/how_to_improve_performance_in_transfer_learning/"/><updated> 2023-05-26T05:52:46+00:00</updated><published> 2023-05-26T05:52:46+00:00</published><title>如何提高迁移学习的性能</title></entry><entry><author><name>/u/Ready-Signature748</name><uri> https://www.reddit.com/user/Ready-Signature748</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Ready-Signature748&quot;>; /u/Ready-Signature748 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://github. com/TransformerOptimus/SuperAGI&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s9xye/github_transformeroptimussuperagi_build_and_run/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s9xye </id><link href="https://www.reddit.com/r/deeplearning/comments/13s9xye/github_transformeroptimussuperagi_build_and_run/"/><updated> 2023-05-26T10:57:46+00:00</updated><published> 2023-05-26T10:57:46+00:00</published><title> GitHub - TransformerOptimus/SuperAGI: 构建和运行有用的自主代理</title></entry><entry><author><name>/u/快乐使命-5901</name><uri> https://www.reddit.com/user/Happy-Mission-5901</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;以下哪些技术可以帮助降低视觉转换器训练中梯度消失问题的概率？&lt;/p>; &lt;p>;&lt;a href=&quot;https: //www.reddit.com/poll/13s89m2&quot;>;查看投票&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Happy-Mission-5901&quot;>; /u/Happy-Mission-5901 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/deeplearning/comments/13s89m2/quiz_vanishing_problems_in_vit/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s89m2/quiz_vanishing_problems_in_vit/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s89m2 </id><link href="https://www.reddit.com/r/deeplearning/comments/13s89m2/quiz_vanishing_problems_in_vit/"/><updated> 2023-05-26T09:27:46+00:00</updated><published> 2023-05-26T09:27:46+00:00</published><title>测验：ViT 中消失的问题</title></entry><entry><author><name>/u/安摩根24</name><uri> https://www.reddit.com/user/Anmorgan24</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://i.redd.it/d2rnkyp1i32b1.gif&quot;>;查看个别错误分类实例，以更好地了解您的模型哪里正确，哪里出错。&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Anmorgan24&quot;>; /u/Anmorgan24 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rvimc/debug_image_classifiers_with_an_interactive/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rvimc/debug_image_classifiers_with_an_interactive/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rvimc </id><link href="https://www.reddit.com/r/deeplearning/comments/13rvimc/debug_image_classifiers_with_an_interactive/"/><updated> 2023-05-25T22:30:23+00:00</updated><published> 2023-05-25T22:30:23+00:00</published><title>使用交互式混淆矩阵调试图像分类器</title></entry><entry><author><name>/u/班图</name><uri>https://www.reddit.com/user/班图</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，对于一个私人项目，我正在尝试为水箱中的活鱼构建一个关键点检测系统。关键点例如眼睛或尾尖，仅在侧视图中。&lt;/p>; &lt;p>;到目前为止，我已经使用 pytorchs Keypoint-RCNN 作为主干构建了一个相当不错的模型。我的重点不是检测图像中的所有实例和关键点，而是只检测非常清晰的实例和关键点（对关键点位置有很高的信心）。这就是为什么在我的第一次尝试中我只注释了“好的”，所以那些鱼：&lt;/p>; &lt;ul>; &lt;li>;完全在图像中&lt;/li>; &lt;li>;没有被其他鱼覆盖&lt; /li>; &lt;li>;距离不太远，因此亮度和大小都合理&lt;/li>; &lt;li>;与图像平面的角度小于 45°（因此仅侧视图）&lt;/li>; &lt;/ ul>; &lt;p>;结果还可以，但不是太好，所以我想知道我应该如何注释图像。我是否应该注释所有实例，即使它们几乎不可见？关键点如何？&lt;/p>; &lt;p>;所附照片可能让我对我的问题有了一些了解。&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it /leofziqic12b1.png?width=980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d8a8480d2dd9b946f7b06bd4a06fec13f518ef7&quot;>;https://preview.redd.it/leofziqic12b1.png?width=980&amp;amp;format=png&amp;auto =webp&amp;amp;s= 6d8a8480d2dd9b946f7b06bd4a06fec13f518ef7&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/rnc39jqic12b1.png?width=980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bb94130d71a4ead40c5 ea624ce53ba9e224c6ec&quot;>;https: //preview.redd.it/rnc39jqic12b1.png?width=980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bb94130d71a4ead40c5ea624ce53ba9e224c6ec&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Banntu&quot;>; /u/Banntu &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rs20a/how_do_deal_with_bad_visibility_in/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rs20a/how_do_deal_with_bad_visibility_in/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rs20a </id><link href="https://www.reddit.com/r/deeplearning/comments/13rs20a/how_do_deal_with_bad_visibility_in/"/><updated> 2023-05-25T20:15:18+00:00</updated><published> 2023-05-25T20:15:18+00:00</published><title>如何处理关键点注释中的不良可见性</title></entry><entry><author><name>/u/sovit-123</name><uri> https://www.reddit.com/user/sovit-123</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Wide Residual Neural Networks – WRNs：论文解释&lt;/p>; &lt;p>;&lt;a href=&quot;https://debuggercafe.com/wide- residual-neural-networks-wrns-paper-explanation/&quot;>;https://debuggercafe.com/wide-residual-neural-networks-wrns-paper-explanation/&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B ;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/iti44puz342b1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59d22a676de4e9288a570deb2d9409f95c08030e&quot;>;https://预览。 redd.it/iti44puz342b1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59d22a676de4e9288a570deb2d9409f95c08030e&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sovit-123&quot;>; /u/sovit-123 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/deeplearning/comments/13ryaz7/paper_explanation_wide_residual_neural_networks/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13ryaz7/paper_explanation_wide_residual_neural_networks/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13ryaz7 </id><link href="https://www.reddit.com/r/deeplearning/comments/13ryaz7/paper_explanation_wide_residual_neural_networks/"/><updated> 2023-05-26T00:32:54+00:00</updated><published> 2023-05-26T00:32:54+00:00</published><title> 【论文讲解】Wide Residual Neural Networks – WRNs: Paper Explanation</title></entry><entry><author><name> /u/CS缩短</name><uri>https://www.reddit.com/user/CShorten</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好！我非常高兴能与 Laura Dietz 教授一起发布 Weaviate 播客 #49！&lt;/p>; &lt;p>;此播客有两个主要部分：(1) 搜索中的神经符号 AI 方法和 (2) 使用 LLM 进行相关性判断。最后，我们还反思了围绕 AI 进步（例如 ChatGPT）的更广泛的社会讨论。 &lt;/p>; &lt;p>;神经符号 AI 广泛地描述了深度学习等受神经启发的计算技术与树搜索或知识图数据结构等符号算法的结合！ Dietz 博士解释了许多有趣的想法，尤其是关于实体链接和实体排名的想法。我认为向量搜索与知识图谱技术的交集非常令人兴奋——当然我们也看到了 LLM 如何使用查询语言的更多组合，或者说 MCTS 在 Tree-of-Thoughts 论文中的表现。 &lt;/p>; &lt;p>;使用 LLM 进行相关性判断是搜索技术的另一个绝对巨大的新兴领域！这有很多方面——Dietz 教授和合作者最近发表了“Perspectives on Large Language Models for Relevance Judgement”在用于注释相关性判断的人机协作频谱上。我个人认为这对于那些希望构建搜索功能但还没有关于查询的用户数据并希望生成合成查询来测试不同模型和排名系统的人来说将非常有影响。 Dietz 博士还解释了这与抽象总结和问题回答的判断有何广泛关联——在精神上与我们最近发布的 ChatArena Weaviate 播客非常相似。&lt;/p>; &lt;p>;我从与Dietz 博士，希望您喜欢播客！&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.youtube.com/watch?v=2s%5C_GGMZ%5C_Zgs&quot;>;https://www. youtube.com/watch?v=2s\_GGMZ\_Zgs&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/CShorten&quot;>; /u/CShorten &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rla6o/new_weaviate_podcast_neurosymbolic_search/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rla6o/new_weaviate_podcast_neurosymbolic_search/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rla6o </id><link href="https://www.reddit.com/r/deeplearning/comments/13rla6o/new_weaviate_podcast_neurosymbolic_search/"/><updated> 2023-05-25T15:49:21+00:00</updated><published> 2023-05-25T15:49:21+00:00</published><title>新的 Weaviate 播客 - 神经符号搜索！</title></entry><entry><author><name> /u/帕帕度08</name><uri> https://www.reddit.com/user/Papadude08</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;标题说你们有什么建议可以让你成为更好的程序员吗？&lt;/p>; &lt;p>;我刚刚做了我知道的 mnist 一个这是一个简单的过程，但我仍然觉得很有成就感！&lt;/p>; &lt;p>;非常希望收到任何反馈或建议，非常感谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Papadude08&quot;>; /u/Papadude08 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rr7o6/any_ideas_for_a_project_would_love_to_do_it_if/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rr7o6/any_ideas_for_a_project_would_love_to_do_it_if/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rr7o6 </id><link href="https://www.reddit.com/r/deeplearning/comments/13rr7o6/any_ideas_for_a_project_would_love_to_do_it_if/"/><updated> 2023-05-25T19:42:38+00:00</updated><published> 2023-05-25T19:42:38+00:00</published><title>如果数据可以访问，任何关于项目的想法都会很乐意去做！</title></entry><entry><author><name> /u/OnlyProggingForFun</name><uri> https://www.reddit.com/user/OnlyProggingForFun</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/OnlyProggingForFun&quot;>; /u/OnlyProggingForFun &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://youtu.be/WsCIsqP7U9c&quot; >;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rhyrb/what_is_explainability_ai_with_yotam_azriel_cto/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rhyrb </id><link href="https://www.reddit.com/r/deeplearning/comments/13rhyrb/what_is_explainability_ai_with_yotam_azriel_cto/"/><updated> 2023-05-25T13:34:17+00:00</updated><published> 2023-05-25T13:34:17+00:00</published><title>什么是可解释性人工智能？与 TensorLeap 首席技术官 Yotam Azriel - 什么是人工智能第 13 集</title></entry><entry><author><name>/你/杰斯特177</name><uri> https://www.reddit.com/user/jesst177</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨！&lt;/p>; &lt;p>;我正在努力提高 Pytorch 训练管道的内存和速度效率。在检查过程中，我意识到我们的 GPU 在每个纪元后都变为 IDLE（最后是视觉）。&lt;/p>; &lt;p>;我们的环境是：&lt;/p>; &lt;ul>; &lt;li>;2X V100（Azure 云）。&lt;/li >; &lt;li>;Pytorch 1.13.&lt;/li>; &lt;li>;CUDA 11.6.&lt;/li>; &lt;li>;AMP 已激活。&lt;/li>; &lt;li>;worker 数量为 8.&lt;/li>; &lt;li>;DataParallel 为&lt;/li>; &lt;li>;批量大小为 32。&lt;/li>; &lt;li>;固定内存集。&lt;/li>; &lt;li>;丢弃最后一组。&lt;/li>; &lt;li>;持久性工作集。&lt;/li >; &lt;li>;我们正在使用图像数据的基本预取，我们将所有图像数据移动到 pytohn 字典对象。&lt;/li>; &lt;li>;我们使用 Albumentations 库应用扩充。&lt;/li>; &lt;/ul>; &lt;p>;大 IDLE 块发生在每个纪元之后，我正在努力寻找它的原因。&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/64rgbkdvxv1b1.png?width=1873&amp;amp; format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=48bb250c9d1d7a115f3eaaea92bc59616889e796&quot;>;https://preview.redd.it/64rgbkdvxv1b1.png?width=1873&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s =48bb250c9d1d7a115f3eaaea92bc59616889e796 &lt;/a>;&lt;/p>; &lt;p>;下面可以看到我们的训练代码，&lt;/p>; &lt;pre>;&lt;code>;for epoch in range(last_epoch, end_epoch): for i_iter, batch in enumerate(trainloader, 0) : model.zero_grad(set_to_none=True) images, labels, bd_gts, _ = batch with torch.amp.autocast(device_type=“cuda”, dtype=torch.float16): images = images.cuda() labels = 标签。 cuda() bd_gts = bd_gts.cuda() losses, pred, acc, loss_list = model(images, labels, bd_gts) 如果不是isinstance(pred, (list, tuple)): pred = [pred] loss = losses.mean( ) acc = acc.mean() scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() lr = adjust_learning_rate(optimizer, config.TRAIN.LR, num_iters, i_iter + epoch * epoch_iters) 如果i_iter % print_freq == 0: print(i_iter) &lt;/code>;&lt;/pre>; &lt;p>;我目前看到的是，减少批量大小有帮助（到 2，甚至 6），但是我们没有利用我们的GPU 的内存已满。&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;我如何查明问题所在或导致此问题的可能原因是什么？&lt;/p>; &lt;/div >;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/jesst177&quot;>; /u/jesst177 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13qxvn2/massive_drop_in_gpu_usage/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13qxvn2/massive_drop_in_gpu_usage/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qxvn2 </id><link href="https://www.reddit.com/r/deeplearning/comments/13qxvn2/massive_drop_in_gpu_usage/"/><updated> 2023-05-24T21:12:08+00:00</updated><published> 2023-05-24T21:12:08+00:00</published><title> GPU 使用率大幅下降</title></entry><entry><author><name>/你/查图阿基</name><uri>https://www.reddit.com/user/chatouaki</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;你准备好释放你的创造力并学习制作令人惊叹的自定义徽标的艺术了吗？无论您是企业主、平面设计爱好者，还是只是想提高艺术技能的人，本教程都非常适合您。 &lt;/p>; &lt;p>;&lt;a href=&quot;https://youtu.be/MBEBNtX80B0&quot;>;教程链接&lt;/a>; &lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href =&quot;https://preview.redd.it/a94h2980oz1b1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1260fc056464f94360cde2d5863b996efc5db7b6&quot;>;https://preview.redd.it/a94h2980oz1b1.png?宽度=1280&amp;amp ;format=png&amp;amp;auto=webp&amp;amp;s=1260fc056464f94360cde2d5863b996efc5db7b6&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/chatouaki&quot;>; /u/chatouaki &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rd29h/hand_lettering_logo_design_in_canva/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rd29h/hand_lettering_logo_design_in_canva/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rd29h </id><link href="https://www.reddit.com/r/deeplearning/comments/13rd29h/hand_lettering_logo_design_in_canva/"/><updated> 2023-05-25T09:36:09+00:00</updated><published> 2023-05-25T09:36:09+00:00</published><title> Canva手写标志设计</title></entry><entry><author><name>/你/杰斯特177</name><uri> https://www.reddit.com/user/jesst177</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨！&lt;/p>; &lt;p>;我们最近决定购买一台预算为 15,000 美元的工作站。我们查看了本地供应商的选项并检查了他们的计算能力，并提出了几个选项。&lt;/p>; &lt;p>;- 4X A4500&lt;/p>; &lt;p>;- 1XA6000&lt;/p>; &lt;p>;我们还可以寻找具有中级选项的任何其他替代方案，例如 2X A5000/A5500。然而，从我们的角度来看，A4500s 拥有更多的计算能力，并将拥有大约 80 GB 的内存。虽然我不确定我们是否可以像在多 GPU 设置中那样将它们全部一起使用（我们可以吗？）这意味着它是更好的选择。&lt;/p>; &lt;p>;我们感兴趣的机器将在 Deep 中使用学习，使用 Transformer 和 ConvNets。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/jesst177&quot;>; /u/jesst177 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13qxd0z/should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13qxd0z/should_we_go_with_a_single_a6000_or_4xa4500_or/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qxd0z </id><link href="https://www.reddit.com/r/deeplearning/comments/13qxd0z/should_we_go_with_a_single_a6000_or_4xa4500_or/"/><updated> 2023-05-24T20:53:11+00:00</updated><published> 2023-05-24T20:53:11+00:00</published><title>我们应该使用单个 A6000 还是 4XA4500 或任何其他替代方案，例如 2XA5000</title></entry><entry><author><name> /u/Cold_Cantaloupe9212</name><uri> https://www.reddit.com/user/Cold_Cantaloupe9212</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我有兴趣开发一个条件扩散模型，以保证给定输入的一致输出。我想减少或消除模型中的随机性以实现此目标。有没有办法在保持一定程度的可变性的同时实现这一目标？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Cold_Cantaloupe9212&quot;>; /u/Cold_Cantaloupe9212 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13r3ndd/deterministic_diffusion_models/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13r3ndd/deterministic_diffusion_models/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13r3ndd </id><link href="https://www.reddit.com/r/deeplearning/comments/13r3ndd/deterministic_diffusion_models/"/><updated> 2023-05-25T01:06:18+00:00</updated><published> 2023-05-25T01:06:18+00:00</published><title>确定性扩散模型</title></entry><entry><author><name>/u/iLikePortugueseTarts</name><uri> https://www.reddit.com/user/iLikePortugueseTarts</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/iLikePortugueseTarts&quot;>; /u/iLikePortugueseTarts &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://duarteocarmo.com/blog/ fine-tune-flan-t5-telegram&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13qn4u4/finetuning_flant5_to_replace_my_friends_using_a/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qn4u4 </id><link href="https://www.reddit.com/r/deeplearning/comments/13qn4u4/finetuning_flant5_to_replace_my_friends_using_a/"/><updated> 2023-05-24T14:22:09+00:00</updated><published> 2023-05-24T14:22:09+00:00</published><title>微调 FLAN-T5 以使用 Telegram 群聊替换我的朋友</title></entry><entry><author><name>/u/Not_Sure204</name><uri> https://www.reddit.com/user/Not_Sure204</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我已经创建了一个 NEAT 的（体面的）工作模型。唯一的问题是随着节点和连接数量的增长，它的运行速度非常慢。我初始化的模型很小，有 10 个输入神经元、4 个输出神经元和 2 个隐藏神经元。&lt;/p>; &lt;p>;这是运行神经网络的函数：&lt;/p>; &lt;pre>;&lt;code>;def run( self, inputs): self.reset() 输出 = [] active_connections = [] self.input_neurons[self.num_inputs].value = 1 self.input_neurons[self.num_inputs+1].value = 0 for i in range(self .nu​​m_inputs): self.input_neurons[i].value = inputs[i] for connection in self.genome_connections: if connection.active == True: active_connections.append(connection) iterations = 0 while True: iterations += 1 neurons_not_set = False for i in range(len(active_connections)): connection = active_connections[i] for neuron in self.genome_neurons: if neuron.id == connection.input_neuron: current_input_neuron = neuron if neuron.id == connection.output_neuron: current_output_neuron =神经元 if current_input_neuron.value != None: current_output_neuron.sum += current_input_neuron.value * connection.weight set_value = True for j in range(i + 1, len(self.genome_connections)): remaining_connection = self.genome_connections[j] 如果remaining_connection.output_neuron == current_output_neuron: set_value = False if set_value == True: current_output_neuron.activate() else: neurons_not_set = True if neurons_not_set == False: 如果迭代中断 >;; 54：#print（“迭代次数太高”）在范围内中断 i（self.num_inputs，self.num_inputs + self.num_outputs）：outputs.append（self.genome_neurons[i].value）max_value = None max_index = None对于 i，枚举中的值（输出）：如果值不是 None：如果 max_value 是 None 或 value >;; max_value: max_value = value max_index = i return max_index &lt;/code>;&lt;/pre>; &lt;p>;如您所见，它非常复杂。我不得不采用一种递归方法，因为没有设置“层”。因此结构主要由连接和每个包含的 id（每个连接的 input_neuron 和 output_neuron 不是节点对象而是它们的 id）定义。我考虑过用 C++ 编写这个，但我对此很陌生，所以也许是最后的选择。我已经尝试在函数的开头将一些列表转换为 numpy 数组，但这并没有带来很大的性能提升。我不得不将 break 部分添加到 while 循环中，因为神经元经常以圆形或自我参照的方式连接，这将花费太长时间来修复我认为如此幼崽。我可以用它来在某种程度上控制函数的速度，但是所有嵌套的 for 循环，即使 while 循环设置为在如此低的迭代次数时中断，它仍然需要很长时间。&lt;/p>; &lt;p >;（顺便说一句，重置函数只是重置每个神经元的总和和值）&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;这是代码的其余部分，以备不时之需。我也可以发布神经元和连接类，但它们除了 __init__ 函数外什么都没有。如果这会导致性能提升，我会准备做一些重构。&lt;/p>;类 Neural_Network(): def __init__(self, num_inputs, num_outputs): self.genome_neurons = [] self.genome_connections = [] self.input_neurons = [] self.output_neurons = [] self.num_inputs = num_inputs self.num_outputs = num_outputs self .in_out_layers() self.runtime = 0 def in_out_layers(self): for i in range(self.num_inputs): neuron = Neuron(i, None, None) self.genome_neurons.append(neuron) self.input_neurons.append(neuron ) 对于我在范围内 (self.num_inputs, self.num_inputs + self.num_outputs): neuron = Neuron(i, None, None) self.genome_neurons.append(neuron) self.output_neurons.append(neuron) neuron = Neuron(self .nu​​m_inputs+self.num_outputs，无，无）self.genome_neurons.append（神经元）self.input_neurons.append（神经元）neuron = Neuron（self.num_inputs+self.num_outputs+1，无，无）self.genome_neurons.append （神经元）self.input_neurons.append（neuron）def create_connection（self，input_neuron，output_neuron）：new_connection = True 对于 globalvars.connections 中的连接：如果 connection.input_neuron == input_neuron 和 connection.output_neuron == output_neuron：new_connection = False innovation_number = connection.innovation_number 如果 new_connection == True: innovation_number = globalvars.next_innovation_number globalvars.next_innovation_number += 1 connection = Connection(input_neuron, output_neuron, innovation_number) if connection not in self.genome_connections: self.genome_connections.append(connection) globalvars.connections .append(connection) return connection def create_node(self, connection): input_neuron = connection.input_neuron output_neuron = connection.output_neuron new_neuron = True 对于 globalvars.neurons 中的神经元：如果 neuron.input_neuron == input_neuron 和 neuron.output_neuron == output_neuron： new_neuron = False neuron_id = neuron.id 如果 new_neuron == True: neuron_id = globalvars.next_id globalvars.next_id += 1 connection.active = False neuron = Neuron(neuron_id, input_neuron, output_neuron) globalvars.neurons.append(neuron) self. genome_neurons.append(neuron) connection1 = self.create_connection(input_neuron, neuron.id) connection2 = self.create_connection(neuron.id, output_neuron) connection1.weight = 1 connection2.weight = connection.weight #run function goes here def reset( self): 对于 self.genome_neurons 中的神经元：neuron.sum = 0 neuron.value = None def crossover(self, parent2): aligned_connections = [] parent1_connections = self.genome_connections parent2_connections = parent2.genome_connections parent1_connections.sort(key=lambda x : x.innovation_number) parent2_connections.sort(key=lambda x: x.innovation_number) p1_index = 0 p2_index = 0 而 p1_index &lt;; len(parent1_connections) 和 p2_index &lt;; len(parent2_connections): connection1 = copy.deepcopy(parent1_connections[p1_index]) connection2 = copy.deepcopy(parent2_connections[p2_index]) 如果connection1.innovation_number == connection2.innovation_number: aligned_connections.append(connection1 if random.random() &lt;; 0.5 else connection2) p1_index += 1 p2_index += 1 elif connection1.innovation_number &lt;; connection2.innovation_number: aligned_connections.append(connection1) p1_index += 1 else: aligned_connections.append(connection2) p2_index += 1 while p1_index &lt;; len(parent1_connections): aligned_connections.append(parent1_connections[p1_index]) p1_index += 1 而 p2_index &lt;; len(parent2_connections): aligned_connections.append(parent2_connections[p2_index]) p2_index += 1 offspring = Neural_Network(self.num_inputs, self.num_outputs) offspring.genome_neurons = [] offspring.input_neurons = [] offspring.output_neurons = [] 后代。 genome_connections = aligned_connections offspring.in_out_layers() offspring_node_ids = [] for neuron in offspring.genome_neurons: offspring_node_ids.append(neuron.id) for connection in aligned_connections: if connection.input_neuron not in offspring_node_ids: offspring_node_ids.append(connection.input_neuron) elif 连接.output_neuron 不在 offspring_node_ids 中：offspring_node_ids.append(connection.output_neuron) for neuron_id in offspring_node_ids: for neuron in self.genome_neurons: if neuron.id == neuron_id: new_neuron = Neuron(neuron.id, neuron.input_neuron, neuron.output_neuron)如果没有（new_neuron.id == neuron.id 对于 offspring.genome_neurons 中的神经元）：offspring.genome_neurons.append（new_neuron）对于 parent2.genome_neurons 中的神经元：如果 neuron.id == neuron_id：new_neuron = Neuron（neuron.id , neuron.input_neuron, neuron.output_neuron) 如果没有(new_neuron.id == neuron.id for neuron in offspring.genome_neurons): offspring.genome_neurons.append(new_neuron) return offspring def mutate_connection(self): non_output_neurons = self.genome_neurons [:self.num_inputs] + self.genome_neurons[self.num_inputs + self.num_outputs + 1:] input_neuron = random.choice(non_output_neurons) while True: output_neuron = random.choice(self.genome_neurons[self.num_inputs:]) 如果output_neuron != input_neuron: 中断 self.create_connection(input_neuron.id, output_neuron.id) def mutate_neuron(self): self.create_node(random.choice(self.genome_connections)) def mutate_enable_disable(self): connection = random.choice(self) .genome_connections) if connection.active == True: connection.active = False elif connection.active == False: connection.active = True def mutate_weight_shift(self): connection = random.choice(self.genome_connections) connection.weight *= (random.random() * 2) def mutate_weight_random(self): connection = random.choice(self.genome_connections) connection.weight = (random.random() * 4) - 2 def mutate(self, probabilities): 如果概率[0]>; random.random(): self.mutate_connection() 如果概率[1] >;; random.random() 和 len(self.genome_connections) >; 0: self.mutate_neuron() 如果概率 [2] >; random.random() 和 len(self.genome_connections) >; 0: self.mutate_enable_disable() 如果概率[3] >;; random.random() 和 len(self.genome_connections) >; 0: self.mutate_weight_shift() 如果概率[4] >;; random.random() 和 len(self.genome_connections) >; 0: self.mutate_weight_random() &lt;/code>;&lt;/pre>; &lt;p>;对此仍然很陌生，因此我们将不胜感激。即使您没有编码解决方案而只是一个想法，也请告诉我。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Not_Sure204&quot;>; /u/Not_Sure204 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13qxxfz/looking_to_optimise_run_function_for_neat_neural/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13qxxfz/looking_to_optimise_run_function_for_neat_neural/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qxxfz </id><link href="https://www.reddit.com/r/deeplearning/comments/13qxxfz/looking_to_optimise_run_function_for_neat_neural/"/><updated> 2023-05-24T21:14:01+00:00</updated><published> 2023-05-24T21:14:01+00:00</published><title>希望优化 NEAT 神经网络的运行功能</title></entry><entry><author><name>/你/keving_93</name><uri> https://www.reddit.com/user/keving_93</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨，我正在用 tensorflow 做一个编码器-解码器，输入图像是 TIF，每个像素有两个值（NBR 和 NDVI），我遇到下一个错误：当我尝试训练模型时，有人可以帮助建议我修复它吗？ &lt;/p>; &lt;p>;---------------------------------------- ------------------------------ ValueError Traceback（最近调用最后）&lt;a href=&quot;https://localhost :8080/#&quot;>;&lt;ipython-input-91-e156620cfdb8>;&lt;/a>; &lt;cell line: 2>;()&lt;br/>; &lt;strong>;1&lt;/strong>; # Entrenar el modelo&lt;br/>; ----&amp;gt; 2 model.fit(input_images_reshaped.reshape(-1, 48, 39), target_images.reshape(-1, 48, 39), epochs=10, batch_size=16) ValueError: 无法将大小为 21504 的数组重塑为形状 (48, 39) ,&lt;/p>; &lt;p>;这是我得到错误的地方：&lt;/p>; &lt;p>;# 训练模型&lt;br/>; model.fit(input_images_reshaped.reshape(-1, 48, 39), target_images .reshape(-1, 48, 39), epochs=10, batch_size=16) &lt;/p>; &lt;p>;代码库是这样的：&lt;a href=&quot;https://github.com/bevins93/encoder -decoder-NBR-NDVI/blob/main/encoder.py&quot;>;encoder-decoder-NBR-NDVI/encoder.py at main · bevins93/encoder-decoder-NBR-NDVI (github.com)&lt;/a>;&lt;/ p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/keving_93&quot;>; /u/keving_93 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13qtfv1/help/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13qtfv1/help/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qtfv1</id><link href="https://www.reddit.com/r/deeplearning/comments/13qtfv1/help/"/><updated> 2023-05-24T18:25:16+00:00</updated><published> 2023-05-24T18:25:16+00:00</published><title>帮助</title></entry><entry><author><name>/你/pkchad</name><uri> https://www.reddit.com/user/pkchad</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;是否有任何 DL 奇才想成为 kaggle 竞赛团队的一员。我和我的团队由数据科学家组成，但希望更多人加入团队以分担工作量。&lt;/p>; &lt;p>;竞赛链接：&lt;a href=&quot;https://www.kaggle.com /competitions/google-research-identify-contrails-reduce-global-warming&quot;>;https://www.kaggle.com/competitions/google-research-identify-contrails-reduce-global-warming&lt;/a>;&lt;/p >; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/pkchad&quot;>; /u/pkchad &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13qn3vq/need_a_partner_to_be_a_part_of_a_team_for_a/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13qn3vq/need_a_partner_to_be_a_part_of_a_team_for_a/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qn3vq </id><link href="https://www.reddit.com/r/deeplearning/comments/13qn3vq/need_a_partner_to_be_a_part_of_a_team_for_a/"/><updated> 2023-05-24T14:21:08+00:00</updated><published> 2023-05-24T14:21:08+00:00</published><title>需要一个合作伙伴成为 kaggle 比赛团队的一员</title></entry><entry><author><name>/u/Due-Net6290</name><uri> https://www.reddit.com/user/Due-Net6290</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我有一个包含 3 个参数的数值数据集：信号、标签和时间戳。数据集的形状为 ( 4020,100,9)。我必须在 train_x、train_y、test_x、test_y、val_x、val_y 上拆分它，但我不知道如何将它应用于我的数据。&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp; #32；由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Due-Net6290&quot;>; /u/Due-Net6290 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/deeplearning/comments/13qlr1y/train_test_split/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13qlr1y/train_test_split/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qlr1y </id><link href="https://www.reddit.com/r/deeplearning/comments/13qlr1y/train_test_split/"/><updated> 2023-05-24T13:29:14+00:00</updated><published> 2023-05-24T13:29:14+00:00</published><title>火车测试拆分</title></entry></feed>