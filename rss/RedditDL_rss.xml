<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category label="r/deeplearning" term="deeplearning"></category><updated> 2023-05-27T12:27:11+00:00</updated><icon> https://www.redditstatic.com/icon.png/</icon><id> /r/深度学习/.rss </id><link href="https://www.reddit.com/r/deeplearning/.rss" rel="self" type="application/atom+xml"/><link href="https://www.reddit.com/r/deeplearning/" rel="alternate" type="text/html"/><title>深度学习</title><entry><author><name>/u/False_Mobile_8823</name><uri> https://www.reddit.com/user/False_Mobile_8823</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我一直在通过我的大学进行无薪实习以获得一些行业准入，我们的小组被分配了一个主题 - 构建一个可以对话的 AI使用变压器回答医疗保健问题（变压器必须从头开始构建）。我已尽力了解变压器的工作原理，尽管我对它背后的数学知识仍然有些薄弱，但我对它的工作原理有一个不错的了解。这是我们计划参考的代码链接 - &lt;a href=&quot;https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator- in-12-days/&quot;>;https://machinelearningmastery.com/building-transformer-models-with-attention-crash-course-build-a-neural-machine-translator-in-12-days/&lt;/a>; &lt;/p>; &lt;p>;我真的不明白如何训练 transformer 进行对话，大多数教程都使用预训练的 transformer 来完成此类任务。我需要一些关于我应该执行哪些步骤的指导，因为我们的行业导师真的很难联系到。任何帮助将非常感激。谢谢 :)&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/False_Mobile_8823&quot;>; /u/False_Mobile_8823 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13t5bey/need_help_with_a_chatbot_mini_project/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13t5bey/need_help_with_a_chatbot_mini_project/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t5bey </id><link href="https://www.reddit.com/r/deeplearning/comments/13t5bey/need_help_with_a_chatbot_mini_project/"/><updated> 2023-05-27T11:47:12+00:00</updated><published> 2023-05-27T11:47:12+00:00</published><title>需要聊天机器人迷你项目的帮助</title></entry><entry><author><name>/u/马克西丹尼尔斯</name><uri>https://www.reddit.com/user/maxiedaniels</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我正在使用 PyTorch 推理进行一些测试，但我很难弄清楚哪些卡被认为是 TPU，等等，而且我认为某处一定有一个包含基准测试的列表……有人吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/maxiedaniels&quot;>; /u/maxiedaniels &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13syulv/list_of_gpus_and_tpus_with_performance_benchmarks/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13syulv/list_of_gpus_and_tpus_with_performance_benchmarks/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13syulv </id><link href="https://www.reddit.com/r/deeplearning/comments/13syulv/list_of_gpus_and_tpus_with_performance_benchmarks/"/><updated> 2023-05-27T05:21:30+00:00</updated><published> 2023-05-27T05:21:30+00:00</published><title>具有性能基准的 GPU 和 TPU 列表？</title></entry><entry><author><name> /u/OnlyProggingForFun</name><uri> https://www.reddit.com/user/OnlyProggingForFun</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/OnlyProggingForFun&quot;>; /u/OnlyProggingForFun &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://youtu.be/r1mh- IqBEjg&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13t63l3/transform_any_image_with_a_single_movement_of/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13t63l3 </id><link href="https://www.reddit.com/r/deeplearning/comments/13t63l3/transform_any_image_with_a_single_movement_of/"/><updated> 2023-05-27T12:25:21+00:00</updated><published> 2023-05-27T12:25:21+00:00</published><title>只需移动鼠标即可转换任何图像：DragGan 解释</title></entry><entry><author><name>/u/V1自行车</name><uri>https://www.reddit.com/user/V1bicycle</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我发现“梯度范数”没有通用资源和定义明确的定义，大多数搜索结果都是基于 ML 专家提供的答案，其中涉及梯度范数或引用它并提供单个句子介绍的论文。&lt;/p>; &lt;p>;是否有任何明确定义的资源我可以参考以获得对它的具体理解？谢谢&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/V1bicycle&quot;>; /u/V1bicycle &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13sw02m/what_exactly_is_gradient_norm/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13sw02m/what_exactly_is_gradient_norm/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sw02m </id><link href="https://www.reddit.com/r/deeplearning/comments/13sw02m/what_exactly_is_gradient_norm/"/><updated> 2023-05-27T02:47:10+00:00</updated><published> 2023-05-27T02:47:10+00:00</published><title> Gradient norm 到底是什么？</title></entry><entry><author><name> /u/公平竞争</name><uri>https://www.reddit.com/user/Play-Equal</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Play-Equal&quot;>; /u/Play-Equal &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://v. redd.it/fvf4ka4qs62b1&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s8e6h/building_a_3_layered_neural_network_from_scratch/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s8e6h </id><link href="https://www.reddit.com/r/deeplearning/comments/13s8e6h/building_a_3_layered_neural_network_from_scratch/"/><updated> 2023-05-26T09:35:36+00:00</updated><published> 2023-05-26T09:35:36+00:00</published><title>仅使用 NumPy 从头开始​​构建 3 层神经网络 https://youtu.be/w7Hn3jmbj1A 如需完整视频，请参阅评论部分</title></entry><entry><author><name>/u/akool_technology</name><uri> https://www.reddit.com/user/akool_technology</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;从简单的文本创建优质图像，比 Midjourney 更好。在此处尝试：&lt;a href=&quot;http://beyond.akool.com/&quot;>;http://beyond.akool.com/&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON - ->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/akool_technology&quot;>; /u/akool_technology &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13sz9vc/beyond_journey_premium_quality_images_from_simple/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13sz9vc/beyond_journey_premium_quality_images_from_simple/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sz9vc </id><link href="https://www.reddit.com/r/deeplearning/comments/13sz9vc/beyond_journey_premium_quality_images_from_simple/"/><updated> 2023-05-27T05:46:20+00:00</updated><published> 2023-05-27T05:46:20+00:00</published><title>超越旅程，来自简单文本的优质图像</title></entry><entry><author><name>/u/SeparateConcert9041</name><uri> https://www.reddit.com/user/SeparateConcert9041</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/SeparateConcert9041&quot;>; /u/SeparateConcert9041 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.youtube.com/ watch?v=EKlRxMOUU-o&amp;amp;t=717s&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13stz86/wilderness_passion_project/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13stz86 </id><link href="https://www.reddit.com/r/deeplearning/comments/13stz86/wilderness_passion_project/"/><updated> 2023-05-27T01:05:19+00:00</updated><published> 2023-05-27T01:05:19+00:00</published><title>荒野激情项目</title></entry><entry><author><name>/u/爱范探</name><uri>https://www.reddit.com/user/ADfantan</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;有没有你希望以前知道的 PyTorch 技巧？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/ADfantan&quot;>; /u/ADfantan &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13skqhq/torch_tips/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13skqhq/torch_tips/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13skqhq</id><link href="https://www.reddit.com/r/deeplearning/comments/13skqhq/torch_tips/"/><updated> 2023-05-26T18:27:44+00:00</updated><published> 2023-05-26T18:27:44+00:00</published><title>手电筒提示</title></entry><entry><author><name>/u/笑佛_</name><uri> https://www.reddit.com/user/SmilingBuddha_</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，我对深度学习中使用的不同优化器有所了解，但我想深入了解著名优化器的细节，包括证明、直觉等. 你能推荐任何相同的教科书/在线资源吗？&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/SmilingBuddha_&quot;>; /u/SmilingBuddha_ &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13sjddd/some_help_to_find_resources_for_optimisation/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13sjddd/some_help_to_find_resources_for_optimisation/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sjddd </id><link href="https://www.reddit.com/r/deeplearning/comments/13sjddd/some_help_to_find_resources_for_optimisation/"/><updated> 2023-05-26T17:30:53+00:00</updated><published> 2023-05-26T17:30:53+00:00</published><title>一些帮助找到优化方法的资源。</title></entry><entry><author><name> /u/Kian5658</name><uri> https://www.reddit.com/user/Kian5658</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，需要帮助！！&lt;/p>; &lt;p>;我一直在尝试训练我的药物审查数据集（一个非常大的 csv 文件）在 BERT 上有一段时间了，在阅读了大量文档之后，我能够训练模型并想出这样的东西......&lt;/p>; &lt;p>;我已经一直在使用 Kaggle 来运行它，但是上面的代码只使用了 5GB 的 GPU 内存，并且在第一个时期（1+ 小时）中途吃掉了整个 CPU 内存（14.8GB），你能给我一些改进代码的建议吗？告诉我如何减少 CPU 内存消耗？&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;import torch&lt;/p>; &lt;p>;from &lt;a href=&quot;https://torch .utils.data&quot;>;torch.utils.data&lt;/a>; import Dataset, DataLoader&lt;/p>; &lt;p>;from transformers import BertTokenizer, BertForSequenceClassification, AdamW&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt; p>;# 定义自定义数据集&lt;/p>; &lt;p>;class ReviewDataset(Dataset):&lt;/p>; &lt;p>;def __init__(self, reviews, tokenizer, max_length):&lt;/p>; &lt;p>;&lt;a href= &quot;https://self.reviews&quot;>;self.reviews&lt;/a>; = 评论&lt;/p>; &lt;p>;self.tokenizer = tokenizer&lt;/p>; &lt;p>;self.max_length = max_length&lt;/p>; &lt;p>; def __len__(self):&lt;/p>; &lt;p>;return len(&lt;a href=&quot;https://self.reviews&quot;>;self.reviews&lt;/a>;)&lt;/p>; &lt;p>;def __getitem__(self, idx):&lt;/p>; &lt;p>;review = str(&lt;a href=&quot;https://self.reviews&quot;>;self.reviews&lt;/a>;[&quot;review&quot;][idx])&lt;/p>; &lt; p>;rating = &lt;a href=&quot;https://self.reviews&quot;>;self.reviews&lt;/a>;[&quot;rating&quot;][idx]&lt;/p>; &lt;p>;encoding = self.tokenizer.encode_plus(&lt; /p>; &lt;p>;review,&lt;/p>; &lt;p>;add_special_tokens=True,&lt;/p>; &lt;p>;max_length=self.max_length,&lt;/p>; &lt;p>;padding=&amp;#39;max_length&amp;#39;, &lt;/p>; &lt;p>;truncation=True,&lt;/p>; &lt;p>;return_tensors=&amp;#39;pt&amp;#39;&lt;/p>; &lt;p>;)&lt;/p>; &lt;p>;input_ids = encoding[&amp;#39 ;input_ids&amp;#39;].squeeze().to(device)&lt;/p>; &lt;p>;attention_mask = encoding[&amp;#39;attention_mask&amp;#39;].squeeze().to(device)&lt;/p>; &lt;p >;return {&lt;/p>; &lt;p>;&amp;#39;input_ids&amp;#39;: input_ids,&lt;/p>; &lt;p>;&amp;#39;attention_mask&amp;#39;: attention_mask,&lt;/p>; &lt;p>;&amp;#39; rating&amp;#39;: torch.tensor((rating - 1), dtype=torch.float)&lt;/p>; &lt;p>;}&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;#设置设备&lt;/p>; &lt;p>;device = torch.device(&#39;cuda&#39;;如果 torch.cuda.is_available() else &amp;#39;cpu&amp;#39;)&lt;/p>; &lt;p>;tokenizer = BertTokenizer.from_pretrained(&amp;#39;bert-base-uncased&amp;#39;)&lt;/p>; &lt;p >;model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;).to(device)&lt;/p>; &lt;p>;分类器 = torch.nn.Linear(768, 10)&lt;/p>; &lt;p >;&lt;a href=&quot;https://classifier.to&quot;>;classifier.to&lt;/a>;(device)&lt;/p>; &lt;p>;#准备数据集和数据加载器&lt;/p>; &lt;p>;&amp;#x200B;&lt; /p>; &lt;p>;dataset = ReviewDataset(train_imp, tokenizer, 512)&lt;/p>; &lt;p>;batch_size = 4&lt;/p>; &lt;p>;dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)&lt;/p >; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;# 设置优化器和学习率&lt;/p>; &lt;p>;optimizer = AdamW(model.parameters(), lr=1e-4)&lt;/p>; &lt; p>;&amp;#x200B;&lt;/p>; &lt;p>;#训练循环&lt;/p>; &lt;p>;num_epochs = 5&lt;/p>; &lt;p>;loss = torch.nn.CrossEntropyLoss()&lt;/p>; &lt;p>;for范围内的纪元(num_epochs):&lt;/p>; &lt;p>;model.train()&lt;/p>; &lt;p>;total_loss = 0&lt;/p>; &lt;p>;对于 n ，在 tqdm(enumerate(dataloader)) 中批处理：&lt; /p>; &lt;p>;print(n) if n%100 == 0 else None&lt;/p>; &lt;p>;input_ids = batch[&amp;#39;input_ids&amp;#39;]&lt;/p>; &lt;p>;# input_ids = input_ids .type(torch.LongTensor)&lt;/p>; &lt;p>;attention_mask = batch[&amp;#39;attention_mask&amp;#39;]&lt;/p>; &lt;p>;labels = batch[&amp;#39;rating&amp;#39;]&lt;/p >; &lt;p>;labels = labels.type(torch.cuda.LongTensor)&lt;/p>; &lt;p>;&lt;a href=&quot;https://labels.to&quot;>;labels.to&lt;/a>;（设备）&lt;/p >; &lt;p>;optimizer.zero_grad()&lt;/p>; &lt;p>;outputs = model(&lt;/p>; &lt;p>;input_ids=input_ids.to(device),&lt;/p>; &lt;p>;attention_mask=attention_mask.to(device ),&lt;/p>; &lt;p>;)&lt;/p>; &lt;p>;logits = classifier(outputs.pooler_output)&lt;/p>; &lt;p>;losses = loss(logits, labels)&lt;/p>; &lt;p>;total_loss +=损失&lt;/p>; &lt;p>;losses.backward()&lt;/p>; &lt;p>;optimizer.step()&lt;/p>; &lt;p>;average_loss = total_loss / len(dataloader)&lt;/p>; &lt;p>;print(f&amp; #39;epoch {epoch + 1}/{num_epochs}，损失：{average_loss}&amp;#39;)&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -- >; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Kian5658&quot;>; /u/Kian5658 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13scsgb/code_optimization_for_better_gpu_usage/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13scsgb/code_optimization_for_better_gpu_usage/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13scsgb </id><link href="https://www.reddit.com/r/deeplearning/comments/13scsgb/code_optimization_for_better_gpu_usage/"/><updated> 2023-05-26T13:07:20+00:00</updated><published> 2023-05-26T13:07:20+00:00</published><title>代码优化以更好地使用 GPU</title></entry><entry><author><name> /u/kateklink</name><uri> https://www.reddit.com/user/kateklink</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/kateklink&quot;>; /u/kateklink &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://refact.ai/blog/ 2023/applying-recent-innovations-to-train-model/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13sd795/applying_all_recent_innovations_to_train_a_code/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sd795 </id><link href="https://www.reddit.com/r/deeplearning/comments/13sd795/applying_all_recent_innovations_to_train_a_code/"/><updated> 2023-05-26T13:24:52+00:00</updated><published> 2023-05-26T13:24:52+00:00</published><title>应用所有最新创新来训练代码模型</title></entry><entry><author><name>/你/unrahul</name><uri> https://www.reddit.com/user/unrahul</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;我无法在网上找到如何在 Intel dGPU 上微调 LLM，所以我做了一个简单的版本。这个特别的可以用来根据你最喜欢的书（例如）生成文本。如果您拥有 Intel 独立 GPU，我希望它对您有用：&lt;a href=&quot;https://github.com/rahulunair/tiny_llm_finetuning&quot;>;https://github.com/rahulunair/tiny_llm_finetuning&lt;/a>;/ p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/unrahul&quot;>; /u/unrahul &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13s0xqu/tiny_llm_finetuning_a_finetuner_for_openllama_llm/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s0xqu/tiny_llm_finetuning_a_finetuner_for_openllama_llm/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s0xqu </id><link href="https://www.reddit.com/r/deeplearning/comments/13s0xqu/tiny_llm_finetuning_a_finetuner_for_openllama_llm/"/><updated> 2023-05-26T02:36:01+00:00</updated><published> 2023-05-26T02:36:01+00:00</published><title> tiny_llm_finetuning - 英特尔离散 GPU 上 openLLaMA LLM 模型的微调器</title></entry><entry><author><name>/你/米尔赫</name><uri>https://www.reddit.com/user/mirhec</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/mirhec&quot;>; /u/mirhec &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://wordsabout.dev/posts/如何提高迁移学习中的表现/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s4ogw/how_to_improve_performance_in_transfer_learning/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s4ogw </id><link href="https://www.reddit.com/r/deeplearning/comments/13s4ogw/how_to_improve_performance_in_transfer_learning/"/><updated> 2023-05-26T05:52:46+00:00</updated><published> 2023-05-26T05:52:46+00:00</published><title>如何提高迁移学习的性能</title></entry><entry><author><name>/u/钶钽铁矿3</name><uri> https://www.reddit.com/user/coltan3</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/coltan3&quot;>; /u/coltan3 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://help.medium.com/ hc/zh-cn/articles/360018677974&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13sfljp/i_just_wrote_a_comprehensive_breakdown_of_mass/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13sfljp </id><link href="https://www.reddit.com/r/deeplearning/comments/13sfljp/i_just_wrote_a_comprehensive_breakdown_of_mass/"/><updated> 2023-05-26T15:03:06+00:00</updated><published> 2023-05-26T15:03:06+00:00</published><title>我刚刚写了一份关于大规模流离失所的全面分类，你可能不知道生成 AI 已经发生了</title></entry><entry><author><name>/u/Ready-Signature748</name><uri> https://www.reddit.com/user/Ready-Signature748</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Ready-Signature748&quot;>; /u/Ready-Signature748 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://github. com/TransformerOptimus/SuperAGI&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s9xye/github_transformeroptimussuperagi_build_and_run/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s9xye </id><link href="https://www.reddit.com/r/deeplearning/comments/13s9xye/github_transformeroptimussuperagi_build_and_run/"/><updated> 2023-05-26T10:57:46+00:00</updated><published> 2023-05-26T10:57:46+00:00</published><title> GitHub - TransformerOptimus/SuperAGI: 构建和运行有用的自主代理</title></entry><entry><author><name>/u/Happy-Mission-5901</name><uri> https://www.reddit.com/user/Happy-Mission-5901</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;以下哪些技术可以帮助降低视觉转换器训练中梯度消失问题的概率？&lt;/p>; &lt;p>;&lt;a href=&quot;https: //www.reddit.com/poll/13s89m2&quot;>;查看投票&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Happy-Mission-5901&quot;>; /u/Happy-Mission-5901 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https: //www.reddit.com/r/deeplearning/comments/13s89m2/quiz_vanishing_problems_in_vit/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13s89m2/quiz_vanishing_problems_in_vit/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13s89m2 </id><link href="https://www.reddit.com/r/deeplearning/comments/13s89m2/quiz_vanishing_problems_in_vit/"/><updated> 2023-05-26T09:27:46+00:00</updated><published> 2023-05-26T09:27:46+00:00</published><title>测验：ViT 中消失的问题</title></entry><entry><author><name>/u/安摩根24</name><uri> https://www.reddit.com/user/Anmorgan24</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href=&quot;https://i.redd.it/d2rnkyp1i32b1.gif&quot;>;查看个别错误分类实例，以更好地了解您的模型哪里正确，哪里出错。&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Anmorgan24&quot;>; /u/Anmorgan24 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rvimc/debug_image_classifiers_with_an_interactive/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rvimc/debug_image_classifiers_with_an_interactive/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rvimc </id><link href="https://www.reddit.com/r/deeplearning/comments/13rvimc/debug_image_classifiers_with_an_interactive/"/><updated> 2023-05-25T22:30:23+00:00</updated><published> 2023-05-25T22:30:23+00:00</published><title>使用交互式混淆矩阵调试图像分类器</title></entry><entry><author><name>/u/班图</name><uri>https://www.reddit.com/user/班图</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好，对于一个私人项目，我正在尝试为水箱中的活鱼构建一个关键点检测系统。关键点例如眼睛或尾尖，仅在侧视图中。&lt;/p>; &lt;p>;到目前为止，我已经使用 pytorchs Keypoint-RCNN 作为主干构建了一个相当不错的模型。我的重点不是检测图像中的所有实例和关键点，而是只检测非常清晰的实例和关键点（对关键点位置有很高的信心）。这就是为什么在我的第一次尝试中我只注释了“好的”，所以那些鱼：&lt;/p>; &lt;ul>; &lt;li>;完全在图像中&lt;/li>; &lt;li>;没有被其他鱼覆盖&lt; /li>; &lt;li>;距离不太远，因此亮度和大小都合理&lt;/li>; &lt;li>;与图像平面的角度小于 45°（因此仅侧视图）&lt;/li>; &lt;/ ul>; &lt;p>;结果还可以，但不是太好，所以我想知道我应该如何注释图像。我是否应该注释所有实例，即使它们几乎不可见？关键点怎么样？&lt;/p>; &lt;p>;所附照片可能让我对我的问题有了一些了解。&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it /leofziqic12b1.png?width=980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6d8a8480d2dd9b946f7b06bd4a06fec13f518ef7&quot;>;https://preview.redd.it/leofziqic12b1.png?width=980&amp;amp;format=png&amp;auto =webp&amp;amp;s= 6d8a8480d2dd9b946f7b06bd4a06fec13f518ef7&lt;/a>;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/rnc39jqic12b1.png?width=980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bb94130d71a4ead40c5 ea624ce53ba9e224c6ec&quot;>;https: //preview.redd.it/rnc39jqic12b1.png?width=980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6bb94130d71a4ead40c5ea624ce53ba9e224c6ec&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32 ;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Banntu&quot;>; /u/Banntu &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rs20a/how_do_deal_with_bad_visibility_in/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rs20a/how_do_deal_with_bad_visibility_in/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rs20a </id><link href="https://www.reddit.com/r/deeplearning/comments/13rs20a/how_do_deal_with_bad_visibility_in/"/><updated> 2023-05-25T20:15:18+00:00</updated><published> 2023-05-25T20:15:18+00:00</published><title>如何处理关键点注释中的不良可见性</title></entry><entry><author><name>/u/sovit-123</name><uri> https://www.reddit.com/user/sovit-123</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;Wide Residual Neural Networks – WRNs：论文解释&lt;/p>; &lt;p>;&lt;a href=&quot;https://debuggercafe.com/wide- residual-neural-networks-wrns-paper-explanation/&quot;>;https://debuggercafe.com/wide-residual-neural-networks-wrns-paper-explanation/&lt;/a>;&lt;/p>; &lt;p>;&amp;#x200B ;&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/iti44puz342b1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59d22a676de4e9288a570deb2d9409f95c08030e&quot;>;https://预览。 redd.it/iti44puz342b1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59d22a676de4e9288a570deb2d9409f95c08030e&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/sovit-123&quot;>; /u/sovit-123 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www. reddit.com/r/deeplearning/comments/13ryaz7/paper_explanation_wide_residual_neural_networks/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13ryaz7/paper_explanation_wide_residual_neural_networks/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13ryaz7 </id><link href="https://www.reddit.com/r/deeplearning/comments/13ryaz7/paper_explanation_wide_residual_neural_networks/"/><updated> 2023-05-26T00:32:54+00:00</updated><published> 2023-05-26T00:32:54+00:00</published><title> 【论文讲解】Wide Residual Neural Networks – WRNs: Paper Explanation</title></entry><entry><author><name> /u/CS缩短</name><uri>https://www.reddit.com/user/CShorten</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;大家好！我非常高兴能与 Laura Dietz 教授一起发布 Weaviate 播客 #49！&lt;/p>; &lt;p>;此播客有两个主要部分：(1) 搜索中的神经符号 AI 方法和 (2) 使用 LLM 进行相关性判断。最后，我们还反思了围绕 AI 进步（例如 ChatGPT）的更广泛的社会讨论。 &lt;/p>; &lt;p>;神经符号 AI 广泛地描述了深度学习等受神经启发的计算技术与树搜索或知识图数据结构等符号算法的结合！ Dietz 博士解释了许多有趣的想法，尤其是关于实体链接和实体排名的想法。我认为向量搜索与知识图谱技术的交集非常令人兴奋——当然我们也看到了 LLM 如何使用查询语言的更多组合，或者说 MCTS 在 Tree-of-Thoughts 论文中的表现。 &lt;/p>; &lt;p>;使用 LLM 进行相关性判断是搜索技术的另一个绝对巨大的新兴领域！这有很多方面——Dietz 教授和合作者最近发表了“Perspectives on Large Language Models for Relevance Judgement”在用于注释相关性判断的人机协作频谱上。我个人认为这对于那些希望构建搜索功能但还没有关于查询的用户数据并希望生成合成查询来测试不同模型和排名系统的人来说将非常有影响。 Dietz 博士还解释了这与抽象总结和问题回答的判断有何广泛关联——在精神上与我们最近发布的 ChatArena Weaviate 播客非常相似。&lt;/p>; &lt;p>;我从与Dietz 博士，希望您喜欢播客！&lt;/p>; &lt;p>;&lt;a href=&quot;https://www.youtube.com/watch?v=2s%5C_GGMZ%5C_Zgs&quot;>;https://www. youtube.com/watch?v=2s\_GGMZ\_Zgs&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/CShorten&quot;>; /u/CShorten &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rla6o/new_weaviate_podcast_neurosymbolic_search/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rla6o/new_weaviate_podcast_neurosymbolic_search/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rla6o </id><link href="https://www.reddit.com/r/deeplearning/comments/13rla6o/new_weaviate_podcast_neurosymbolic_search/"/><updated> 2023-05-25T15:49:21+00:00</updated><published> 2023-05-25T15:49:21+00:00</published><title>新的 Weaviate 播客 - 神经符号搜索！</title></entry><entry><author><name> /u/帕帕度08</name><uri> https://www.reddit.com/user/Papadude08</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;标题说你们有什么建议可以让你成为更好的程序员吗？&lt;/p>; &lt;p>;我刚刚做了我知道的 mnist 一个这是一个简单的过程，但我仍然觉得很有成就感！&lt;/p>; &lt;p>;非常希望收到任何反馈或建议，非常感谢！&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/Papadude08&quot;>; /u/Papadude08 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rr7o6/any_ideas_for_a_project_would_love_to_do_it_if/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rr7o6/any_ideas_for_a_project_would_love_to_do_it_if/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rr7o6 </id><link href="https://www.reddit.com/r/deeplearning/comments/13rr7o6/any_ideas_for_a_project_would_love_to_do_it_if/"/><updated> 2023-05-25T19:42:38+00:00</updated><published> 2023-05-25T19:42:38+00:00</published><title>如果数据可以访问，任何关于项目的想法都会很乐意去做！</title></entry><entry><author><name> /u/alurayewastaken</name><uri> https://www.reddit.com/user/alurayewastaken</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/alurayewastaken&quot;>; /u/alurayewastaken &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://i.redd.it/ raozsly2d92b1.jpg&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13srhgc/who_can_make_a_deepnude_from_her/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13srhgc </id><link href="https://www.reddit.com/r/deeplearning/comments/13srhgc/who_can_make_a_deepnude_from_her/"/><updated> 2023-05-26T23:12:49+00:00</updated><published> 2023-05-26T23:12:49+00:00</published><title>谁能从她身上做一个深裸体？</title></entry><entry><author><name> /u/OnlyProggingForFun</name><uri> https://www.reddit.com/user/OnlyProggingForFun</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/OnlyProggingForFun&quot;>; /u/OnlyProggingForFun &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://youtu.be/WsCIsqP7U9c&quot; >;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rhyrb/what_is_explainability_ai_with_yotam_azriel_cto/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rhyrb </id><link href="https://www.reddit.com/r/deeplearning/comments/13rhyrb/what_is_explainability_ai_with_yotam_azriel_cto/"/><updated> 2023-05-25T13:34:17+00:00</updated><published> 2023-05-25T13:34:17+00:00</published><title>什么是可解释性人工智能？与 TensorLeap 首席技术官 Yotam Azriel - 什么是人工智能第 13 集</title></entry><entry><author><name>/你/杰斯特177</name><uri> https://www.reddit.com/user/jesst177</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;嗨！&lt;/p>; &lt;p>;我正在努力提高 Pytorch 训练管道的内存和速度效率。在检查过程中，我意识到我们的 GPU 在每个纪元后都变为 IDLE（最后是视觉）。&lt;/p>; &lt;p>;我们的环境是：&lt;/p>; &lt;ul>; &lt;li>;2X V100（Azure 云）。&lt;/li >; &lt;li>;Pytorch 1.13.&lt;/li>; &lt;li>;CUDA 11.6.&lt;/li>; &lt;li>;AMP 已激活。&lt;/li>; &lt;li>;worker 数量为 8.&lt;/li>; &lt;li>;DataParallel 为&lt;/li>; &lt;li>;批量大小为 32。&lt;/li>; &lt;li>;固定内存集。&lt;/li>; &lt;li>;丢弃最后一组。&lt;/li>; &lt;li>;持久性工作集。&lt;/li >; &lt;li>;我们正在使用图像数据的基本预取，我们将所有图像数据移动到 pytohn 字典对象。&lt;/li>; &lt;li>;我们使用 Albumentations 库应用扩充。&lt;/li>; &lt;/ul>; &lt;p>;大 IDLE 块发生在每个纪元之后，我正在努力寻找它的原因。&lt;/p>; &lt;p>;&lt;a href=&quot;https://preview.redd.it/64rgbkdvxv1b1.png?width=1873&amp;amp; format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=48bb250c9d1d7a115f3eaaea92bc59616889e796&quot;>;https://preview.redd.it/64rgbkdvxv1b1.png?width=1873&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s =48bb250c9d1d7a115f3eaaea92bc59616889e796 &lt;/a>;&lt;/p>; &lt;p>;下面可以看到我们的训练代码，&lt;/p>; &lt;pre>;&lt;code>;for epoch in range(last_epoch, end_epoch): for i_iter, batch in enumerate(trainloader, 0) : model.zero_grad(set_to_none=True) images, labels, bd_gts, _ = batch with torch.amp.autocast(device_type=“cuda”, dtype=torch.float16): images = images.cuda() labels = 标签。 cuda() bd_gts = bd_gts.cuda() losses, pred, acc, loss_list = model(images, labels, bd_gts) 如果不是isinstance(pred, (list, tuple)): pred = [pred] loss = losses.mean( ) acc = acc.mean() scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() lr = adjust_learning_rate(optimizer, config.TRAIN.LR, num_iters, i_iter + epoch * epoch_iters) 如果i_iter % print_freq == 0: print(i_iter) &lt;/code>;&lt;/pre>; &lt;p>;我目前看到的是，减少批量大小有帮助（到 2，甚至 6），但是我们没有利用我们的GPU 的内存已满。&lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;我如何查明问题所在或导致此问题的可能原因是什么？&lt;/p>; &lt;/div >;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/jesst177&quot;>; /u/jesst177 &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13qxvn2/massive_drop_in_gpu_usage/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13qxvn2/massive_drop_in_gpu_usage/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13qxvn2 </id><link href="https://www.reddit.com/r/deeplearning/comments/13qxvn2/massive_drop_in_gpu_usage/"/><updated> 2023-05-24T21:12:08+00:00</updated><published> 2023-05-24T21:12:08+00:00</published><title> GPU 使用率大幅下降</title></entry><entry><author><name>/你/查图阿基</name><uri>https://www.reddit.com/user/chatouaki</uri></author><category label="r/deeplearning" term="deeplearning"></category><content type="html">&lt;!-- SC_OFF -->;&lt;div class=&quot;md&quot;>;&lt;p>;你准备好释放你的创造力并学习制作令人惊叹的自定义徽标的艺术了吗？无论您是企业主、平面设计爱好者，还是只是想提高艺术技能的人，本教程都非常适合您。 &lt;/p>; &lt;p>;&lt;a href=&quot;https://youtu.be/MBEBNtX80B0&quot;>;教程链接&lt;/a>; &lt;/p>; &lt;p>;&amp;#x200B;&lt;/p>; &lt;p>;&lt;a href =&quot;https://preview.redd.it/a94h2980oz1b1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1260fc056464f94360cde2d5863b996efc5db7b6&quot;>;https://preview.redd.it/a94h2980oz1b1.png?宽度=1280&amp;amp ;format=png&amp;amp;auto=webp&amp;amp;s=1260fc056464f94360cde2d5863b996efc5db7b6&lt;/a>;&lt;/p>; &lt;/div>;&lt;!-- SC_ON -->; &amp;#32;由&amp;#32;提交&lt;a href=&quot;https://www.reddit.com/user/chatouaki&quot;>; /u/chatouaki &lt;/a>; &lt;br/>; &lt;span>;&lt;a href=&quot;https://www.reddit.com/ r/deeplearning/comments/13rd29h/hand_lettering_logo_design_in_canva/&quot;>;[链接]&lt;/a>;&lt;/span>; &amp;#32; &lt;span>;&lt;a href=&quot;https://www.reddit.com/r/deeplearning/comments/13rd29h/hand_lettering_logo_design_in_canva/&quot;>;[评论]&lt;/a>;&lt;/span>;</content><id> t3_13rd29h </id><link href="https://www.reddit.com/r/deeplearning/comments/13rd29h/hand_lettering_logo_design_in_canva/"/><updated> 2023-05-25T09:36:09+00:00</updated><published> 2023-05-25T09:36:09+00:00</published><title> Canva手写标志设计</title></entry></feed>